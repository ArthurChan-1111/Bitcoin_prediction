{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ArthurChan-1111/Bitcoin_prediction/blob/main/Master_dataset_Fear%20and%20greed_by_year.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JlqCL3Z5-Lpg"
      },
      "source": [
        "#Import Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "fe_jdv1pJbmt",
        "outputId": "e48015a5-ba43-4b98-f636-36dbec6395be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pandas_ta\n",
            "  Downloading pandas_ta-0.3.14b.tar.gz (115 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/115.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.1/115.1 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from pandas_ta) (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas->pandas_ta) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->pandas_ta) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->pandas_ta) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->pandas_ta) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->pandas_ta) (1.17.0)\n",
            "Building wheels for collected packages: pandas_ta\n",
            "  Building wheel for pandas_ta (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pandas_ta: filename=pandas_ta-0.3.14b0-py3-none-any.whl size=218910 sha256=68dbaadb18d36507a78f118a62540ae06ad5097ade3594249723893216217aad\n",
            "  Stored in directory: /root/.cache/pip/wheels/7f/33/8b/50b245c5c65433cd8f5cb24ac15d97e5a3db2d41a8b6ae957d\n",
            "Successfully built pandas_ta\n",
            "Installing collected packages: pandas_ta\n",
            "Successfully installed pandas_ta-0.3.14b0\n",
            "Collecting numpy==1.23.5\n",
            "  Downloading numpy-1.23.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.3 kB)\n",
            "Downloading numpy-1.23.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.1/17.1 MB\u001b[0m \u001b[31m64.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 1.23.5 which is incompatible.\n",
            "chex 0.1.89 requires numpy>=1.24.1, but you have numpy 1.23.5 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.23.5 which is incompatible.\n",
            "scikit-image 0.25.2 requires numpy>=1.24, but you have numpy 1.23.5 which is incompatible.\n",
            "blosc2 3.3.1 requires numpy>=1.26, but you have numpy 1.23.5 which is incompatible.\n",
            "albucore 0.0.23 requires numpy>=1.24.4, but you have numpy 1.23.5 which is incompatible.\n",
            "xarray 2025.3.1 requires numpy>=1.24, but you have numpy 1.23.5 which is incompatible.\n",
            "imbalanced-learn 0.13.0 requires numpy<3,>=1.24.3, but you have numpy 1.23.5 which is incompatible.\n",
            "albumentations 2.0.5 requires numpy>=1.24.4, but you have numpy 1.23.5 which is incompatible.\n",
            "jax 0.5.2 requires numpy>=1.25, but you have numpy 1.23.5 which is incompatible.\n",
            "treescope 0.1.9 requires numpy>=1.25.2, but you have numpy 1.23.5 which is incompatible.\n",
            "jaxlib 0.5.1 requires numpy>=1.25, but you have numpy 1.23.5 which is incompatible.\n",
            "bigframes 2.1.0 requires numpy>=1.24.0, but you have numpy 1.23.5 which is incompatible.\n",
            "pymc 5.22.0 requires numpy>=1.25.0, but you have numpy 1.23.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-1.23.5\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              },
              "id": "64b6d3185aeb440baa1887390d27a5f4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (1.23.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (0.13.2)\n",
            "Requirement already satisfied: numpy!=1.24.0,>=1.20 in /usr/local/lib/python3.11/dist-packages (from seaborn) (1.23.5)\n",
            "Requirement already satisfied: pandas>=1.2 in /usr/local/lib/python3.11/dist-packages (from seaborn) (2.2.2)\n",
            "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in /usr/local/lib/python3.11/dist-packages (from seaborn) (3.10.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.2->seaborn) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.2->seaborn) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn) (1.17.0)\n",
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.11/dist-packages (2.1.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from xgboost) (1.23.5)\n",
            "Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.11/dist-packages (from xgboost) (2.21.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from xgboost) (1.15.2)\n",
            "Cloning into 'Bitcoin_prediction'...\n",
            "remote: Enumerating objects: 595, done.\u001b[K\n",
            "remote: Counting objects: 100% (60/60), done.\u001b[K\n",
            "remote: Compressing objects: 100% (45/45), done.\u001b[K\n",
            "remote: Total 595 (delta 49), reused 15 (delta 15), pack-reused 535 (from 2)\u001b[K\n",
            "Receiving objects: 100% (595/595), 215.58 MiB | 21.04 MiB/s, done.\n",
            "Resolving deltas: 100% (322/322), done.\n",
            "/content/Bitcoin_prediction\n",
            "Requirement already satisfied: pandas_ta in /usr/local/lib/python3.11/dist-packages (0.3.14b0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from pandas_ta) (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas->pandas_ta) (1.23.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->pandas_ta) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->pandas_ta) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->pandas_ta) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->pandas_ta) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "# Check if running in Colab\n",
        "try:\n",
        "    import google.colab\n",
        "    IN_COLAB = True\n",
        "except:\n",
        "    IN_COLAB = False\n",
        "\n",
        "if IN_COLAB:\n",
        "    !pip install pandas_ta\n",
        "    !pip install numpy==1.23.5\n",
        "    !pip install pandas\n",
        "    !pip install seaborn\n",
        "    !pip install xgboost\n",
        "\n",
        "    !git clone https://github.com/ArthurChan-1111/Bitcoin_prediction.git\n",
        "\n",
        "    %cd Bitcoin_prediction\n",
        "    %pip install pandas_ta\n",
        "else:\n",
        "    print(\"Not running in Colab, skipping git clone and directory change\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "RfF2sFtH-Xjn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 418
        },
        "outputId": "c987770b-fad3-4178-a39e-67f23a30dff6"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-e3e3b9a6b570>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# General Libraries\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas_ta\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mta\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcsv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     35\u001b[0m     ) from _err\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m from pandas._config import (\n\u001b[0m\u001b[1;32m     38\u001b[0m     \u001b[0mget_option\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mset_option\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/_config/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;34m\"warn_copy_on_write\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m ]\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_config\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_config\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdates\u001b[0m  \u001b[0;31m# pyright: ignore[reportUnusedImport]  # noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m from pandas._config.config import (\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/_config/config.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m from pandas._typing import (\n\u001b[0m\u001b[1;32m     69\u001b[0m     \u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/_typing.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 198\u001b[0;31m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGenerator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBitGenerator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRandomState\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numpy/__init__.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(attr)\u001b[0m\n\u001b[1;32m    335\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m2.0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1e-5\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAssertionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 337\u001b[0;31m         \u001b[0;32mexcept\u001b[0m \u001b[0mAssertionError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    338\u001b[0m             msg = (\"The current Numpy installation ({!r}) fails to \"\n\u001b[1;32m    339\u001b[0m                    \u001b[0;34m\"pass simple sanity checks. This can be caused for example \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numpy/random/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[0;31m# add these for module-freeze analysis (like PyInstaller)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_pickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_common\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_bounded_integers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numpy/random/_pickle.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmtrand\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRandomState\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_philox\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPhilox\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_pcg64\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPCG64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPCG64DXSM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_sfc64\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSFC64\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mmtrand.pyx\u001b[0m in \u001b[0;36minit numpy.random.mtrand\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject"
          ]
        }
      ],
      "source": [
        "# General Libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pandas_ta as ta\n",
        "import csv\n",
        "import warnings\n",
        "from tqdm import tqdm\n",
        "from tqdm.auto import tqdm\n",
        "from scipy import stats\n",
        "from scipy.stats import zscore\n",
        "\n",
        "# Visualization Libraries\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import pylab as py\n",
        "\n",
        "# Data Preprocessing\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "\n",
        "# General Model Selection and Metrics\n",
        "from sklearn.model_selection import (\n",
        "    train_test_split,\n",
        "    GridSearchCV,\n",
        "    RandomizedSearchCV,\n",
        "    TimeSeriesSplit,\n",
        "    RepeatedKFold,\n",
        "    cross_val_score,\n",
        "    StratifiedKFold\n",
        ")\n",
        "\n",
        "from sklearn.metrics import (\n",
        "    mean_squared_error,\n",
        "    mean_absolute_error,\n",
        "    r2_score\n",
        ")\n",
        "\n",
        "# Regression Models and Tools\n",
        "from sklearn.linear_model import (\n",
        "    LinearRegression,\n",
        "    Ridge,\n",
        "    Lasso,\n",
        "    ElasticNet,\n",
        "    RidgeCV,\n",
        "    LassoCV,\n",
        "    ElasticNetCV\n",
        ")\n",
        "\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import (\n",
        "    BaggingRegressor,\n",
        "    RandomForestRegressor,\n",
        "    GradientBoostingRegressor,\n",
        "    AdaBoostRegressor,\n",
        "    StackingRegressor\n",
        ")\n",
        "from xgboost import XGBRegressor\n",
        "\n",
        "# Stats Models\n",
        "import statsmodels.api as sm\n",
        "import statsmodels.formula.api as smf\n",
        "import statsmodels.stats.api as sms\n",
        "from statsmodels.stats.outliers_influence import (\n",
        "    variance_inflation_factor,\n",
        "    summary_table,\n",
        "    OLSInfluence\n",
        ")\n",
        "from statsmodels.regression.linear_model import OLSResults\n",
        "from statsmodels.stats.stattools import durbin_watson as dwtest\n",
        "from statsmodels.sandbox.stats.runs import runstest_1samp\n",
        "from statsmodels.stats.diagnostic import het_white\n",
        "from statsmodels.compat import lzip\n",
        "\n",
        "# Classification Models and Metrics\n",
        "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.inspection import permutation_importance\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score,\n",
        "    precision_score,\n",
        "    recall_score,\n",
        "    f1_score,\n",
        "    confusion_matrix,\n",
        "    classification_report,\n",
        "    roc_curve,\n",
        "    auc,\n",
        "    roc_auc_score\n",
        ")\n",
        "\n",
        "# Settings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "pd.set_option(\"display.float_format\", lambda x: \"%.4f\" % x)\n",
        "pd.set_option(\"display.max_columns\", None)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k5CupF2I1WTP"
      },
      "source": [
        "#Load dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CTX7TsDYc3wx"
      },
      "outputs": [],
      "source": [
        "btc_data = pd.read_csv('Bitcoin Historical Data.csv', on_bad_lines='skip', lineterminator='\\n')\n",
        "# Check if 'Change %\\r' exists and rename it to 'Change'\n",
        "if 'Change %\\r' in btc_data.columns:\n",
        "    btc_data.rename(columns={'Change %\\r': 'Change'}, inplace=True)\n",
        "btc_data.tail(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XWH7v2wHeHJn"
      },
      "outputs": [],
      "source": [
        "# Convert the dictionary to a DataFrame\n",
        "btc_data = pd.DataFrame(btc_data)\n",
        "\n",
        "# Parse the \"Date\" column into datetime format\n",
        "btc_data[\"Date\"] = pd.to_datetime(btc_data[\"Date\"], format=\"%m/%d/%Y\")\n",
        "\n",
        "# Sort the data by date in ascending order\n",
        "btc_data.sort_values(by=\"Date\", ascending=True, inplace=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0uoXtNE4qYsG"
      },
      "outputs": [],
      "source": [
        "gold_price_data = pd.read_csv('gold_price_data.csv', on_bad_lines='skip', lineterminator='\\n', sep=';')\n",
        "# Check if 'Gold_Volume %\\r' exists and rename it to 'Gold_Volume %'\n",
        "if 'Gold_Volume\\r' in gold_price_data.columns:\n",
        "    gold_price_data.rename(columns={'Gold_Volume\\r': 'Gold_Volume'}, inplace=True)\n",
        "gold_price_data.tail(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wtDmOlHZrsXk"
      },
      "outputs": [],
      "source": [
        "risk_free_data = pd.read_csv('risk_free_data.csv', on_bad_lines='skip', lineterminator='\\n', sep=',')\n",
        "# rename the Yield_Spread\\r to Yield_Spread\n",
        "risk_free_data.rename(columns={'Yield_Spread\\r': 'Yield_Spread'}, inplace=True)\n",
        "risk_free_data.tail(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pTZaq_uv-GPH"
      },
      "outputs": [],
      "source": [
        "spy_data = pd.read_csv('spy_data.csv', on_bad_lines='skip', lineterminator='\\n', sep=',')\n",
        "# Check if 'SPY_Volume\\r' exists and rename it to 'SPY_Volume'\n",
        "if 'SPY_Volume\\r' in spy_data.columns:\n",
        "    spy_data.rename(columns={'SPY_Volume\\r': 'SPY_Volume'}, inplace=True)\n",
        "spy_data.tail(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wM2GhEuUFNUq"
      },
      "outputs": [],
      "source": [
        "eth_data = pd.read_csv('eth_data.csv', on_bad_lines='skip', lineterminator='\\n', sep=',')\n",
        "\n",
        "# Remove the dollar sign and convert 'ETH_Price' to a numeric type\n",
        "eth_data['ETH_Price'] = eth_data['ETH_Price'].replace('[\\$,]', '', regex=True).astype(float)\n",
        "\n",
        "# Check if 'ETH_Volume %\\r' exists and rename it to 'ETH_Volume %'\n",
        "if 'ETH_Volume\\r' in eth_data.columns:\n",
        "    eth_data.rename(columns={'ETH_Volume\\r': 'ETH_Volume'}, inplace=True)\n",
        "\n",
        "eth_data.tail(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yRmPrKkqrw-6"
      },
      "outputs": [],
      "source": [
        "#Change the 'Date' to datetime format\n",
        "gold_price_data['Date'] = pd.to_datetime(gold_price_data['Date'])\n",
        "risk_free_data['Date'] = pd.to_datetime(risk_free_data['Date'])\n",
        "spy_data['Date'] = pd.to_datetime(spy_data['Date'])\n",
        "eth_data['Date'] = pd.to_datetime(eth_data['Date'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KgU2-lQ2Awx1"
      },
      "source": [
        "#Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uImVKfuKAp7F"
      },
      "outputs": [],
      "source": [
        "#Function to clean and convert volume data\n",
        "def clean_volume(volume):\n",
        "    if isinstance(volume, str):  # Check if the value is a string\n",
        "        volume = volume.replace(',', '')  # Remove commas\n",
        "        if 'B' in volume:  # If the value contains 'B' (billions)\n",
        "            return float(volume.replace('B', '')) * 1_000_000_000\n",
        "        elif 'M' in volume:  # If the value contains 'M' (millions)\n",
        "            return float(volume.replace('M', '')) * 1_000_000\n",
        "        elif 'K' in volume:  # If the value contains 'K' (thousands)\n",
        "            return float(volume.replace('K', '')) * 1_000\n",
        "        else:  # If no suffix is present, convert to float directly\n",
        "            return float(volume)\n",
        "    return np.nan  # Handle unexpected cases\n",
        "\n",
        "# Apply the cleaning function to the volume column\n",
        "btc_data[\"Volume\"] = btc_data[\"Vol. ('000)\"].apply(clean_volume)\n",
        "btc_data = btc_data.drop(\"Vol. ('000)\", axis=1)\n",
        "\n",
        "# 4. Remove '%' from \"Change %\" and convert to numeric\n",
        "btc_data.rename(columns={\"Change %\\r\": \"Change\"}, inplace=True)\n",
        "btc_data[\"Change\"] = btc_data[\"Change\"].str.replace(\"%\", \"\").str.strip().astype(float)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o7RHHNjbHqiC"
      },
      "outputs": [],
      "source": [
        "# Define the missing data as a dictionary\n",
        "missing_data = {\n",
        "    'Date': '01/12/2025',\n",
        "    'Price': 94541.8,\n",
        "    'Open': 94600.0,\n",
        "    'High': 95384.3,\n",
        "    'Low': 93711.2,\n",
        "    'Volume': 17.60,  # Assuming 'Vol. (\\'000)' is in thousands\n",
        "    'Change': -0.07  # Assuming 'Change %' is already converted to a float\n",
        "}\n",
        "\n",
        "# Convert the dictionary to a DataFrame\n",
        "missing_data_df = pd.DataFrame([missing_data])\n",
        "\n",
        "# Ensure the 'Date' column is in datetime format\n",
        "missing_data_df['Date'] = pd.to_datetime(missing_data_df['Date'])\n",
        "\n",
        "# Append the missing data to btc_data\n",
        "btc_data = pd.concat([btc_data, missing_data_df], ignore_index=True)\n",
        "\n",
        "# Filter the data for the date range 01/10/2025 to 01/20/2025\n",
        "filtered_data = btc_data[(btc_data['Date'] >= '2025-01-10') & (btc_data['Date'] <= '2025-01-20')]\n",
        "\n",
        "# Display the filtered data\n",
        "print(filtered_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kOl-feBeiLbJ"
      },
      "outputs": [],
      "source": [
        "# Day of the week (0=Monday, 6=Sunday)\n",
        "btc_data[\"Day_of_Week\"] = btc_data[\"Date\"].dt.dayofweek\n",
        "\n",
        "# Week of the year\n",
        "btc_data[\"Week_of_Year\"] = btc_data[\"Date\"].dt.isocalendar().week\n",
        "\n",
        "# Month of the year\n",
        "btc_data[\"Month\"] = btc_data[\"Date\"].dt.month\n",
        "\n",
        "# Quarter of the year\n",
        "btc_data[\"Quarter\"] = btc_data[\"Date\"].dt.quarter\n",
        "\n",
        "# Year\n",
        "btc_data[\"Year\"] = btc_data[\"Date\"].dt.year"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tzdpX4IHvXDW"
      },
      "source": [
        "## Signal Detection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hYeEyw86J_lU"
      },
      "outputs": [],
      "source": [
        "# Calculate RSI, EMA, SMA, and MACD\n",
        "btc_data[\"RSI_6\"] = ta.rsi(btc_data[\"Price\"], length=6)  # Relative Strength Index\n",
        "btc_data[\"RSI_12\"] = ta.rsi(btc_data[\"Price\"], length=12)  # Relative Strength Index\n",
        "btc_data[\"EMA_14\"] = ta.ema(btc_data[\"Price\"], length=14)  # Exponential Moving Average\n",
        "btc_data[\"SMA_14\"] = ta.sma(btc_data[\"Price\"], length=14)  # Simple Moving Average\n",
        "\n",
        "# Add On-Balance Volume (OBV)\n",
        "btc_data[\"OBV\"] = ta.obv(btc_data[\"Price\"], btc_data[\"Volume\"])\n",
        "\n",
        "# Calculate MACD --------------------------------------------------------------------------------------\n",
        "macd = ta.macd(btc_data[\"Price\"], fast=12, slow=26, signal=9)\n",
        "btc_data[\"MACD\"] = macd[\"MACD_12_26_9\"]\n",
        "btc_data[\"MACD_Signal\"] = macd[\"MACDs_12_26_9\"]\n",
        "\n",
        "# Create MACD_buy: 1 if MACD crosses above MACD_Signal, otherwise 0\n",
        "btc_data[\"MACD_buy\"] = 0  # Default to 0 (no buy signal)\n",
        "btc_data.loc[\n",
        "    (btc_data[\"MACD\"] > btc_data[\"MACD_Signal\"]) &  # MACD is above Signal\n",
        "    (btc_data[\"MACD\"].shift(1) <= btc_data[\"MACD_Signal\"].shift(1)),  # Previous MACD was below or equal to Signal\n",
        "    \"MACD_buy\"] = 1  # Buy signal\n",
        "\n",
        "# Create MACD_sell: 1 if MACD crosses below MACD_Signal, otherwise 0\n",
        "btc_data[\"MACD_sell\"] = 0  # Default to 0 (no sell signal)\n",
        "btc_data.loc[\n",
        "    (btc_data[\"MACD\"] <= btc_data[\"MACD_Signal\"]) &  # MACD is below or equal to Signal\n",
        "    (btc_data[\"MACD\"].shift(1) > btc_data[\"MACD_Signal\"].shift(1)),  # Previous MACD was above Signal\n",
        "    \"MACD_sell\"] = 1  # Sell signal\n",
        "\n",
        "# New Variable: MACD_above_signal (1 if MACD > MACD_Signal, otherwise 0)\n",
        "btc_data[\"MACD_above_signal\"] = (btc_data[\"MACD\"] > btc_data[\"MACD_Signal\"]).astype(int)\n",
        "\n",
        "# New Variable: MACD_below_signal (1 if MACD <= MACD_Signal, otherwise 0)\n",
        "btc_data[\"MACD_below_signal\"] = (btc_data[\"MACD\"] <= btc_data[\"MACD_Signal\"]).astype(int)\n",
        "\n",
        "# Create MACD_cum_buy: Accumulate consecutive days where MACD_above_signal is 1\n",
        "btc_data[\"MACD_cum_buy\"] = (\n",
        "    btc_data[\"MACD_above_signal\"]\n",
        "    .groupby((btc_data[\"MACD_above_signal\"] == 0).cumsum())  # Group by resets when MACD_above_signal is 0\n",
        "    .cumcount()  # Count consecutive days starting from 1\n",
        ")\n",
        "btc_data.loc[btc_data[\"MACD_above_signal\"] == 0, \"MACD_cum_buy\"] = 0  # Reset to 0 where the condition is not met\n",
        "\n",
        "# Create MACD_cum_sell: Accumulate consecutive days where MACD_below_signal is 1\n",
        "btc_data[\"MACD_cum_sell\"] = (\n",
        "    btc_data[\"MACD_below_signal\"]\n",
        "    .groupby((btc_data[\"MACD_below_signal\"] == 0).cumsum())  # Group by resets when MACD_below_signal is 0\n",
        "    .cumcount()  # Count consecutive days starting from 1\n",
        ")\n",
        "btc_data.loc[btc_data[\"MACD_below_signal\"] == 0, \"MACD_cum_sell\"] = 0  # Reset to 0\n",
        "\n",
        "\n",
        "# Calculate Bollinger Bands --------------------------------------------------------------------------------------\n",
        "bb = ta.bbands(btc_data[\"Price\"], length=20, std=2)  # 20-period BB with 2 standard deviations\n",
        "btc_data[\"BB_upper\"] = bb.get(\"BBU_20_2.0\")  # Upper Bollinger Band\n",
        "btc_data[\"BB_lower\"] = bb.get(\"BBL_20_2.0\")  # Lower Bollinger Band\n",
        "\n",
        "# Create BB_Buy: 1 if Price crosses below the Lower Band, otherwise 0\n",
        "btc_data[\"BB_Buy\"] = 0  # Default to 0 (no buy signal)\n",
        "btc_data.loc[\n",
        "    (btc_data[\"Price\"] < btc_data[\"BB_lower\"]) &  # Price is below the Lower Band\n",
        "    (btc_data[\"Price\"].shift(1) >= btc_data[\"BB_lower\"].shift(1)),  # Previous Price was above or equal to Lower Band\n",
        "    \"BB_Buy\"] = 1  # Buy signal\n",
        "\n",
        "# Create BB_Sell: 1 if Price crosses above the Upper Band, otherwise 0\n",
        "btc_data[\"BB_Sell\"] = 0  # Default to 0 (no sell signal)\n",
        "btc_data.loc[\n",
        "    (btc_data[\"Price\"] > btc_data[\"BB_upper\"]) &  # Price is above the Upper Band\n",
        "    (btc_data[\"Price\"].shift(1) <= btc_data[\"BB_upper\"].shift(1)),  # Previous Price was below or equal to Upper Band\n",
        "    \"BB_Sell\"] = 1  # Sell signal\n",
        "\n",
        "# New Variable: Price_above_lower_band (1 if Price > BB_lower, otherwise 0)\n",
        "btc_data[\"Price_below_BB_lower\"] = (btc_data[\"Price\"] < btc_data[\"BB_lower\"]).astype(int)\n",
        "\n",
        "# New Variable: Price_below_upper_band (1 if Price < BB_upper, otherwise 0)\n",
        "btc_data[\"Price_above_BB_upper\"] = (btc_data[\"Price\"] > btc_data[\"BB_upper\"]).astype(int)\n",
        "\n",
        "# Create BB_cum_Buy: Accumulate consecutive days where Price_below_BB_lower is 1\n",
        "btc_data[\"BB_cum_Buy\"] = (\n",
        "    btc_data[\"Price_below_BB_lower\"]\n",
        "    .groupby((btc_data[\"Price_below_BB_lower\"] == 0).cumsum())  # Group by resets when Price_below_BB_lower is 0\n",
        "    .cumcount()  # Count consecutive days starting from 1\n",
        ")\n",
        "btc_data.loc[btc_data[\"Price_below_BB_lower\"] == 0, \"BB_cum_Buy\"] = 0  # Reset to 0 where the condition is not met\n",
        "\n",
        "# Create BB_cum_Sell: Accumulate consecutive days where Price_above_BB_upper is 1\n",
        "btc_data[\"BB_cum_Sell\"] = (\n",
        "    btc_data[\"Price_above_BB_upper\"]\n",
        "    .groupby((btc_data[\"Price_above_BB_upper\"] == 0).cumsum())  # Group by resets when Price_above_BB_upper is 0\n",
        "    .cumcount()  # Count consecutive days starting from 1\n",
        ")\n",
        "btc_data.loc[btc_data[\"Price_above_BB_upper\"] == 0, \"BB_cum_Sell\"] = 0  # Reset to 0 where the condition is not met\n",
        "\n",
        "# Calculate ATR --------------------------------------------------------------------------------------\n",
        "btc_data[\"ATR\"] = ta.atr(btc_data[\"High\"], btc_data[\"Low\"], btc_data[\"Price\"], length=14)\n",
        "\n",
        "# Set Stop-Loss Levels (Example with Long Trade)\n",
        "atr_multiplier = 2\n",
        "btc_data[\"Stop_Loss_Long\"] = btc_data[\"Price\"] - (btc_data[\"ATR\"] * atr_multiplier) #Stop-Loss for Buy (Long) Trade\n",
        "btc_data[\"Stop_Loss_Short\"] = btc_data[\"Price\"] + (btc_data[\"ATR\"] * atr_multiplier) #Stop-Loss for Sell (Short) Trade\n",
        "\n",
        "# Calculate VWAP --------------------------------------------------------------------------------------\n",
        "btc_data.set_index(\"Date\", inplace=True) # Set the \"Date\" column as the index\n",
        "btc_data[\"VWAP\"] = ta.vwap(btc_data[\"High\"], btc_data[\"Low\"], btc_data[\"Price\"], btc_data[\"Volume\"])\n",
        "\n",
        "# Create VWAP_Buy: 1 if Price crosses above VWAP, otherwise 0\n",
        "btc_data[\"VWAP_Buy\"] = 0  # Default to 0 (no buy signal)\n",
        "btc_data.loc[\n",
        "    (btc_data[\"Price\"] > btc_data[\"VWAP\"]) &  # Price is above VWAP\n",
        "    (btc_data[\"Price\"].shift(1) <= btc_data[\"VWAP\"].shift(1)),  # Previous Price was below or equal to VWAP\n",
        "    \"VWAP_Buy\"] = 1  # Buy signal\n",
        "\n",
        "# Create VWAP_Sell: 1 if Price crosses below VWAP, otherwise 0\n",
        "btc_data[\"VWAP_Sell\"] = 0  # Default to 0 (no sell signal)\n",
        "btc_data.loc[\n",
        "    (btc_data[\"Price\"] < btc_data[\"VWAP\"]) &  # Price is below VWAP\n",
        "    (btc_data[\"Price\"].shift(1) >= btc_data[\"VWAP\"].shift(1)),  # Previous Price was above or equal to VWAP\n",
        "    \"VWAP_Sell\"] = 1  # Sell signal\n",
        "\n",
        "# Fear and greed index\n",
        "fear_and_greed_index = pd.read_csv('fear_and_greed_index.csv')\n",
        "# rename the date column in fear_and_greed_index to Date\n",
        "fear_and_greed_index.rename(columns={\"date\": \"Date\", \"value\": \"Fear_and_Greed_Index\"}, inplace=True)\n",
        "# convert the date column to datetime\n",
        "fear_and_greed_index[\"Date\"] = pd.to_datetime(fear_and_greed_index[\"Date\"], format=\"%Y-%m-%d\")\n",
        "# dropping the timestamp, value_classification, time_until_update columns\n",
        "fear_and_greed_index.drop(columns=[\"timestamp\", \"value_classification\", \"time_until_update\"], inplace=True)\n",
        "fear_and_greed_index.head()\n",
        "\n",
        "# Join the two dataframes on the \"Date\" column, dropping\n",
        "btc_data = btc_data.merge(fear_and_greed_index, on=\"Date\", how=\"left\")\n",
        "btc_data.head(20)\n",
        "\n",
        "# Support and Resistance Levels\n",
        "btc_data[\"Support\"] = btc_data[\"Low\"].rolling(window=20).min()  # Lowest low in the past 20 days\n",
        "btc_data[\"Resistance\"] = btc_data[\"High\"].rolling(window=20).max()  # Highest high in the past 20 days\n",
        "\n",
        "# Add two columns for extreme fear and extreme greed\n",
        "btc_data[\"Extreme_Fear\"] = np.where(btc_data[\"Fear_and_Greed_Index\"] < 20, 1, 0)\n",
        "btc_data[\"Extreme_Greed\"] = np.where(btc_data[\"Fear_and_Greed_Index\"] > 80, 1, 0)\n",
        "btc_data.describe().T\n",
        "\n",
        "# Use 70 in RSI to classify overbought and 30 to classify oversold\n",
        "btc_data[\"RSI_12_Oversold\"] = np.where(btc_data[\"RSI_12\"] < 30, 1, 0)  # Create RSI_12_Oversold column\n",
        "btc_data.loc[btc_data[\"RSI_12_Oversold\"] == 0, \"Consecutive_RSI_12_Oversold\"] = 0  # Reset to 0 where the condition is not met\n",
        "btc_data[\"RSI_12_Overbought\"] = np.where(btc_data[\"RSI_12\"] > 70, 1, 0)\n",
        "\n",
        "# Adding RSI_6 greater than RSI_12 to show bullish divergence and conversely\n",
        "btc_data[\"RSI_Divergence\"] = np.where(btc_data[\"RSI_6\"] > btc_data[\"RSI_12\"], 1, 0)\n",
        "btc_data.describe().T\n",
        "\n",
        "\n",
        "# Count consecutive appearances of Extreme_Fear\n",
        "btc_data[\"Consecutive_Extreme_Fear\"] = (\n",
        "    btc_data[\"Extreme_Fear\"]\n",
        "    .groupby((btc_data[\"Extreme_Fear\"] == 0).cumsum())  # Group by resets when Extreme_Fear is 0\n",
        "    .cumcount()  # Count consecutive days starting from 1\n",
        ")\n",
        "btc_data.loc[btc_data[\"Extreme_Fear\"] == 0, \"Consecutive_Extreme_Fear\"] = 0  # Reset to 0 where the condition is not met\n",
        "\n",
        "# Count consecutive appearances of Extreme_Greed\n",
        "btc_data[\"Consecutive_Extreme_Greed\"] = (\n",
        "    btc_data[\"Extreme_Greed\"]\n",
        "    .groupby((btc_data[\"Extreme_Greed\"] == 0).cumsum())  # Group by resets when Extreme_Greed is 0\n",
        "    .cumcount()  # Count consecutive days starting from 1\n",
        ")\n",
        "btc_data.loc[btc_data[\"Extreme_Greed\"] == 0, \"Consecutive_Extreme_Greed\"] = 0  # Reset to 0 where the condition is not met\n",
        "\n",
        "# Forward fill the missing value in 'Fear_and_Greed_Index'\n",
        "btc_data['Fear_and_Greed_Index'] = btc_data['Fear_and_Greed_Index'].fillna(method='ffill')\n",
        "\n",
        "# Count consecutive appearances of RSI_12_Overbought\n",
        "btc_data[\"Consecutive_RSI_12_Overbought\"] = (\n",
        "    btc_data[\"RSI_12_Overbought\"]\n",
        "    .groupby((btc_data[\"RSI_12_Overbought\"] == 0).cumsum())  # Group by resets when RSI_12_Overbought is 0\n",
        "    .cumcount()  # Count consecutive days starting from 1\n",
        ")\n",
        "btc_data.loc[btc_data[\"RSI_12_Overbought\"] == 0, \"Consecutive_RSI_12_Overbought\"] = 0  # Reset to 0 where the condition is not met\n",
        "\n",
        "# Count consecutive appearances of RSI_12_Oversold\n",
        "btc_data[\"Consecutive_RSI_12_Oversold\"] = (\n",
        "    btc_data[\"RSI_12_Oversold\"]\n",
        "    .groupby((btc_data[\"RSI_12_Oversold\"] == 0).cumsum())  # Group by resets when RSI_12_Oversold is 0\n",
        "    .cumcount()  # Count consecutive days starting from 1\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VwCi0T_SmYTy"
      },
      "outputs": [],
      "source": [
        "#Add continous variables\n",
        "\n",
        "#ATR ratio\n",
        "btc_data['ATR_Ratio'] = (btc_data['ATR'] / btc_data['Price']) * 100\n",
        "\n",
        "# Calculate 50-day and 200-day moving averages\n",
        "btc_data['MA50'] = btc_data['Price'].rolling(window=50).mean()\n",
        "btc_data['MA200'] = btc_data['Price'].rolling(window=200).mean()\n",
        "\n",
        "# Calculate Price relative to 50-day MA (%)\n",
        "btc_data['Price_vs_MA50'] = ((btc_data['Price'] - btc_data['MA50']) / btc_data['MA50']) * 100\n",
        "\n",
        "# Calculate Price relative to 200-day MA (%)\n",
        "btc_data['Price_vs_MA200'] = ((btc_data['Price'] - btc_data['MA200']) / btc_data['MA200']) * 100\n",
        "\n",
        "# Calculate MA Distance Ratio (Ratio of MA50 to MA200 distance)\n",
        "btc_data['MA_Distance_Ratio'] = ((btc_data['MA50'] - btc_data['MA200']) / btc_data['Price']) * 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Td80ru1Z-ulJ"
      },
      "outputs": [],
      "source": [
        "from scipy.stats import linregress\n",
        "\n",
        "# Ensure btc_data is sorted by Date\n",
        "btc_data = btc_data.sort_values('Date')\n",
        "\n",
        "# 1. Volume_Ratio: Current volume / 20-day avg volume\n",
        "btc_data['Avg_Volume_20'] = btc_data['Volume'].rolling(window=20).mean()\n",
        "btc_data['Volume_Ratio'] = btc_data['Volume'] / btc_data['Avg_Volume_20']\n",
        "\n",
        "# 2. OBV_Slope: 5-day slope of On-Balance Volume (OBV)\n",
        "# Calculate OBV\n",
        "btc_data['OBV'] = (np.sign(btc_data['Price'].diff()) * btc_data['Volume']).fillna(0).cumsum()\n",
        "\n",
        "# Calculate 5-day slope of OBV\n",
        "def calculate_slope(series):\n",
        "    x = np.arange(len(series))\n",
        "    slope, _, _, _, _ = linregress(x, series)\n",
        "    return slope\n",
        "\n",
        "btc_data['OBV_Slope'] = btc_data['OBV'].rolling(window=5).apply(calculate_slope, raw=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Y-jkoPrKL_7"
      },
      "outputs": [],
      "source": [
        "# Calculate the percentage price change over the next 7-day and 2-day (Target Variable for Regression)\n",
        "btc_data[\"Pct_Change_7D\"] = ((btc_data[\"Price\"].shift(-7) - btc_data[\"Price\"]) / btc_data[\"Price\"]) * 100\n",
        "btc_data[\"Pct_Change_2D\"] = ((btc_data[\"Price\"].shift(-2) - btc_data[\"Price\"]) / btc_data[\"Price\"]) * 100\n",
        "\n",
        "# Create binary variables for 7-day and 2-day percentage changes\n",
        "btc_data[\"Positive_7D\"] = (btc_data[\"Pct_Change_7D\"] > 0).astype(int)  # 1 if positive, 0 if negative\n",
        "btc_data[\"Positive_2D\"] = (btc_data[\"Pct_Change_2D\"] > 0).astype(int)  # 1 if positive, 0 if negative"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yj_e_l3XvMv7"
      },
      "source": [
        "## Pattern Detection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Uvu3PlfvlHX"
      },
      "outputs": [],
      "source": [
        "# Asending Triangle\n",
        "# Sort data by Date\n",
        "btc_data = btc_data.sort_values('Date')\n",
        "\n",
        "# Calculate exponential moving average for Resistance_Level\n",
        "btc_data['Resistance_Level'] = btc_data['High'].ewm(span=7, adjust=False).mean()\n",
        "\n",
        "# Identify rising trendline (higher lows)\n",
        "btc_data['Low_Shifted'] = btc_data['Low'].shift(1)\n",
        "btc_data['Higher_Low'] = (btc_data['Low'] > btc_data['Low_Shifted']).astype(int)\n",
        "\n",
        "# Detect breakout using rolling mean of Volume\n",
        "btc_data['Breakout'] = btc_data['Price'] > btc_data['Resistance_Level']\n",
        "\n",
        "# Add ascending triangle detection signal\n",
        "btc_data['Ascending_Triangle_Breakout'] = btc_data['Breakout']\n",
        "\n",
        "# Ensure Breakout is binary (1 or 0)\n",
        "btc_data['Breakout'] = btc_data['Breakout'].astype(int)\n",
        "\n",
        "# Ensure Ascending_Triangle_Breakout is binary (1 or 0)\n",
        "btc_data['Ascending_Triangle_Breakout'] = btc_data['Ascending_Triangle_Breakout'].astype(int)\n",
        "\n",
        "# Calculate the rolling mean of Volume over 7 days\n",
        "btc_data['Avg_Volume'] = btc_data['Volume'].rolling(window=7).mean()\n",
        "\n",
        "# Detect breakout with volume confirmation\n",
        "btc_data['Breakout_With_Volume'] = ((btc_data['High'] > btc_data['Resistance_Level']) &\n",
        "                                    (btc_data['Volume'] > btc_data['Avg_Volume'])).astype(int)\n",
        "\n",
        "# Add ascending triangle detection signal with volume\n",
        "btc_data['Ascending_Triangle_Breakout_With_Volume'] = btc_data['Breakout_With_Volume']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RpkSoHAQvk98"
      },
      "outputs": [],
      "source": [
        "# Count the number of breakouts\n",
        "breakout_count = btc_data['Ascending_Triangle_Breakout_With_Volume'].sum()\n",
        "print(f\"Number of breakouts: {breakout_count}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lAQMsKoemCuP"
      },
      "outputs": [],
      "source": [
        "btc_data['Triangle_Completion'] = btc_data['Higher_Low'].rolling(window=7).mean()  # Simulate completion as a rolling mean of Higher_Low\n",
        "btc_data['Triangle_Height'] = ((btc_data['High'] - btc_data['Low']) / btc_data['Price']).rolling(window=7).mean()  # Height as % of price\n",
        "btc_data['Triangle_Duration'] = btc_data['Higher_Low'].rolling(window=7).sum()  # Duration as the sum of Higher_Low over 7 days\n",
        "btc_data['Breakout_Strength'] = ((btc_data['High'] - btc_data['Resistance_Level']) * btc_data['Volume']).rolling(window=7).mean()  # Volume-adjusted breakout strength"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ebGGzgsO7qr3"
      },
      "outputs": [],
      "source": [
        "#ADX Pattern (Show the strength of the trend, please only use ADX_Buy_Signal)\n",
        "!pip install ta\n",
        "import ta\n",
        "# Calculate ADX, +DI, and -DI using the ta library\n",
        "btc_data['ADX'] = ta.trend.adx(btc_data['High'], btc_data['Low'], btc_data['Price'], window=14)\n",
        "btc_data['Positive_DI'] = ta.trend.adx_pos(btc_data['High'], btc_data['Low'], btc_data['Price'], window=14)\n",
        "btc_data['Negative_DI'] = ta.trend.adx_neg(btc_data['High'], btc_data['Low'], btc_data['Price'], window=14)\n",
        "\n",
        "# Define the ADX threshold for a strong trend\n",
        "adx_threshold = 25\n",
        "\n",
        "# Generate trade signals\n",
        "btc_data['ADX_Buy_Signal'] = ((btc_data['ADX'] > adx_threshold) & (btc_data['Positive_DI'] > btc_data['Negative_DI'])).astype(int)\n",
        "btc_data['ADX_Sell_Signal'] = ((btc_data['ADX'] > adx_threshold) & (btc_data['Negative_DI'] > btc_data['Positive_DI'])).astype(int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UxsP_-CmH2rN"
      },
      "outputs": [],
      "source": [
        "# Create columns for consecutive ADX Buy and Sell signals\n",
        "btc_data['Consecutive_ADX_Buy'] = (\n",
        "    btc_data['ADX_Buy_Signal']\n",
        "    .groupby((btc_data['ADX_Buy_Signal'] == 0).cumsum())\n",
        "    .cumcount()\n",
        ")\n",
        "btc_data.loc[btc_data['ADX_Buy_Signal'] == 0, 'Consecutive_ADX_Buy'] = 0\n",
        "\n",
        "btc_data['Consecutive_ADX_Sell'] = (\n",
        "    btc_data['ADX_Sell_Signal']\n",
        "    .groupby((btc_data['ADX_Sell_Signal'] == 0).cumsum())\n",
        "    .cumcount()\n",
        ")\n",
        "btc_data.loc[btc_data['ADX_Sell_Signal'] == 0, 'Consecutive_ADX_Sell'] = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ejD4BOm-7qdN"
      },
      "outputs": [],
      "source": [
        "# Count the number of breakout signals\n",
        "breakout_count = btc_data['ADX_Buy_Signal'].sum()\n",
        "print(f\"Number of breakout signals: {breakout_count}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cguOIO8WmK5Z"
      },
      "outputs": [],
      "source": [
        "# Add continuous features\n",
        "# ADX_Strength: How far ADX is above the threshold (ADX - 25, 0 if below)\n",
        "btc_data['ADX_Strength'] = (btc_data['ADX'] - adx_threshold).clip(lower=0)\n",
        "\n",
        "# ADX_Trend_Duration: Number of consecutive days ADX > 25\n",
        "btc_data['ADX_Trend_Duration'] = (btc_data['ADX'] > adx_threshold).astype(int).groupby((btc_data['ADX'] <= adx_threshold).astype(int).cumsum()).cumsum()\n",
        "\n",
        "# ADX_Trend_Direction: +DI minus -DI (positive for uptrend, negative for downtrend)\n",
        "btc_data['ADX_Trend_Direction'] = btc_data['Positive_DI'] - btc_data['Negative_DI']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eU5mhlpV-ntq"
      },
      "outputs": [],
      "source": [
        "# Calculate Stochastic Oscillator (%K and %D)\n",
        "btc_data['Stochastic_perK'] = ta.momentum.stoch(\n",
        "    btc_data['High'], btc_data['Low'], btc_data['Price'], window=14, smooth_window=3\n",
        ")\n",
        "btc_data['Stochastic_perD'] = ta.momentum.stoch_signal(\n",
        "    btc_data['High'], btc_data['Low'], btc_data['Price'], window=14, smooth_window=3\n",
        ")\n",
        "\n",
        "# Define Stochastic Buy and Sell Signals\n",
        "btc_data['Stochastic_Buy_Signal'] = np.where(\n",
        "    (btc_data['Stochastic_perK'] < 20) & (btc_data['Stochastic_perK'] > btc_data['Stochastic_perD']),\n",
        "    1,  # Buy Signal\n",
        "    0   # No Signal\n",
        ")\n",
        "\n",
        "btc_data['Stochastic_Sell_Signal'] = np.where(\n",
        "    (btc_data['Stochastic_perK'] > 80) & (btc_data['Stochastic_perK'] < btc_data['Stochastic_perD']),\n",
        "    1,  # Sell Signal\n",
        "    0   # No Signal\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6GmYoGZNH9Xj"
      },
      "outputs": [],
      "source": [
        "# Add consecutive features for Stochastic signals\n",
        "btc_data['Consecutive_Stochastic_Buy'] = (\n",
        "    btc_data['Stochastic_Buy_Signal']\n",
        "    .groupby((btc_data['Stochastic_Buy_Signal'] == 0).cumsum())\n",
        "    .cumcount()\n",
        ")\n",
        "\n",
        "btc_data['Consecutive_Stochastic_Sell'] = (\n",
        "    btc_data['Stochastic_Sell_Signal']\n",
        "    .groupby((btc_data['Stochastic_Sell_Signal'] == 0).cumsum())\n",
        "    .cumcount()\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bIdDYQw1r8PZ"
      },
      "outputs": [],
      "source": [
        "#Bullish and Bearish Engulfing\n",
        "def detect_bullish_engulfing(data):\n",
        "    return (\n",
        "        (data['Price'] > data['Open']) &  # Current candle is bullish\n",
        "        (data['Price'].shift(1) < data['Open'].shift(1)) &  # Previous candle is bearish\n",
        "        (data['Open'] < data['Price'].shift(1)) &  # Current open is below previous close\n",
        "        (data['Price'] > data['Open'].shift(1))    # Current close is above previous open\n",
        "    )\n",
        "\n",
        "def detect_bearish_engulfing(data):\n",
        "    return (\n",
        "        (data['Price'] < data['Open']) &  # Current candle is bearish\n",
        "        (data['Price'].shift(1) > data['Open'].shift(1)) &  # Previous candle is bullish\n",
        "        (data['Open'] > data['Price'].shift(1)) &  # Current open is above previous close\n",
        "        (data['Price'] < data['Open'].shift(1))    # Current close is below previous open\n",
        "    )\n",
        "\n",
        "# Apply the detection functions to btc_data\n",
        "btc_data['Bullish_Engulfing'] = detect_bullish_engulfing(btc_data).astype(int)\n",
        "btc_data['Bearish_Engulfing'] = detect_bearish_engulfing(btc_data).astype(int)\n",
        "\n",
        "# Display the rows where patterns are detected\n",
        "bullish_patterns = btc_data[btc_data['Bullish_Engulfing'] == 1]\n",
        "bearish_patterns = btc_data[btc_data['Bearish_Engulfing'] == 1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ow8D8L0Sr8H0"
      },
      "outputs": [],
      "source": [
        "breakout_count = btc_data['Bullish_Engulfing'].sum()\n",
        "print(f\"Number of breakout signals: {breakout_count}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tJ6CxrGWr8Ad"
      },
      "outputs": [],
      "source": [
        "breakout_count = btc_data['Bearish_Engulfing'].sum()\n",
        "print(f\"Number of breakout signals: {breakout_count}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bKw4kLG8lAuV"
      },
      "outputs": [],
      "source": [
        "def detect_hammer(data):\n",
        "    \"\"\"\n",
        "    Detects hammer candlestick patterns.\n",
        "    A hammer has:\n",
        "    1. A small body at the top\n",
        "    2. A long lower shadow (at least 2x the body)\n",
        "    3. Little or no upper shadow\n",
        "    \"\"\"\n",
        "\n",
        "    # Calculate body and shadows\n",
        "    data['Body'] = data['Price'] - data['Open']\n",
        "    data['Upper_Shadow'] = data['High'] - data[['Open', 'Price']].max(axis=1)\n",
        "    data['Lower_Shadow'] = data[['Open', 'Price']].min(axis=1) - data['Low']\n",
        "\n",
        "    # Define hammer criteria\n",
        "    data['Hammer'] = (\n",
        "        # Body is small (using 0.3 times high-low range as threshold)\n",
        "        (abs(data['Body']) <= 0.3 * (data['High'] - data['Low'])) &\n",
        "        # Lower shadow is at least 2x the body length\n",
        "        (data['Lower_Shadow'] >= 2 * abs(data['Body'])) &\n",
        "        # Upper shadow is relatively small\n",
        "        (data['Upper_Shadow'] <= 0.1 * (data['High'] - data['Low']))\n",
        "    ).astype(int)\n",
        "\n",
        "    # Clean up temporary columns\n",
        "    data.drop(['Body', 'Upper_Shadow', 'Lower_Shadow'], axis=1, inplace=True)\n",
        "\n",
        "    # Count consecutive hammers\n",
        "    data['Consecutive_Hammer'] = (\n",
        "        data['Hammer']\n",
        "        .groupby((data['Hammer'] == 0).cumsum())\n",
        "        .cumcount()\n",
        "    )\n",
        "    data.loc[data['Hammer'] == 0, 'Consecutive_Hammer'] = 0\n",
        "\n",
        "    return data\n",
        "\n",
        "# Apply hammer detection to the data\n",
        "btc_data = detect_hammer(btc_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Tde3F5WlAuV"
      },
      "outputs": [],
      "source": [
        "hammer_count = btc_data['Hammer'].sum()\n",
        "print(f\"Total number of hammer patterns detected: {hammer_count}\")\n",
        "\n",
        "# Also get the number of cases where we had consecutive hammers\n",
        "consecutive_hammers = btc_data[btc_data['Consecutive_Hammer'] > 0]\n",
        "print(f\"Number of cases with consecutive hammers: {len(consecutive_hammers)}\")\n",
        "print(f\"Maximum number of consecutive hammers: {btc_data['Consecutive_Hammer'].max()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Q8lSznMlAuV",
        "vscode": {
          "languageId": "ruby"
        }
      },
      "outputs": [],
      "source": [
        "def detect_three_white_soldiers(data, min_body_ratio=0.5):\n",
        "    # Create candle body and shadow lengths\n",
        "    data['Body'] = data['Price'] - data['Open']\n",
        "    data['Upper_Shadow'] = data['High'] - data[['Price', 'Open']].max(axis=1)\n",
        "    data['Lower_Shadow'] = data[['Price', 'Open']].min(axis=1) - data['Low']\n",
        "\n",
        "    # Initialize Three White Soldiers pattern array\n",
        "    three_white_soldiers = np.zeros(len(data))\n",
        "\n",
        "    for i in range(2, len(data)):\n",
        "        # Check if we have 3 consecutive bullish candles\n",
        "        bullish_candles = (data['Body'].iloc[i] > 0 and\n",
        "                          data['Body'].iloc[i-1] > 0 and\n",
        "                          data['Body'].iloc[i-2] > 0)\n",
        "\n",
        "        if bullish_candles:\n",
        "            # Calculate total candle lengths\n",
        "            candle_lengths = [\n",
        "                data['High'].iloc[i] - data['Low'].iloc[i],\n",
        "                data['High'].iloc[i-1] - data['Low'].iloc[i-1],\n",
        "                data['High'].iloc[i-2] - data['Low'].iloc[i-2]\n",
        "            ]\n",
        "\n",
        "            # Calculate body ratios\n",
        "            body_ratios = [\n",
        "                data['Body'].iloc[i] / candle_lengths[0],\n",
        "                data['Body'].iloc[i-1] / candle_lengths[1],\n",
        "                data['Body'].iloc[i-2] / candle_lengths[2]\n",
        "            ]\n",
        "\n",
        "            # Check if each candle has a sufficient body ratio\n",
        "            strong_bodies = all(ratio >= min_body_ratio for ratio in body_ratios)\n",
        "\n",
        "            # Check if each candle opens within previous candle's body\n",
        "            progressive_opens = (\n",
        "                data['Open'].iloc[i] > data['Open'].iloc[i-1] and\n",
        "                data['Open'].iloc[i-1] > data['Open'].iloc[i-2]\n",
        "            )\n",
        "\n",
        "            # Check if each candle closes higher than previous candle\n",
        "            progressive_closes = (\n",
        "                data['Price'].iloc[i] > data['Price'].iloc[i-1] and\n",
        "                data['Price'].iloc[i-1] > data['Price'].iloc[i-2]\n",
        "            )\n",
        "\n",
        "            # If all conditions are met, mark as Three White Soldiers\n",
        "            if strong_bodies and progressive_opens and progressive_closes:\n",
        "                three_white_soldiers[i] = 1\n",
        "\n",
        "    # Add the pattern to the dataframe\n",
        "    data['Three_White_Soldiers'] = three_white_soldiers\n",
        "\n",
        "    # Count the occurrences\n",
        "    pattern_count = int(three_white_soldiers.sum())\n",
        "\n",
        "    return pattern_count, data['Three_White_Soldiers']\n",
        "\n",
        "# Apply the detection\n",
        "pattern_count, three_white_soldiers = detect_three_white_soldiers(btc_data)\n",
        "print(f\"Number of Three White Soldiers patterns detected: {pattern_count}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i8HYxhux9X3X"
      },
      "outputs": [],
      "source": [
        "def detect_three_black_crows(data, min_body_ratio=0.5):\n",
        "    # Create candle body and shadow lengths\n",
        "    data['Body'] = data['Price'] - data['Open']\n",
        "    data['Upper_Shadow'] = data['High'] - data[['Price', 'Open']].max(axis=1)\n",
        "    data['Lower_Shadow'] = data[['Price', 'Open']].min(axis=1) - data['Low']\n",
        "\n",
        "    # Initialize Three Black Crows pattern array\n",
        "    three_black_crows = np.zeros(len(data))\n",
        "\n",
        "    for i in range(2, len(data)):\n",
        "        # Check if we have 3 consecutive bearish candles\n",
        "        bearish_candles = (data['Body'].iloc[i] < 0 and\n",
        "                          data['Body'].iloc[i-1] < 0 and\n",
        "                          data['Body'].iloc[i-2] < 0)\n",
        "\n",
        "        if bearish_candles:\n",
        "            # Calculate total candle lengths\n",
        "            candle_lengths = [\n",
        "                data['High'].iloc[i] - data['Low'].iloc[i],\n",
        "                data['High'].iloc[i-1] - data['Low'].iloc[i-1],\n",
        "                data['High'].iloc[i-2] - data['Low'].iloc[i-2]\n",
        "            ]\n",
        "\n",
        "            # Calculate body ratios\n",
        "            body_ratios = [\n",
        "                abs(data['Body'].iloc[i]) / candle_lengths[0],\n",
        "                abs(data['Body'].iloc[i-1]) / candle_lengths[1],\n",
        "                abs(data['Body'].iloc[i-2]) / candle_lengths[2]\n",
        "            ]\n",
        "\n",
        "            # Check if each candle has a sufficient body ratio\n",
        "            strong_bodies = all(ratio >= min_body_ratio for ratio in body_ratios)\n",
        "\n",
        "            # Check if each candle opens within previous candle's body\n",
        "            progressive_opens = (\n",
        "                data['Open'].iloc[i] < data['Open'].iloc[i-1] and\n",
        "                data['Open'].iloc[i-1] < data['Open'].iloc[i-2]\n",
        "            )\n",
        "\n",
        "            # Check if each candle closes lower than previous candle\n",
        "            progressive_closes = (\n",
        "                data['Price'].iloc[i] < data['Price'].iloc[i-1] and\n",
        "                data['Price'].iloc[i-1] < data['Price'].iloc[i-2]\n",
        "            )\n",
        "\n",
        "            # If all conditions are met, mark as Three Black Crows\n",
        "            if strong_bodies and progressive_opens and progressive_closes:\n",
        "                three_black_crows[i] = 1\n",
        "\n",
        "    # Add the pattern to the dataframe\n",
        "    data['Three_Black_Crows'] = three_black_crows\n",
        "\n",
        "    # Count the occurrences\n",
        "    pattern_count = int(three_black_crows.sum())\n",
        "\n",
        "    return pattern_count, data['Three_Black_Crows']\n",
        "\n",
        "# Apply the detection\n",
        "pattern_count, three_black_crows = detect_three_black_crows(btc_data)\n",
        "print(f\"Number of Three Black Crows patterns detected: {pattern_count}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YLR8IOQVveOv"
      },
      "source": [
        "##Join External Data Source"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wdcMrJgHr0ag"
      },
      "outputs": [],
      "source": [
        "#Left join gold_price_data, risk_free_data, spy500 to btc_data\n",
        "btc_data = pd.merge(btc_data, gold_price_data, on='Date', how='left') #till 2025-02-03\n",
        "btc_data = pd.merge(btc_data, risk_free_data, on='Date', how='left')\n",
        "btc_data = pd.merge(btc_data, spy_data, on='Date', how='left') #till 2025-04-11\n",
        "btc_data = pd.merge(btc_data, eth_data, on='Date', how='left') #from 2015-08-07\n",
        "#As other external data have market closeure, all na filled by previous day close data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "St8vfp29ZukO"
      },
      "outputs": [],
      "source": [
        "btc_data.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "78r4m6m2wyCg"
      },
      "outputs": [],
      "source": [
        "# Plot Bitcoin Volume over date\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(btc_data['Date'], btc_data['Volume'])\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Bitcoin Volume')\n",
        "plt.ylim(bottom=-2)  # Set the lower limit to 0\n",
        "plt.title('Bitcoin Volume Over Time')\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mP5ErQLgwyCh"
      },
      "outputs": [],
      "source": [
        "btc_data['Volume'].describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LkVC0-VBwyCh"
      },
      "source": [
        "Since we can observe the spikes happened from 2018 to mid-2022, we try to fit the model avoiding the spikes here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GCleimScwyCh"
      },
      "outputs": [],
      "source": [
        "# Find the min and max of date with volume greater than 1\n",
        "btc_data[btc_data['Volume'] > 1]['Date'].min(), btc_data[btc_data['Volume'] > 1]['Date'].max()\n",
        "# Print the min and max date with volume greater than 1\n",
        "print(\"Min date with volume > 1:\", btc_data[btc_data['Volume'] > 1]['Date'].min())\n",
        "print(\"Max date with volume > 1:\", btc_data[btc_data['Volume'] > 1]['Date'].max())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iPtOUvLuwyCh"
      },
      "outputs": [],
      "source": [
        "# Drop the rows with date before 2022-05-09\n",
        "btc_data = btc_data[btc_data['Date'] >= '2022-05-09']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UA1mIe1ZwyCh"
      },
      "outputs": [],
      "source": [
        "# Plot Bitcoin Volume over date\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(btc_data['Date'], btc_data['Volume'])\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Bitcoin Volume')\n",
        "plt.ylim(bottom=-2)  # Set the lower limit to 0\n",
        "plt.title('Bitcoin Volume Over Time')\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AA0o_13tnTZW"
      },
      "source": [
        "## **Re-scale and Transformation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mjJ0Usfy00v7"
      },
      "source": [
        "**Yeo-Johnson Transformation** for Price, Change, Pct_Change"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yKJWVEl9nN7f"
      },
      "outputs": [],
      "source": [
        "# For Price, Change, 7D_Pct_Change, 2D_Pct_Change (use yeo-johnson)\n",
        "from sklearn.preprocessing import PowerTransformer\n",
        "import pandas as pd\n",
        "\n",
        "# Initialize PowerTransformer for each column\n",
        "power_transformer_price = PowerTransformer(method='yeo-johnson', standardize=False)  # Disable standardization\n",
        "power_transformer_change = PowerTransformer(method='yeo-johnson', standardize=False)\n",
        "power_transformer_7d_pct_change = PowerTransformer(method='yeo-johnson', standardize=False)\n",
        "power_transformer_2d_pct_change = PowerTransformer(method='yeo-johnson', standardize=False)\n",
        "\n",
        "# Apply the Yeo-Johnson transformation to each column\n",
        "btc_data['YeoJohnson_Price'] = power_transformer_price.fit_transform(btc_data[['Price']])\n",
        "btc_data['YeoJohnson_Change'] = power_transformer_change.fit_transform(btc_data[['Change']])\n",
        "btc_data['YeoJohnson_7D_Pct_Change'] = power_transformer_7d_pct_change.fit_transform(btc_data[['Pct_Change_7D']])\n",
        "btc_data['YeoJohnson_2D_Pct_Change'] = power_transformer_2d_pct_change.fit_transform(btc_data[['Pct_Change_2D']])\n",
        "\n",
        "# Store lambda values for each column\n",
        "lambdas = {\n",
        "    'Price_lambda': power_transformer_price.lambdas_[0],\n",
        "    'Change_lambda': power_transformer_change.lambdas_[0],\n",
        "    'Pct_Change_lambda_7D': power_transformer_7d_pct_change.lambdas_[0],\n",
        "    'Pct_Change_lambda_2D': power_transformer_2d_pct_change.lambdas_[0]\n",
        "}\n",
        "\n",
        "# Calculate mean and standard deviation for each transformed column\n",
        "transformed_means = {\n",
        "    'Price_mean': btc_data['YeoJohnson_Price'].mean(),\n",
        "    'Change_mean': btc_data['YeoJohnson_Change'].mean(),\n",
        "    'Pct_Change_mean_7D': btc_data['YeoJohnson_7D_Pct_Change'].mean(),\n",
        "    'Pct_Change_mean_2D': btc_data['YeoJohnson_2D_Pct_Change'].mean()\n",
        "}\n",
        "\n",
        "transformed_stds = {\n",
        "    'Price_std': btc_data['YeoJohnson_Price'].std(),\n",
        "    'Change_std': btc_data['YeoJohnson_Change'].std(),\n",
        "    'Pct_Change_std_7D': btc_data['YeoJohnson_7D_Pct_Change'].std(),\n",
        "    'Pct_Change_std_2D': btc_data['YeoJohnson_2D_Pct_Change'].std()\n",
        "}\n",
        "\n",
        "# Print the collected lambdas, means, and standard deviations\n",
        "print(f\"Lambdas: {lambdas}\")\n",
        "print(f\"Means: {transformed_means}\")\n",
        "print(f\"Standard Deviations: {transformed_stds}\")\n",
        "\n",
        "### Step 2: Back-Transformation ###\n",
        "# Recreate PowerTransformers with stored lambdas for back-transformation\n",
        "power_transformer_price.lambdas_ = [lambdas['Price_lambda']]\n",
        "power_transformer_change.lambdas_ = [lambdas['Change_lambda']]\n",
        "power_transformer_7d_pct_change.lambdas_ = [lambdas['Pct_Change_lambda_7D']]\n",
        "power_transformer_2d_pct_change.lambdas_ = [lambdas['Pct_Change_lambda_2D']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "keI8TIbuS-SN"
      },
      "outputs": [],
      "source": [
        "binary_variables = [\n",
        "    \"MACD_buy\",  # 1 if MACD crosses above MACD_Signal, otherwise 0\n",
        "    \"MACD_sell\",  # 1 if MACD crosses below MACD_Signal, otherwise 0\n",
        "    \"MACD_above_signal\",  # 1 if MACD > MACD_Signal, otherwise 0\n",
        "    \"MACD_below_signal\",  # 1 if MACD <= MACD_Signal, otherwise 0\n",
        "    \"BB_Buy\",  # 1 if Price crosses below the Lower Bollinger Band, otherwise 0\n",
        "    \"BB_Sell\",  # 1 if Price crosses above the Upper Bollinger Band, otherwise 0\n",
        "    \"Price_below_BB_lower\",  # 1 if Price < BB_lower, otherwise 0\n",
        "    \"Price_above_BB_upper\",  # 1 if Price > BB_upper, otherwise 0\n",
        "    \"VWAP_Buy\",  # 1 if Price crosses above VWAP, otherwise 0\n",
        "    \"VWAP_Sell\",  # 1 if Price crosses below VWAP, otherwise 0\n",
        "    \"RSI_12_Oversold\",  # 1 if RSI_12 < 30, otherwise 0\n",
        "    \"RSI_12_Overbought\",  # 1 if RSI_12 > 70, otherwise 0\n",
        "    \"RSI_Divergence\",  # 1 if RSI_6 > RSI_12, otherwise 0\n",
        "    \"Positive_7D\",  # 1 if Pct_Change_7D > 0, otherwise 0\n",
        "    \"Positive_2D\",  # 1 if Pct_Change_2D > 0, otherwise 0\n",
        "    \"Higher_Low\",  # 1 if current low > previous low, otherwise 0\n",
        "    \"Breakout\",  # 1 if Price > Resistance_Level, otherwise 0\n",
        "    \"Ascending_Triangle_Breakout\",  # Same as Breakout\n",
        "    \"Breakout_With_Volume\",  # 1 if breakout with volume confirmation, otherwise 0\n",
        "    \"Ascending_Triangle_Breakout_With_Volume\",  # Same as Breakout_With_Volume\n",
        "    \"ADX_Buy_Signal\",  # 1 if ADX > 25 and Positive_DI > Negative_DI, otherwise 0\n",
        "    \"ADX_Sell_Signal\",  # 1 if ADX > 25 and Negative_DI > Positive_DI, otherwise 0\n",
        "    \"Stochastic_Buy_Signal\",  # 1 if Stochastic %K < 20 and %K > %D, otherwise 0\n",
        "    \"Stochastic_Sell_Signal\",  # 1 if Stochastic %K > 80 and %K < %D, otherwise 0\n",
        "    \"Bullish_Engulfing\",  # 1 if Bullish Engulfing pattern is detected, otherwise 0\n",
        "    \"Bearish_Engulfing\",  # 1 if Bearish Engulfing pattern is detected, otherwise 0\n",
        "    \"Hammer\",  # 1 if Hammer candlestick pattern is detected, otherwise 0\n",
        "    \"Three_White_Soldiers\",  # 1 if Three White Soldiers pattern is detected, otherwise 0\n",
        "    \"Three_Black_Crows\", # 1 if Three Black Crows pattern is detected, otherwise 0\n",
        "    'Extreme_Fear', 'Extreme_Greed'\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0e46zGXtS-K3"
      },
      "outputs": [],
      "source": [
        "continuous_variables = [\n",
        "    'Price', 'Open', 'High', 'Low', 'Change',\n",
        "    \"RSI_6\",  # Relative Strength Index with a 6-period length\n",
        "    \"RSI_12\",  # Relative Strength Index with a 12-period length\n",
        "    \"EMA_14\",  # Exponential Moving Average with a 14-period length\n",
        "    \"SMA_14\",  # Simple Moving Average with a 14-period length\n",
        "    \"OBV\",  # On-Balance Volume\n",
        "    \"MACD\",  # Moving Average Convergence Divergence\n",
        "    \"MACD_Signal\",  # Signal line of MACD\n",
        "    \"MACD_cum_buy\",  # Cumulative count of consecutive days where MACD_above_signal is 1\n",
        "    \"MACD_cum_sell\",  # Cumulative count of consecutive days where MACD_below_signal is 1\n",
        "    \"BB_upper\",  # Upper Bollinger Band\n",
        "    \"BB_lower\",  # Lower Bollinger Band\n",
        "    \"BB_cum_Buy\",  # Cumulative count of consecutive days where Price_below_BB_lower is 1\n",
        "    \"BB_cum_Sell\",  # Cumulative count of consecutive days where Price_above_BB_upper is 1\n",
        "    \"ATR\",  # Average True Range\n",
        "    \"ATR_Ratio\",  # ATR as a percentage of Price\n",
        "    \"Stop_Loss_Long\",  # Stop-loss level for long trades\n",
        "    \"Stop_Loss_Short\",  # Stop-loss level for short trades\n",
        "    \"VWAP\",  # Volume Weighted Average Price\n",
        "    \"Support\",  # Lowest low in the past 20 days\n",
        "    \"Resistance\",  # Highest high in the past 20 days\n",
        "    \"Consecutive_RSI_12_Overbought\",  # Cumulative count of consecutive days where RSI_12_Overbought is 1\n",
        "    \"Consecutive_RSI_12_Oversold\",  # Cumulative count of consecutive days where RSI_12_Oversold is 1\n",
        "    \"Pct_Change_7D\",  # Percentage price change over the next 7 days\n",
        "    \"Pct_Change_2D\",  # Percentage price change over the next 2 days\n",
        "    \"MA50\",  # 50-day moving average\n",
        "    \"MA200\",  # 200-day moving average\n",
        "    \"Price_vs_MA50\",  # Price relative to 50-day MA (%)\n",
        "    \"Price_vs_MA200\",  # Price relative to 200-day MA (%)\n",
        "    \"MA_Distance_Ratio\",  # Ratio of MA50 to MA200 distance\n",
        "    \"Avg_Volume_20\",  # 20-day average volume\n",
        "    \"Volume_Ratio\",  # Current volume / 20-day avg volume\n",
        "    \"OBV_Slope\",  # 5-day slope of OBV\n",
        "    \"Resistance_Level\",  # Exponential moving average of high price\n",
        "    \"Avg_Volume\",  # 7-day rolling average of volume\n",
        "    \"Triangle_Completion\",  # Rolling mean of Higher_Low\n",
        "    \"Triangle_Height\",  # Height of the triangle as % of price\n",
        "    \"Triangle_Duration\",  # Sum of Higher_Low over 7 days\n",
        "    \"Breakout_Strength\",  # Volume-adjusted breakout strength\n",
        "    \"ADX\",  # Average Directional Index\n",
        "    \"Positive_DI\",  # Positive Directional Index\n",
        "    \"Negative_DI\",  # Negative Directional Index\n",
        "    \"Consecutive_ADX_Buy\",  # Consecutive days of ADX_Buy_Signal\n",
        "    \"Consecutive_ADX_Sell\",  # Consecutive days of ADX_Sell_Signal\n",
        "    \"ADX_Strength\",  # How far ADX is above the threshold\n",
        "    \"ADX_Trend_Duration\",  # Consecutive days where ADX > 25\n",
        "    \"ADX_Trend_Direction\",  # Difference between Positive_DI and Negative_DI\n",
        "    \"Stochastic_perK\",  # %K line of the Stochastic Oscillator\n",
        "    \"Stochastic_perD\",  # %D line of the Stochastic Oscillator\n",
        "    \"Consecutive_Stochastic_Buy\",  # Consecutive days of Stochastic_Buy_Signal\n",
        "    \"Consecutive_Stochastic_Sell\",  # Consecutive days of Stochastic_Sell_Signal\n",
        "    \"Consecutive_Hammer\",  # Consecutive hammer patterns\n",
        "    \"SPY_Price\",  # Price of SPY (S&P 500 ETF)\n",
        "    \"SPY_Volume\",  # Trading volume of SPY\n",
        "    \"ETH_Price\",  # Price of Ethereum (ETH)\n",
        "    \"ETH_Volume\",  # Trading volume of Ethereum (ETH)\n",
        "    \"Gold_Price\",  # Price of gold\n",
        "    \"Gold_Volume\",  # Trading volume of gold\n",
        "    \"US_10Y\",  # Yield of the US 10-year Treasury bond\n",
        "    \"Yield_Spread\",  # Spread between different bond yields\n",
        "    \"YeoJohnson_Price\",  # Yeo-Johnson transformed Price\n",
        "    \"YeoJohnson_Change\",  # Yeo-Johnson transformed Change\n",
        "    \"YeoJohnson_7D_Pct_Change\",  # Yeo-Johnson transformed Pct_Change_7D\n",
        "    \"YeoJohnson_2D_Pct_Change\", # Yeo-Johnson transformed Pct_Change_2D\n",
        "    'Fear_and_Greed_Index', 'Consecutive_Extreme_Fear', 'Consecutive_Extreme_Greed'\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RKA_3cMOS-CB"
      },
      "outputs": [],
      "source": [
        "# Set binary variables to int64\n",
        "btc_data[binary_variables] = btc_data[binary_variables].astype(\"int64\")\n",
        "\n",
        "# Set continuous variables to float64\n",
        "btc_data[continuous_variables] = btc_data[continuous_variables].astype(\"float64\")\n",
        "\n",
        "# Verify the data types\n",
        "for column in btc_data.columns:\n",
        "    print(f\"{column}: {btc_data[column].dtype}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f-TX3OjvnvEU"
      },
      "outputs": [],
      "source": [
        "numerical_variable = btc_data.select_dtypes(include=['number'])\n",
        "btc_data.columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oy-QyHrV0Q84"
      },
      "source": [
        "**Standardize all numeric variable**\n",
        "\n",
        "store to scaled_btc_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MZ0Euw5rILY-"
      },
      "outputs": [],
      "source": [
        "# Define count columns\n",
        "count_columns = [ 'MACD_Signal', 'MACD_buy', 'MACD_sell', 'MACD_above_signal', 'MACD_cum_buy', 'MACD_cum_sell', 'MACD_below_signal', 'BB_Buy', 'BB_Sell', 'Price_below_BB_lower', 'Price_above_BB_upper',\n",
        "                 'BB_cum_Buy', 'BB_cum_Sell', 'VWAP_Buy', 'VWAP_Sell', 'Price_above_VWAP', 'Price_below_VWAP', 'VWAP_cum_Buy', 'VWAP_cum_Sell', 'RSI12_Oversold' ,'Consecutive_RSI_12_Oversold',\n",
        "                 'RSI_12_Overbought', 'RSI_Divergence', 'Consecutive_RSI_12_Overbought', 'Ascending_Triangle_Breakout', 'Ascending_Triangle_Breakout_With_Volume', 'ADX_Buy_Signal', 'ADX_Sell_Signal', 'Consecutive_ADX_Buy',\n",
        "                 'Consecutive_ADX_Sell', 'Stochastic_Buy_Signal', 'Stochastic_Sell_Signal', 'Consecutive_Stochastic_Buy', 'Consecutive_Stochastic_Sell', 'Bullish_Engulfing', 'Bearish_Engulfing', 'Hammer', 'Consecutive_Hammer',\n",
        "                 'Three_white_soldiers', 'Three_Black_Crows', 'Fear_and_Greed_Index', 'Extreme_Fear', 'Extreme_Greed', 'Consecutive_Extreme_Fear', 'Consecutive_Extreme_Greed'\n",
        "                 ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2PtKNmtQngxy"
      },
      "outputs": [],
      "source": [
        "#Standardize all numeric variable\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "\n",
        "# Initialize the MinMaxScaler and StandardScaler\n",
        "min_max_scaler = MinMaxScaler()\n",
        "standard_scaler = StandardScaler()\n",
        "\n",
        "# Apply log transformation to 'Volume' to reduce the range\n",
        "btc_data['Volume'] = np.log1p(btc_data['Volume'])  # log1p handles log(0) safely\n",
        "\n",
        "# Apply Scaler\n",
        "btc_data['Volume'] = min_max_scaler.fit_transform(btc_data[['Volume']])\n",
        "btc_data['Volume'] = standard_scaler.fit_transform(btc_data[['Volume']])\n",
        "\n",
        "# List of features for scaling\n",
        "numeric_features = continuous_variables\n",
        "\n",
        "# Fit and transform only the numeric features\n",
        "scaled_features = standard_scaler.fit_transform(btc_data[numeric_features])\n",
        "\n",
        "# Create a new DataFrame for scaled data\n",
        "scaled_btc_data = btc_data.copy()  # Copy original DataFrame\n",
        "\n",
        "# Replace numeric columns in the new DataFrame with scaled values\n",
        "scaled_btc_data[numeric_features] = scaled_features\n",
        "\n",
        "scaled_btc_data.tail()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uOlP23hMTjta"
      },
      "outputs": [],
      "source": [
        "#numeric_features = [\n",
        "  #'Price', 'Open', 'High', 'Low', 'Change', 'RSI_6', 'RSI_12', 'EMA_14',\n",
        "  #'SMA_14', 'OBV', 'MACD', 'MACD_Signal','MACD_above_signal', 'MACD_below_signal','MACD_cum_buy', 'MACD_cum_sell',\n",
        "  #'BB_lower', 'BB_upper','Price_below_BB_lower', 'Price_above_BB_upper', 'BB_cum_Buy', 'BB_cum_Sell',\n",
        "  #'ATR', 'Stop_Loss_Long', 'Stop_Loss_Short', 'VWAP','Support', 'Resistance', 'YeoJohnson_Price',\n",
        "  #'YeoJohnson_Change','YeoJohnson_7D_Pct_Change', 'YeoJohnson_2D_Pct_Change', 'Gold_Price', 'Gold_Volume',\n",
        "  #'US_10Y', 'Yield_Spread','Triangle_Completion', 'Triangle_Height', 'Breakout_Strength',\n",
        "  #'ADX_Strength', 'ATR_Ratio', 'Price_vs_MA50', 'Price_vs_MA200', 'MA_Distance_Ratio', 'OBV_Slope',\n",
        "  #'Stochastic_perK', 'Stochastic_perD', 'SPY_Price', 'SPY_Volume', 'ETH_Price', 'ETH_Volume',\n",
        "  #'Fear_and_Greed_Index', 'Extreme_Fear', 'Extreme_Greed', 'Consecutive_Extreme_Fear', 'Consecutive_Extreme_Greed']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mVtmTpdKpGOC"
      },
      "outputs": [],
      "source": [
        "scaled_btc_data.describe().T"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S9dUM_dbs0p1"
      },
      "source": [
        "# **Please use scaled_btc_data for analysis**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vexswtnEIcvp"
      },
      "outputs": [],
      "source": [
        "scaled_btc_data.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RXMA930702Go"
      },
      "source": [
        "**Drop NA before Analysis**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R04OuOWmIc8Q"
      },
      "outputs": [],
      "source": [
        "# Drop rows with NaN values in scaled_btc_data\n",
        "scaled_btc_data = scaled_btc_data.dropna()\n",
        "\n",
        "# Optionally, reset the index after dropping rows\n",
        "scaled_btc_data = scaled_btc_data.reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DZlthiApIiON"
      },
      "outputs": [],
      "source": [
        "scaled_btc_data.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pxz2sLmEIjNf"
      },
      "outputs": [],
      "source": [
        "#Check data are consecutive\n",
        "# Ensure 'Date' is sorted\n",
        "scaled_btc_data = scaled_btc_data.sort_values('Date')\n",
        "\n",
        "# Calculate the difference between consecutive dates\n",
        "scaled_btc_data['Date_Diff'] = scaled_btc_data['Date'].diff().dt.days\n",
        "\n",
        "# Check if all differences are 1 day\n",
        "if (scaled_btc_data['Date_Diff'].iloc[1:] == 1).all():\n",
        "    print(\"All dates are consecutive.\")\n",
        "else:\n",
        "    print(\"There are missing dates.\")\n",
        "    print(scaled_btc_data[scaled_btc_data['Date_Diff'] > 1])  # Display rows with missing dates\n",
        "\n",
        "# Drop the 'Date_Diff' column after the check\n",
        "scaled_btc_data.drop(columns=['Date_Diff'], inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HqrQEBBYIkt2"
      },
      "outputs": [],
      "source": [
        "# Check the start and end dates\n",
        "start_date = scaled_btc_data['Date'].min()\n",
        "end_date = scaled_btc_data['Date'].max()\n",
        "\n",
        "print(f\"Data starts from: {start_date}\")\n",
        "print(f\"Data ends at: {end_date}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b55IBivgGgmg"
      },
      "source": [
        "#Exploratory Data Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5KR7Uq27lAuY"
      },
      "outputs": [],
      "source": [
        "#Print the name of the columns\n",
        "print(scaled_btc_data.columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fi7LeczFpcJr"
      },
      "outputs": [],
      "source": [
        "# Moving date to index\n",
        "scaled_btc_data.set_index('Date', inplace=True)\n",
        "\n",
        "target = 'YeoJohnson_7D_Pct_Change'\n",
        "# target_binary = 'Positive_2D'\n",
        "target_binary = 'Positive_7D'\n",
        "\n",
        "exclude_list = ['Price', 'Open', 'High', 'Low', 'Change', 'Day_of_Week', 'Week_of_Year', 'Month','Quarter','Year',\n",
        "                'VWAP', 'VWAP_Buy', 'VWAP_Sell', 'Resistance_Level', 'Low_Shifted', 'Higher_Low',\n",
        "                'Breakout', 'Ascending_Triangle_Breakout', 'Avg_Volume', 'Breakout_With_Volume', 'Avg_Volume_20','Pct_Change_7D', 'Pct_Change_2D','Positive_2D','YeoJohnson_Price','YeoJohnson_Change','YeoJohnson_2D_Pct_Change']\n",
        "# Select binary columns\n",
        "binary_variables = scaled_btc_data.drop(columns=exclude_list+[target,target_binary]).select_dtypes(include=['int64']).columns.tolist()\n",
        "\n",
        "# Identify 2-dimensional columns\n",
        "for column in binary_variables:\n",
        "    if len(scaled_btc_data[column].shape) != 1:  # Check if the column is not 1D\n",
        "        print(f\"Column '{column}' is 2-dimensional with shape {scaled_btc_data[column].shape}\")\n",
        "\n",
        "# Filter out 2D columns and remove target from predictors\n",
        "binary_variables = [col for col in binary_variables if len(scaled_btc_data[col].shape) == 1 and col != target]\n",
        "\n",
        "\n",
        "# Define predictors, all columns except the 7D_Pct_Change or YeoJohnson_7D_Pct_Change or 2D_Pct_Change or YeoJohnson_2D_Pct_Change\n",
        "predictors = scaled_btc_data.drop(columns=[target, target_binary] + exclude_list + binary_variables, axis=1).columns.tolist()\n",
        "\n",
        "# Display the first few rows of the DataFrame\n",
        "scaled_btc_data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IBuAYn0IlAuY"
      },
      "outputs": [],
      "source": [
        "# Check the predictors\n",
        "predictors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U0XLUiYKHVWQ"
      },
      "source": [
        "##Univariate Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XN-D0AY5HZSL"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "cols = predictors[1:] + [target]\n",
        "ncols = 5\n",
        "nrows = math.ceil(len(cols) / ncols)\n",
        "\n",
        "fig, axes = plt.subplots(nrows, ncols, figsize=(20, 4 * nrows))\n",
        "axes = axes.flatten()\n",
        "\n",
        "for ax, col in zip(axes, cols):\n",
        "    scaled_btc_data[col].hist(\n",
        "        bins=30,\n",
        "        ax=ax,\n",
        "        edgecolor='black'\n",
        "    )\n",
        "    ax.set_title(col)\n",
        "    ax.set_xlabel('')\n",
        "    ax.set_ylabel('')\n",
        "\n",
        "# turn off any unused subplots\n",
        "for ax in axes[len(cols):]:\n",
        "    fig.delaxes(ax)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yE1HSaF4HZt0"
      },
      "source": [
        "##Bivariate Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K9z1Q8Wgt8nN"
      },
      "outputs": [],
      "source": [
        "# Select numeric columns\n",
        "numeric_variables = scaled_btc_data.select_dtypes(include=['float64']).columns.tolist()\n",
        "\n",
        "# Identify 2-dimensional columns\n",
        "for column in numeric_variables:\n",
        "    if len(scaled_btc_data[column].shape) != 1:  # Check if the column is not 1D\n",
        "        print(f\"Column '{column}' is 2-dimensional with shape {scaled_btc_data[column].shape}\")\n",
        "\n",
        "# Filter out 2D columns and remove target from predictors\n",
        "numeric_variables = [col for col in numeric_variables if len(scaled_btc_data[col].shape) == 1 and col != target]\n",
        "\n",
        "# Drop rows with missing values\n",
        "data_numeric = scaled_btc_data[numeric_variables + [target]].dropna()\n",
        "\n",
        "# Define number of rows and columns for the subplot grid\n",
        "n_cols = 3\n",
        "n_rows = (len(numeric_variables) + n_cols - 1) // n_cols  # Ceiling division\n",
        "\n",
        "# Create figure with subplots\n",
        "fig = plt.figure(figsize=(15, 5 * n_rows))\n",
        "\n",
        "# Create scatter plots\n",
        "for i, column in enumerate(numeric_variables, 1):\n",
        "    ax = plt.subplot(n_rows, n_cols, i)\n",
        "    sns.scatterplot(data=data_numeric, x=column, y=target, ax=ax)\n",
        "    ax.set_title(f\"{column} vs {target}\")\n",
        "    ax.set_xlabel(column)\n",
        "    ax.set_ylabel(target)\n",
        "\n",
        "    # Rotate x-axis labels if they're too long\n",
        "    plt.xticks(rotation=45)\n",
        "\n",
        "# Adjust layout to prevent overlap\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LdtI_-7wugDu"
      },
      "outputs": [],
      "source": [
        "# Create correlation matrix\n",
        "correlation_matrix = data_numeric.corr()\n",
        "\n",
        "plt.figure(figsize=(20, 20))\n",
        "\n",
        "# Create heatmap with annotations\n",
        "sns.heatmap(correlation_matrix,\n",
        "            annot=True,\n",
        "            cmap='coolwarm',\n",
        "            center=0,\n",
        "            square=True,\n",
        "            fmt='.2f',\n",
        "            linewidths=0.5,\n",
        "            cbar_kws={\"shrink\": .5})\n",
        "\n",
        "plt.title('Correlation Heatmap: Numeric Variables vs Price Change', pad=20)\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.yticks(rotation=0)\n",
        "plt.tight_layout()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qmB9ZBX4qeb8"
      },
      "outputs": [],
      "source": [
        "# Drop rows with missing values\n",
        "data_binary = scaled_btc_data[binary_variables + [target]].dropna()\n",
        "\n",
        "# Define number of rows and columns for the subplot grid\n",
        "n_cols = 3\n",
        "n_rows = (len(binary_variables) + n_cols - 1) // n_cols  # Ceiling division\n",
        "\n",
        "# Create figure with subplots\n",
        "fig = plt.figure(figsize=(15, 5 * n_rows))\n",
        "\n",
        "# Create scatter plots\n",
        "for i, column in enumerate(binary_variables, 1):\n",
        "    ax = plt.subplot(n_rows, n_cols, i)\n",
        "    sns.scatterplot(data=data_binary, x=column, y=target, ax=ax)\n",
        "    ax.set_title(f\"{column} vs {target}\")\n",
        "    ax.set_xlabel(column)\n",
        "    ax.set_ylabel(target)\n",
        "\n",
        "    # Rotate x-axis labels if they're too long\n",
        "    plt.xticks(rotation=45)\n",
        "\n",
        "# Adjust layout to prevent overlap\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Y9u2_ldsDjY"
      },
      "outputs": [],
      "source": [
        "# Create correlation matrix\n",
        "correlation_matrix = data_binary.corr()\n",
        "\n",
        "plt.figure(figsize=(12, 10))\n",
        "\n",
        "# Create heatmap with annotations\n",
        "sns.heatmap(correlation_matrix,\n",
        "            annot=True,\n",
        "            cmap='coolwarm',\n",
        "            center=0,\n",
        "            square=True,\n",
        "            fmt='.3f',\n",
        "            linewidths=0.5,\n",
        "            cbar_kws={\"shrink\": .5})\n",
        "\n",
        "plt.title('Correlation Heatmap: Binary Variables vs Price Change', pad=20)\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.yticks(rotation=0)\n",
        "plt.tight_layout()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xVqfeS8kIIV7"
      },
      "source": [
        "# Model Building"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bm9dvWirVqQZ"
      },
      "source": [
        "- Time-Based Splitting\n",
        "- Selecting predictors\n",
        "- Interaction terms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j22H2a1Fxb5t"
      },
      "outputs": [],
      "source": [
        "from itertools import chain, combinations\n",
        "\n",
        "def gen_combinations(input):\n",
        "    return sum([list(map(list, combinations(input, i))) for i in range(1,len(input) + 1)], [])\n",
        "\n",
        "def gen_interactions(X,de,pname):\n",
        "    s = gen_combinations(range(len(pname)))\n",
        "    VX = []\n",
        "    VarX = []\n",
        "    for i in range(len(s)):\n",
        "        if len(s[i]) <= de:\n",
        "            VX.append(np.prod(X[:,s[i]],axis=1))\n",
        "            VarX.append(list([pname[x] for x in s[i]]))\n",
        "    VarX2 = []\n",
        "    for x in VarX:\n",
        "        if len(x) > 1:\n",
        "            VarX2.append('_'.join(x))\n",
        "        else:\n",
        "            VarX2.append(x[0])\n",
        "    VX = pd.DataFrame(np.array(VX).T,columns=VarX2)\n",
        "    return VX, VarX2\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uNDQMhX0rS0B"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import itertools\n",
        "\n",
        "# Define feature groups\n",
        "feature_groups = {\n",
        "    'Group 1': [\n",
        "        'Negative_DI', 'Positive_DI', 'ADX_Buy_Signal', 'ADX_Sell_Signal', 'ADX_Strength',\n",
        "        'ADX_Trend_Direction', 'ADX_Trend_Duration', 'ADX'\n",
        "    ],\n",
        "    'Group 2': [\n",
        "        'Ascending_Triangle_Breakout_With_Volume', 'Triangle_Completion', 'Triangle_Duration', 'Triangle_Height'\n",
        "    ],\n",
        "    'Group 3': [\n",
        "        'ATR_Ratio', 'ATR'\n",
        "    ],\n",
        "    'Group 4': [\n",
        "        'BB_Buy', 'BB_cum_Buy', 'BB_cum_Sell', 'BB_lower', 'BB_Sell', 'BB_upper',\n",
        "        'Price_above_BB_upper', 'Price_below_BB_lower'\n",
        "    ],\n",
        "    'Group 7': [\n",
        "        'Consecutive_RSI_12_Overbought', 'Consecutive_RSI_12_Oversold', 'RSI_12_Overbought',\n",
        "        'RSI_12_Oversold', 'RSI_12', 'RSI_6', 'RSI_Divergence'\n",
        "    ],\n",
        "    'Group 8': [\n",
        "        'Consecutive_Stochastic_Buy', 'Consecutive_Stochastic_Sell', 'Stochastic_perD',\n",
        "        'Stochastic_perK', 'Stochastic_Buy_Signal', 'Stochastic_Sell_Signal'\n",
        "    ],\n",
        "    'Group 11': [\n",
        "        'MA_Distance_Ratio', 'MA200', 'MA50', 'MACD_above_signal', 'MACD_below_signal',\n",
        "        'MACD_buy', 'MACD_cum_buy', 'MACD_cum_sell', 'MACD_sell', 'MACD_Signal',\n",
        "        'MACD', 'Price_vs_MA200', 'Price_vs_MA50'\n",
        "    ],\n",
        "    'Group 12': [\n",
        "        'OBV_Slope', 'OBV', 'Resistance', 'Support', 'Volume_Ratio', 'Volume'\n",
        "    ],\n",
        "    'Group 13': [\n",
        "        'SPY_Price', 'SPY_Volume'\n",
        "    ],\n",
        "    'Group 14': [\n",
        "        'US_10Y', 'Yield_Spread','Fear_and_Greed_Index', 'Extreme_Fear', 'Extreme_Greed', 'Consecutive_Extreme_Fear', 'Consecutive_Extreme_Greed'\n",
        "    ],\n",
        "    'Group 15': [\n",
        "        'Gold_Price', 'Gold_Volume'\n",
        "    ],\n",
        "    'Group 16': [\n",
        "        'ETH_Price', 'ETH_Volume'\n",
        "    ]\n",
        "}\n",
        "\n",
        "# Define interaction rules (between groups)\n",
        "interaction_rules = [\n",
        "    ('Group 1', 'Group 2'),  # ADX x Triangle Patterns\n",
        "    ('Group 3', 'Group 4'),  # ATR x Bollinger Bands\n",
        "    ('Group 7', 'Group 8'),  # RSI x Stochastic Oscillator\n",
        "    ('Group 11', 'Group 12'),  # MACD x Volume and Support/Resistance\n",
        "    ('Group 14', 'Group 13'),  # Macro Indicators x SPY\n",
        "    ('Group 15', 'Group 16')  # Gold x ETH\n",
        "]\n",
        "\n",
        "# Initialize an empty DataFrame for interaction terms\n",
        "interaction_terms = pd.DataFrame(index=scaled_btc_data.index)\n",
        "\n",
        "# Generate between-group interaction terms\n",
        "for group_a, group_b in interaction_rules:\n",
        "    print(f\"Generating interactions between {group_a} and {group_b}...\")\n",
        "    features_a = feature_groups[group_a]\n",
        "    features_b = feature_groups[group_b]\n",
        "    for feature_a, feature_b in itertools.product(features_a, features_b):  # All combinations between groups\n",
        "        interaction_name = f\"{feature_a}_x_{feature_b}\"\n",
        "        interaction_terms[interaction_name] = scaled_btc_data[feature_a] * scaled_btc_data[feature_b]\n",
        "\n",
        "\n",
        "interaction_list = interaction_terms.columns.tolist()\n",
        "# Add interaction terms to the original DataFrame\n",
        "scaled_btc_data_with_interactions = scaled_btc_data.join(interaction_terms)\n",
        "\n",
        "scaled_btc_data_with_interactions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s0xNlgt0T3I5"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "# Generate within-group interaction terms\n",
        "for group_name, features in feature_groups.items():\n",
        "    print(f\"Generating within-group interactions for {group_name}...\")\n",
        "    for feature_a, feature_b in itertools.combinations(features, 2):  # All unique pairs within the group\n",
        "        interaction_name = f\"{feature_a}_x_{feature_b}\"\n",
        "        interaction_terms[interaction_name] = scaled_btc_data[feature_a] * scaled_btc_data[feature_b]\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1XAV9LIN7kWU"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "# 1) Select and clean your base predictors '''\n",
        "relevant_predictors = [\n",
        "    'VWAP_Buy', 'MACD_buy', 'BB_Buy',\n",
        "    'RSI_12_Oversold', 'RSI_Divergence',\n",
        "    'VWAP', 'MACD', 'RSI_6', 'RSI_12', 'ATR'\n",
        "]\n",
        "df_base = scaled_btc_data[relevant_predictors].dropna()\n",
        "\n",
        "# 2) Generate all up-to-2-way interactions\n",
        "X       = df_base.values\n",
        "pnames  = df_base.columns.tolist()\n",
        "inter_df, inter_names = gen_interactions(X, de=2, pname=pnames)\n",
        "\n",
        "# 3) Build a set for quick membership tests\n",
        "excluded = { tuple(sorted(pair)) for pair in [\n",
        "    ('VWAP_Buy', 'VWAP_Sell'),\n",
        "    ('MACD_buy','MACD_sell'),\n",
        "    ('BB_Buy','BB_Sell'),\n",
        "]}\n",
        "\n",
        "# 4) Keep only the interactions we want\n",
        "keep_mask = [\n",
        "    not (len(name.split('_')) == 2\n",
        "         and tuple(sorted(name.split('_'))) in excluded)\n",
        "    for name in inter_names\n",
        "]\n",
        "valid_names = [name for name, keep in zip(inter_names, keep_mask) if keep]\n",
        "\n",
        "# 5) Subset and align with the original index\n",
        "inter_df = inter_df.loc[:, valid_names]\n",
        "inter_df.index = df_base.index\n",
        "\n",
        "# 6) Add a suffix to interaction columns to avoid overlap\n",
        "inter_df = inter_df.add_suffix('_interaction')\n",
        "\n",
        "# Join back onto your full DataFrame\n",
        "scaled_btc_data_with_interactions = scaled_btc_data.join(inter_df)\n",
        "scaled_btc_data_with_interactions\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4rcfqiT_yK9f"
      },
      "source": [
        "## Splitting the data into training, testing and validation datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8IeSTVB9bumJ"
      },
      "source": [
        "###5102"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cgx2HkU_HYr-"
      },
      "outputs": [],
      "source": [
        "final_df = scaled_btc_data_with_interactions.copy()\n",
        "X = final_df[predictors + interaction_list + binary_variables]  # Define predictors\n",
        "y = final_df[target]  # Define target variable\n",
        "\n",
        "\n",
        "X = X.apply(pd.to_numeric, errors='coerce')\n",
        "y = pd.to_numeric(y, errors='coerce')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZxlcxK1eybPG"
      },
      "outputs": [],
      "source": [
        "# Splitting data (70% train, 15% validation, 15% test)\n",
        "train_size = int(len(final_df) * 0.7)\n",
        "val_size = int(len(final_df) * 0.15)\n",
        "\n",
        "# Time-based splits\n",
        "X_train = X.iloc[:train_size]\n",
        "y_train = y.iloc[:train_size]\n",
        "\n",
        "X_val = X.iloc[train_size : train_size + val_size]\n",
        "y_val = y.iloc[train_size : train_size + val_size]\n",
        "\n",
        "X_test = X.iloc[train_size + val_size :]\n",
        "y_test = y.iloc[train_size + val_size :]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ByQWDUplbx_w"
      },
      "source": [
        "###5104"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lkd0FEqub-HK"
      },
      "outputs": [],
      "source": [
        "# Create a new dataframe for binary classification\n",
        "final_df_2 = scaled_btc_data_with_interactions.copy()\n",
        "\n",
        "# Define binary target\n",
        "\n",
        "\n",
        "print(final_df_2[target_binary].value_counts())\n",
        "\n",
        "# Define predictors for classification\n",
        "\n",
        "final_df_2 = scaled_btc_data_with_interactions.copy()\n",
        "X_2 = final_df_2[predictors + binary_variables + interaction_list]  # Independent variables\n",
        "y_2 = final_df_2[target_binary]  # Target variable\n",
        "\n",
        "y_2 = pd.to_numeric(y_2, errors='coerce')\n",
        "\n",
        "\n",
        "# Time-based splitting (70% train, 15% validation, 15% test)\n",
        "train_size = int(len(final_df_2) * 0.7)\n",
        "val_size = int(len(final_df_2) * 0.15)\n",
        "\n",
        "# Create the splits for binary classification\n",
        "X_train_2 = X_2.iloc[:train_size]\n",
        "y_train_2 = y_2.iloc[:train_size]\n",
        "\n",
        "X_val_2 = X_2.iloc[train_size:train_size + val_size]\n",
        "y_val_2 = y_2.iloc[train_size:train_size + val_size]\n",
        "\n",
        "X_test_2 = X_2.iloc[train_size + val_size:]\n",
        "y_test_2 = y_2.iloc[train_size + val_size:]\n",
        "\n",
        "print(f\"Binary classification data shape: {X_2.shape}\")\n",
        "print(f\"Training set: {X_train_2.shape[0]} samples\")\n",
        "print(f\"Validation set: {X_val_2.shape[0]} samples\")\n",
        "print(f\"Test set: {X_test_2.shape[0]} samples\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wihir_uoypKo"
      },
      "source": [
        "## Defining functions for model performance evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E0I5Pm06ahzj"
      },
      "source": [
        "###5102"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YYyJhr8GywSo"
      },
      "outputs": [],
      "source": [
        "def adj_r2_score(predictors, targets, predictions):\n",
        "    r2 = r2_score(targets, predictions)\n",
        "    n = predictors.shape[0]\n",
        "    k = predictors.shape[1]\n",
        "    return 1 - ((1 - r2) * (n - 1) / (n - k - 1))\n",
        "\n",
        "\n",
        "# function to compute MAPE\n",
        "def mape_score(targets, predictions):\n",
        "    return np.mean(np.abs(targets - predictions) / targets) * 100\n",
        "\n",
        "\n",
        "# function to compute different metrics to check performance of a regression model\n",
        "def model_performance_regression(model, predictors, target):\n",
        "    \"\"\"\n",
        "    Function to compute different metrics to check regression model performance\n",
        "\n",
        "    model: regressor\n",
        "    predictors: independent variables\n",
        "    target: dependent variable\n",
        "    \"\"\"\n",
        "\n",
        "    # predicting using the independent variables\n",
        "    pred = model.predict(predictors)\n",
        "\n",
        "    r2 = r2_score(target, pred)  # to compute R-squared\n",
        "    adjr2 = adj_r2_score(predictors, target, pred)  # to compute adjusted R-squared\n",
        "    rmse = np.sqrt(mean_squared_error(target, pred))  # to compute RMSE\n",
        "    mae = mean_absolute_error(target, pred)  # to compute MAE\n",
        "    mape = mape_score(target, pred)  # to compute MAPE\n",
        "\n",
        "    # creating a dataframe of metrics\n",
        "    df_perf = pd.DataFrame(\n",
        "        {\n",
        "            \"RMSE\": rmse,\n",
        "            \"MAE\": mae,\n",
        "            \"R-squared\": r2,\n",
        "            \"Adj. R-squared\": adjr2,\n",
        "            \"MAPE\": mape,\n",
        "        },\n",
        "        index=[0],\n",
        "    )\n",
        "\n",
        "    return df_perf\n",
        "\n",
        "# RMSE\n",
        "def rmse(predictions, targets):\n",
        "    return np.sqrt(((targets - predictions) ** 2).mean())\n",
        "\n",
        "\n",
        "# MAPE\n",
        "def mape(predictions, targets):\n",
        "    return np.mean(np.abs((targets - predictions)) / targets) * 100\n",
        "\n",
        "\n",
        "# MAE\n",
        "def mae(predictions, targets):\n",
        "    return np.mean(np.abs((targets - predictions)))\n",
        "\n",
        "# To see the feature importance of variables in the final model\n",
        "def feature_importances(model, feature_names, n=10):\n",
        "    if isinstance(model,LinearRegression):\n",
        "        importances = model.coef_\n",
        "    else:\n",
        "        importances = model.feature_importances_\n",
        "    zipped = sorted(zip(feature_names, importances), key=lambda x: -x[1])\n",
        "    for i, f in enumerate(zipped[:n]):\n",
        "        print(\"%d: Feature: %s, %.3f\" % (i+1, f[0], f[1]))\n",
        "\n",
        "\n",
        "# Model Performance on test and train data\n",
        "def model_pref(olsmodel, x_train, x_test, y_train,y_test):\n",
        "\n",
        "    # Insample Prediction\n",
        "    y_pred_train = olsmodel.predict(x_train)\n",
        "    y_observed_train = y_train\n",
        "\n",
        "    # Prediction on test data\n",
        "    y_pred_test = olsmodel.predict(x_test)\n",
        "    y_observed_test = y_test\n",
        "\n",
        "    print(\n",
        "        pd.DataFrame(\n",
        "            {\n",
        "                \"Data\": [\"Train\", \"Test\"],\n",
        "                \"RMSE\": [\n",
        "                    rmse(y_pred_train, y_observed_train),\n",
        "                    rmse(y_pred_test, y_observed_test),\n",
        "                ],\n",
        "                \"MAE\": [\n",
        "                    mae(y_pred_train, y_observed_train),\n",
        "                    mae(y_pred_test, y_observed_test),\n",
        "                ],\n",
        "                \"MAPE\": [\n",
        "                    mape(y_pred_train, y_observed_train),\n",
        "                    mape(y_pred_test, y_observed_test),\n",
        "                ],\n",
        "            }\n",
        "        )\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ATFLsX2TakXk"
      },
      "source": [
        "###5014"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ltsI3J8alty"
      },
      "outputs": [],
      "source": [
        "# Function to evaluate classification models\n",
        "def model_performance_classification(model, X, y):\n",
        "    \"\"\"Evaluate classification model performance with comprehensive metrics\"\"\"\n",
        "    from sklearn.metrics import (accuracy_score, precision_score, recall_score,\n",
        "                               f1_score, roc_curve, auc, confusion_matrix)\n",
        "\n",
        "    # Generate predictions\n",
        "    y_pred = model.predict(X)\n",
        "\n",
        "    # Calculate confusion matrix values\n",
        "    tn, fp, fn, tp = confusion_matrix(y, y_pred).ravel()\n",
        "\n",
        "    # Get probability predictions if available\n",
        "    if hasattr(model, \"predict_proba\"):\n",
        "        y_prob = model.predict_proba(X)[:, 1]\n",
        "        fpr, tpr, _ = roc_curve(y == list(set(y))[1], y_prob)\n",
        "        roc_auc = auc(fpr, tpr)\n",
        "    else:\n",
        "        roc_auc = None\n",
        "\n",
        "    # Return all metrics\n",
        "    return {\n",
        "        'accuracy': accuracy_score(y, y_pred),\n",
        "        'precision': precision_score(y, y_pred),\n",
        "        'recall': recall_score(y, y_pred),\n",
        "        'f1': f1_score(y, y_pred),\n",
        "        'error_rate': 1 - accuracy_score(y, y_pred),\n",
        "        'specificity': tn / (tn + fp),\n",
        "        'roc_auc': roc_auc\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IDeICjsATJ7h"
      },
      "source": [
        "##Model 1: Linear Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9c8yV4gOfOIY"
      },
      "outputs": [],
      "source": [
        "\n",
        "def gen_regstring(varlist,Y):\n",
        "    st = Y+' ~ '\n",
        "    if len(varlist) > 0:\n",
        "        st = st+' + '.join(varlist)\n",
        "    else:\n",
        "        st = st + '1'\n",
        "    return st\n",
        "\n",
        "def gen_reg(varlist, depvar, data):\n",
        "    lm = smf.ols(formula= gen_regstring(varlist,depvar), data = data).fit()\n",
        "    return lm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dsGXpsjX_TUS"
      },
      "outputs": [],
      "source": [
        "model = gen_reg(X_train.columns.tolist(),target,final_df)\n",
        "print(model.summary())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Tio_XUIfOIY"
      },
      "outputs": [],
      "source": [
        "model_performance_regression(model, X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y6DD68xtfOIZ"
      },
      "outputs": [],
      "source": [
        "model_performance_regression(model, X_test, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ubry1QLBIOMn"
      },
      "source": [
        "### Checking Model Assumptions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f-VadFZkIzA6"
      },
      "source": [
        "Model assumption 1: Mean of residuals should be 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "thayF43TJriA"
      },
      "outputs": [],
      "source": [
        "residuals = model.resid\n",
        "mean_residual = residuals.mean()\n",
        "print(f\"Mean of residuals: {mean_residual:.6f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_vgfBf5vI0pJ"
      },
      "source": [
        "Model assumption 2: No Heteroscedasticity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_3ZSB82HJr4D"
      },
      "outputs": [],
      "source": [
        "from statsmodels.stats.diagnostic import het_breuschpagan\n",
        "residuals = model.resid\n",
        "exog = model.model.exog  # predictors including intercept\n",
        "\n",
        "# Breusch-Pagan test\n",
        "bp_test = het_breuschpagan(residuals, exog)\n",
        "\n",
        "# Results\n",
        "names = ['Lagrange multiplier statistic', 'p-value',\n",
        "         'f-value', 'f p-value']\n",
        "print(dict(zip(names, bp_test)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M-NzTRi4TnIf"
      },
      "outputs": [],
      "source": [
        "plt.scatter(model.fittedvalues, model.resid)\n",
        "plt.axhline(0, color='red', linestyle='--')\n",
        "plt.xlabel('Fitted Values')\n",
        "plt.ylabel('Residuals')\n",
        "plt.title('Residuals vs Fitted Values')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6vBZ_isOI7Qb"
      },
      "source": [
        "Model assumption 3: Linearity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KVOkyz6SJsaU"
      },
      "outputs": [],
      "source": [
        "y_pred = model.predict(sm.add_constant(X_train))\n",
        "\n",
        "# Plot\n",
        "plt.scatter(y_train, y_pred)\n",
        "plt.plot([y_train.min(), y_train.max()], [y_train.min(), y_train.max()], color='red', linestyle='--')  # 45-degree line\n",
        "plt.xlabel('Actual Values')\n",
        "plt.ylabel('Predicted Values')\n",
        "plt.title('Actual vs Predicted')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RdM_R0YpI-Yo"
      },
      "source": [
        "Model assumption 4: Normality of the residuals"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Ny_XieUJsst"
      },
      "outputs": [],
      "source": [
        "sm.qqplot(model.resid, line='45')\n",
        "plt.title('Q-Q plot of residuals')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6RauNoQzJfOc"
      },
      "source": [
        "###Checking for Autocorrelation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4zLE2NDKJqTe"
      },
      "outputs": [],
      "source": [
        "from statsmodels.stats.stattools import durbin_watson\n",
        "dw = durbin_watson(model.resid)\n",
        "print(f\"Durbin-Watson statistic: {dw:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RtqK3tRNJn-B"
      },
      "source": [
        "###Checking for Multicollinearity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wq5yNxOkJq2N"
      },
      "outputs": [],
      "source": [
        "vif_data = pd.DataFrame()\n",
        "vif_data[\"Variable\"] = X_train.columns\n",
        "vif_data[\"VIF\"] = [variance_inflation_factor(X_train.values, i)\n",
        "                   for i in range(X_train.shape[1])]\n",
        "\n",
        "# 3. Print\n",
        "sorted_vif = vif_data.sort_values(\"VIF\", ascending=False)\n",
        "sorted_vif"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pjvSc8m1wyCk"
      },
      "source": [
        "Drop VIF>10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FScYIaI2ah_i"
      },
      "outputs": [],
      "source": [
        "# count the number of variables is infinite\n",
        "# Drop rows where the \"VIF\" column is infinite\n",
        "\n",
        "inf_feats = sorted_vif[sorted_vif[\"VIF\"].apply(np.isinf)][\"Variable\"].tolist()\n",
        "X_reduce_inf = X_train.drop(columns=inf_feats)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OyJy8Ug4wyCl"
      },
      "outputs": [],
      "source": [
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "\n",
        "def drop_high_vif(X: pd.DataFrame,\n",
        "                  thresh: float = 10.0,\n",
        "                  bulk_thresh: float = 1e7):\n",
        "    X_current = X.copy()\n",
        "    while True:\n",
        "        # compute VIF for each column\n",
        "        vif = pd.Series(\n",
        "            [variance_inflation_factor(X_current.values, i)\n",
        "             for i in range(X_current.shape[1])],\n",
        "            index=X_current.columns\n",
        "        )\n",
        "\n",
        "        # 1) bulk-drop any with VIF > bulk_thresh\n",
        "        bulk_feats = vif[vif > bulk_thresh].index.tolist()\n",
        "        if bulk_feats:\n",
        "            for feat in bulk_feats:\n",
        "                print(f\"Dropping '{feat}' (VIF={vif[feat]:.0f} > {bulk_thresh:.0f})\")\n",
        "            X_current = X_current.drop(columns=bulk_feats)\n",
        "            continue\n",
        "\n",
        "        # 2) drop highest VIF > thresh one at a time\n",
        "        max_vif = vif.max()\n",
        "        if max_vif <= thresh:\n",
        "            break\n",
        "\n",
        "        drop_feat = vif.idxmax()\n",
        "        print(f\"Dropping '{drop_feat}' (VIF={max_vif:.2f})\")\n",
        "        X_current = X_current.drop(columns=[drop_feat])\n",
        "\n",
        "    return X_current"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D10nBa3_wyCl"
      },
      "outputs": [],
      "source": [
        "vifLess10_df = drop_high_vif(X_reduce_inf)\n",
        "vifLess10_list= vifLess10_df.columns.tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lxh8J7IGwyCl"
      },
      "outputs": [],
      "source": [
        "model_reduce_vif = gen_reg(vifLess10_list,target,final_df)\n",
        "print(model_reduce_vif.summary())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XPBoYBRhTnIg"
      },
      "source": [
        "Back Elimination"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vN91YigiTnIg"
      },
      "outputs": [],
      "source": [
        "import statsmodels.formula.api as smf\n",
        "\n",
        "def backward_elimination(\n",
        "    data,\n",
        "    response: str,\n",
        "    predictors: list[str],\n",
        "    sl: float = 0.05,\n",
        "    verbose: bool = True\n",
        ") -> list[str]:\n",
        "    selected = predictors.copy()\n",
        "    while True:\n",
        "        # Build formula like \"y ~ x1 + x2 + x3\"\n",
        "        formula = f\"{response} ~ {' + '.join(selected)}\"\n",
        "        model   = smf.ols(formula, data=data).fit()\n",
        "        # Drop the intercept from consideration\n",
        "        pvals   = model.pvalues.drop('Intercept', errors='ignore')\n",
        "        max_p   = pvals.max()\n",
        "\n",
        "        if max_p > sl:\n",
        "            worst = pvals.idxmax()\n",
        "            selected.remove(worst)\n",
        "            if verbose:\n",
        "                print(f\"Dropping '{worst}' (p = {max_p:.3f})\")\n",
        "        else:\n",
        "            break\n",
        "\n",
        "    return selected\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u25TZZHrwyCl"
      },
      "outputs": [],
      "source": [
        "beSelected = backward_elimination(final_df, target, vifLess10_list, sl=0.05)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "InuzHgdRTnIg"
      },
      "outputs": [],
      "source": [
        "model_reduce =gen_reg(beSelected,target,final_df)\n",
        "print(model_reduce.summary())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-_hh1zCnSgqt"
      },
      "outputs": [],
      "source": [
        "model_performance_regression(model_reduce,X_train[beSelected] , y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MX6YzygbSgqt"
      },
      "outputs": [],
      "source": [
        "model_performance_regression(model_reduce, X_test[beSelected], y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PiKz8WrYSgqt"
      },
      "outputs": [],
      "source": [
        "#Model assumption 1: Mean of residuals should be 0\n",
        "residuals = model_reduce.resid\n",
        "mean_residual = residuals.mean()\n",
        "print(f\"Mean of residuals: {mean_residual:.6f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vgox6kFqSgqt"
      },
      "outputs": [],
      "source": [
        "#Model assumption 2: No Heteroscedasticity\n",
        "residuals = model_reduce.resid\n",
        "exog = model_reduce.model.exog  # predictors including intercept\n",
        "\n",
        "# Breusch-Pagan test\n",
        "bp_test = het_breuschpagan(residuals, exog)\n",
        "\n",
        "# Results\n",
        "names = ['Lagrange multiplier statistic', 'p-value',\n",
        "         'f-value', 'f p-value']\n",
        "print(dict(zip(names, bp_test)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z5Bn2N-PSgqu"
      },
      "outputs": [],
      "source": [
        "plt.scatter(model_reduce.fittedvalues, model_reduce.resid)\n",
        "plt.axhline(0, color='red', linestyle='--')\n",
        "plt.xlabel('Fitted Values')\n",
        "plt.ylabel('Residuals')\n",
        "plt.title('Residuals vs Fitted Values')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Se2XSXC7Sgqu"
      },
      "outputs": [],
      "source": [
        "#Model assumption 3: Linearity\n",
        "y_pred = model_reduce.predict(sm.add_constant(X_train[beSelected]))\n",
        "\n",
        "# Plot\n",
        "plt.scatter(y_train, y_pred)\n",
        "plt.plot([y_train.min(), y_train.max()], [y_train.min(), y_train.max()], color='red', linestyle='--')  # 45-degree line\n",
        "plt.xlabel('Actual Values')\n",
        "plt.ylabel('Predicted Values')\n",
        "plt.title('Actual vs Predicted')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D4cvrOHXSgqu"
      },
      "outputs": [],
      "source": [
        "#Model assumption 4: Normality of the residuals\n",
        "sm.qqplot(model_reduce.resid, line='45')\n",
        "plt.title('Q-Q plot of residuals')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hGPaZE1wwyCm"
      },
      "source": [
        "stepwise selection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8nA1KD9lwyCm"
      },
      "outputs": [],
      "source": [
        "import statsmodels.formula.api as smf\n",
        "\n",
        "def stepwise_selection(\n",
        "    data,\n",
        "    response: str,\n",
        "    predictors: list[str],\n",
        "    p_enter: float = 0.05,\n",
        "    p_remove: float = 0.05,\n",
        "    verbose: bool = True\n",
        ") -> list[str]:\n",
        "    selected = []\n",
        "    remaining = predictors.copy()\n",
        "    step = 0\n",
        "\n",
        "    while True:\n",
        "        step += 1\n",
        "        changed = False\n",
        "\n",
        "        # --- Forward step\n",
        "        best_pval = None\n",
        "        best_candidate = None\n",
        "        for cand in remaining:\n",
        "            formula = f\"{response} ~ \" + \" + \".join(selected + [cand])\n",
        "            model = smf.ols(formula, data=data).fit()\n",
        "            pval = model.pvalues.get(cand, None)\n",
        "            if pval is not None and (best_pval is None or pval < best_pval):\n",
        "                best_pval, best_candidate = pval, cand\n",
        "\n",
        "        if best_candidate and best_pval <= p_enter:\n",
        "            selected.append(best_candidate)\n",
        "            remaining.remove(best_candidate)\n",
        "            changed = True\n",
        "            if verbose:\n",
        "                print(f\"Step {step}: Add    '{best_candidate}' (p = {best_pval:.4f})\")\n",
        "\n",
        "        # --- Backward step\n",
        "        if selected:\n",
        "            formula = f\"{response} ~ \" + \" + \".join(selected)\n",
        "            model = smf.ols(formula, data=data).fit()\n",
        "            # drop the intercept from consideration\n",
        "            pvals = model.pvalues.drop(\"Intercept\", errors=\"ignore\")\n",
        "            worst_var = pvals.idxmax()\n",
        "            worst_p  = pvals.max()\n",
        "            if worst_p > p_remove:\n",
        "                selected.remove(worst_var)\n",
        "                remaining.append(worst_var)\n",
        "                changed = True\n",
        "                if verbose:\n",
        "                    print(f\"Step {step}: Drop   '{worst_var}' (p = {worst_p:.4f})\")\n",
        "\n",
        "        if not changed:\n",
        "            break\n",
        "\n",
        "    return selected\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VSiQTdkSwyCm"
      },
      "outputs": [],
      "source": [
        "steplist = stepwise_selection(final_df, target, vifLess10_list)\n",
        "steplist"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uTEm9vbQwyCm"
      },
      "outputs": [],
      "source": [
        "model_step = gen_reg(steplist,target,final_df)\n",
        "print(model_step.summary())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ma-yuQ2HMooK"
      },
      "source": [
        "PCA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K6Q-ZqtNqWn_"
      },
      "outputs": [],
      "source": [
        "from scipy.stats import zscore\n",
        "\n",
        "def stat_PCA(df, pname, n_components=None):\n",
        "    # Subset and compute correlation matrix\n",
        "    data = df[pname]\n",
        "    corr = data.corr()\n",
        "\n",
        "    # Eigen decomposition\n",
        "    eigvals, eigvecs = np.linalg.eig(corr)\n",
        "    # Sort by descending eigenvalue\n",
        "    idx = np.argsort(eigvals)[::-1]\n",
        "    eigvals = eigvals[idx]\n",
        "    eigvecs = eigvecs[:, idx]\n",
        "\n",
        "    # Determine number of components\n",
        "    p = len(pname)\n",
        "    if n_components is None or n_components > p:\n",
        "        n_components = p\n",
        "\n",
        "    # Select top n_components\n",
        "    eigvecs = eigvecs[:, :n_components]\n",
        "\n",
        "    # Compute PCA scores\n",
        "    Z = data.apply(zscore)\n",
        "    scores = Z.dot(eigvecs)\n",
        "    scores.columns = [f'P{i+1}' for i in range(n_components)]\n",
        "\n",
        "    # Create loadings DataFrame\n",
        "    loadings = pd.DataFrame(eigvecs,\n",
        "                            index=pname,\n",
        "                            columns=[f'P{i+1}' for i in range(n_components)])\n",
        "\n",
        "    return scores, loadings\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2m0crBs0qWn_"
      },
      "outputs": [],
      "source": [
        "pcscore_train,loadings_train = stat_PCA(final_df,X_train.drop(columns=binary_variables+interaction_list).columns.tolist(), n_components=10)\n",
        "pcscore_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tVwUa8ojvSXA"
      },
      "outputs": [],
      "source": [
        "eigvals = (loadings_train ** 2).sum(axis=0)\n",
        "\n",
        "# 2) turn them into proportions of total variance:\n",
        "#    (for a correlation‐matrix PCA, sum of eigenvalues = n_variables)\n",
        "prop_explained = eigvals / eigvals.sum()\n",
        "\n",
        "print(prop_explained)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DULXlmUbqWn_"
      },
      "outputs": [],
      "source": [
        "loadings_train['Prop_Var_Explained'] = (loadings_train ** 2).sum(axis=1)\n",
        "loadings_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0EhDFM6Fntok"
      },
      "outputs": [],
      "source": [
        "# Ensure y_train is aligned with pcscore_train\n",
        "# Convert pcscore_train and target to float64 to avoid complex numbers\n",
        "pclist = pcscore_train.columns.tolist()\n",
        "\n",
        "pcscore_train = pcscore_train.astype('float64')\n",
        "\n",
        "pcscore_train['target'] = final_df[target].astype('float64')\n",
        "\n",
        "# Generate the regression model\n",
        "model_pca = gen_reg(pclist, 'target', pcscore_train)\n",
        "print(model_pca.summary())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xgxkO5USXCXz"
      },
      "source": [
        "Lasso/ Ridge?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oOYg-XksbJOF"
      },
      "source": [
        "##Model 2: ANN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0FBLRp64Sgqu"
      },
      "source": [
        "5102"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Dt2tdvdbaVW"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "def ann_lin(x_train, y_train, x_test, y_test,\n",
        "            size, max_iter=100, trace=False, n_try=5):\n",
        "    best_train_mse = np.inf\n",
        "    best_test_mse  = np.inf\n",
        "    best_train_model = None\n",
        "    best_test_model  = None\n",
        "\n",
        "    for i in range(n_try):\n",
        "        # create a regressor with one hidden layer of 'size' units\n",
        "        model = MLPRegressor(hidden_layer_sizes=(size,),\n",
        "                             activation='relu',\n",
        "                             solver='adam',\n",
        "                             max_iter=max_iter,\n",
        "                             random_state=i)\n",
        "\n",
        "        model.fit(x_train, y_train)\n",
        "\n",
        "        # compute train MSE\n",
        "        y_train_pred = model.predict(x_train)\n",
        "        train_mse = mean_squared_error(y_train, y_train_pred)\n",
        "\n",
        "        # compute test MSE\n",
        "        y_test_pred = model.predict(x_test)\n",
        "        test_mse = mean_squared_error(y_test, y_test_pred)\n",
        "\n",
        "        if trace:\n",
        "            print(f\"Run {i+1}/{n_try} → train MSE: {train_mse:.4f}, test MSE: {test_mse:.4f}\")\n",
        "\n",
        "        # update best-by-train\n",
        "        if train_mse < best_train_mse:\n",
        "            best_train_mse = train_mse\n",
        "            best_train_model = model\n",
        "\n",
        "        # update best-by-test\n",
        "        if test_mse < best_test_mse:\n",
        "            best_test_mse = test_mse\n",
        "            best_test_model = model\n",
        "\n",
        "    return {\n",
        "        'best_train_mse': best_train_mse,\n",
        "        'best_test_mse':  best_test_mse,\n",
        "        'best_train_model': best_train_model,\n",
        "        'best_test_model':  best_test_model\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S-ngrcg1wyCm"
      },
      "outputs": [],
      "source": [
        "model_ann = ann_lin(X_train, y_train, X_test, y_test,\n",
        "            3, max_iter=10000, trace=False, n_try=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5IK7i0QQwyCm"
      },
      "outputs": [],
      "source": [
        "model_ann"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6XfK1xXJSgqu"
      },
      "outputs": [],
      "source": [
        "model_performance_regression(model_ann['best_train_model'], X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bZ6eQ1v6Sgqu"
      },
      "outputs": [],
      "source": [
        "model_performance_regression(model_ann['best_test_model'], X_test, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mdwZtQHdSgqv"
      },
      "source": [
        "5104 ann"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S0wV4AcTSgqv"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "def ann_cla(x_train, y_train, x_test, y_test,\n",
        "            size, max_iter=100, trace=False, n_try=5):\n",
        "    best_train_mse = np.inf\n",
        "    best_test_mse  = np.inf\n",
        "    best_train_model = None\n",
        "    best_test_model  = None\n",
        "\n",
        "    for i in range(n_try):\n",
        "        model = MLPClassifier(\n",
        "            hidden_layer_sizes=(size,),\n",
        "            activation='relu',\n",
        "            solver='adam',\n",
        "            max_iter=max_iter,\n",
        "            random_state=i\n",
        "        )\n",
        "        model.fit(x_train, y_train)\n",
        "\n",
        "        # predict class labels (0/1)\n",
        "        y_train_pred = model.predict(x_train)\n",
        "        y_test_pred  = model.predict(x_test)\n",
        "\n",
        "        # MSE on 0/1 labels = error rate\n",
        "        train_mse = mean_squared_error(y_train, y_train_pred)\n",
        "        test_mse  = mean_squared_error(y_test,  y_test_pred)\n",
        "\n",
        "        if trace:\n",
        "            print(f\"Run {i+1}/{n_try} → train MSE: {train_mse:.4f}, test MSE: {test_mse:.4f}\")\n",
        "\n",
        "        # pick best by train\n",
        "        if train_mse < best_train_mse:\n",
        "            best_train_mse   = train_mse\n",
        "            best_train_model = model\n",
        "\n",
        "        # pick best by test\n",
        "        if test_mse < best_test_mse:\n",
        "            best_test_mse    = test_mse\n",
        "            best_test_model  = model\n",
        "\n",
        "    return {\n",
        "        'best_train_mse':   best_train_mse,\n",
        "        'best_test_mse':    best_test_mse,\n",
        "        'best_train_model': best_train_model,\n",
        "        'best_test_model':  best_test_model\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LpnUHoJHSgqv"
      },
      "outputs": [],
      "source": [
        "model_ann_cla = ann_cla(X_train_2, y_train_2, X_test_2, y_test_2,\n",
        "                8, max_iter=100, trace=False, n_try=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pX7aT57bSgqv"
      },
      "outputs": [],
      "source": [
        "model_ann_cla\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ann_model = model_ann_cla['best_test_model']"
      ],
      "metadata": {
        "id": "UHIQNsr_v6eB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_szDdbxDd2LD"
      },
      "source": [
        "##Model 3: Random Forest (5102)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ve2piG1dd-zQ"
      },
      "outputs": [],
      "source": [
        "# Build random forest model\n",
        "regressor = RandomForestRegressor(\n",
        "    n_estimators=200,\n",
        "    max_depth=10,\n",
        "    min_samples_leaf=100,\n",
        "    max_features='log2',\n",
        "    min_samples_split=100,\n",
        "    bootstrap=True,\n",
        "    oob_score=True,\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# Fit on training data\n",
        "regressor.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YQ8YAsH9pLA6"
      },
      "source": [
        "Validation performance:     RMSE    MAE  R-squared  Adj. R-squared      MAPE\n",
        "0 1.1382 0.9288    -0.6153          1.8947 -159.9778"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gScVS6nTeSR-"
      },
      "outputs": [],
      "source": [
        "# Evaluate on validation set\n",
        "val_performance = model_performance_regression(regressor, X_val, y_val)\n",
        "print(\"Validation performance:\", val_performance)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fenwpvyMpReY"
      },
      "source": [
        "\n",
        "Final test performance:     RMSE    MAE  R-squared  Adj. R-squared    MAPE\n",
        "0 0.8903 0.6983    -0.0437          1.5842 81.3601"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6MqyUle0eZz4"
      },
      "outputs": [],
      "source": [
        "# Final evaluation on test set\n",
        "final_test_performance_regressor = model_performance_regression(regressor, X_test, y_test)\n",
        "print(\"\\nFinal test performance:\", final_test_performance_regressor)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JNCcQZ99_rSt"
      },
      "source": [
        "###Feature Selection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iCCP8nCLzDOT"
      },
      "outputs": [],
      "source": [
        "from sklearn.inspection import permutation_importance\n",
        "# Calculate permutation importance on validation set\n",
        "perm_result = permutation_importance(\n",
        "    regressor, X_val, y_val,\n",
        "    n_repeats=10,\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# Get importance scores and sort them\n",
        "importances = perm_result.importances_mean\n",
        "indices = np.argsort(importances)[::-1]  # Sort in descending order"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ilM1pOUY_2VO"
      },
      "outputs": [],
      "source": [
        "# Plot top 15 feature importances\n",
        "n_top = min(15, len(indices))\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.title('Top 15 Feature Importance', fontsize=15)\n",
        "plt.bar(range(n_top), importances[indices[:n_top]], color='darkgreen')\n",
        "plt.xticks(range(n_top), [X_train.columns[i] for i in indices[:n_top]], rotation=45, ha='right')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Print top features\n",
        "print(\"\\nTop 15 features:\")\n",
        "for i in range(n_top):\n",
        "    print(f\"{X_train.columns[indices[i]]}: {importances[indices[i]]:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KCQkA6GY_5-Z"
      },
      "outputs": [],
      "source": [
        "# Select features with positive importance\n",
        "threshold = 0  # Only include features that improve predictions\n",
        "selected_features = [X_train.columns[i] for i in indices if importances[i] > threshold]\n",
        "print(f\"Selected {len(selected_features)} out of {X_train.shape[1]} features\")\n",
        "print(\"Selected features:\", selected_features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xvOTWwwLe1Ci"
      },
      "outputs": [],
      "source": [
        "# Create datasets with only selected features\n",
        "X_train_selected = X_train[selected_features]\n",
        "X_val_selected = X_val[selected_features]\n",
        "X_test_selected = X_test[selected_features]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dalClfC7WinT"
      },
      "source": [
        "###Model Validation and Hyperparameter Tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_LaKr3VM_WD7"
      },
      "source": [
        "Process time around 80 mins"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_gR1RY6rzUqt"
      },
      "outputs": [],
      "source": [
        "from tqdm.auto import tqdm\n",
        "\n",
        "# Hyperparameter tuning\n",
        "rf_tuned = RandomForestRegressor(random_state=42)\n",
        "\n",
        "# Grid of parameters to choose from\n",
        "\n",
        "parameters = {\n",
        "'n_estimators': [100, 300],\n",
        "'max_depth': [None, 10, 20],\n",
        "'max_features': ['sqrt', 'log2', 0.7],\n",
        "'min_samples_split': [2, 5],\n",
        "'min_samples_leaf': [1, 2, 4],\n",
        "'bootstrap': [True],\n",
        "'criterion': ['squared_error'],\n",
        "'random_state': [42],\n",
        "'n_jobs': [-1],\n",
        "'max_samples': [0.8],\n",
        "}\n",
        "\n",
        "'''parameters = {\n",
        "    'n_estimators': [100, 120],\n",
        "    'max_depth': [None, 15],\n",
        "    'max_features': ['sqrt', 'log2', 0.7],\n",
        "    'min_samples_split': [2, 5],\n",
        "    'min_samples_leaf': [1, 2, 4],\n",
        "    'bootstrap': [True, False]\n",
        "} # process time around 16 mins'''\n",
        "\n",
        "'''parameters = {\n",
        "    'n_estimators': [100, 200, 300],\n",
        "    'max_depth': [None, 10, 20],\n",
        "    'max_features': ['sqrt', 'log2', 0.7],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4],\n",
        "    'bootstrap': [True]\n",
        "}'''\n",
        "\n",
        "'''parameters = {\n",
        "    \"n_estimators\": [100, 200, 300],\n",
        "    \"max_depth\": [None, 5, 10, 15],\n",
        "    \"max_features\": [\"sqrt\", \"log2\", 0.8, 1],\n",
        "    \"min_samples_split\": [2, 5, 10],\n",
        "    \"min_samples_leaf\": [1, 2, 4]\n",
        "}'''\n",
        "\n",
        "'''parameters = {\n",
        "    \"n_estimators\": [110, 120],\n",
        "    \"max_depth\": [5, 7],\n",
        "    \"max_features\": [0.8, 1]\n",
        "}'''\n",
        "\n",
        "'''parameters = {\n",
        "    'n_estimators': [100, 200],\n",
        "    'max_depth': [None, 15],\n",
        "    'max_features': ['sqrt', 0.7]\n",
        "}'''\n",
        "\n",
        "# Use time series cross-validation\n",
        "tscv = TimeSeriesSplit(n_splits=3)\n",
        "\n",
        "# Calculate total parameter combinations for progress tracking\n",
        "total_combinations = (\n",
        "    len(parameters['n_estimators']) *\n",
        "    len(parameters['max_depth']) *\n",
        "    len(parameters['max_features']) *\n",
        "    len(parameters['min_samples_split']) *\n",
        "    len(parameters['min_samples_leaf']) *\n",
        "    len(parameters['bootstrap']) *\n",
        "    len(parameters['criterion']) *\n",
        "    len(parameters['random_state']) *\n",
        "    len(parameters['n_jobs']) *\n",
        "    len(parameters['max_samples'])\n",
        ")\n",
        "\n",
        "print(f\"Running grid search with {total_combinations} parameter combinations and {tscv.n_splits} CV splits\")\n",
        "print(f\"Total model fits to perform: {total_combinations * tscv.n_splits}\")\n",
        "\n",
        "# Custom verbose callback to show progress\n",
        "class TqdmCallback:\n",
        "    def __init__(self, total):\n",
        "        self.pbar = tqdm(total=total, desc=\"GridSearchCV Progress\")\n",
        "        self.count = 0\n",
        "\n",
        "    def __call__(self, *args, **kwargs):\n",
        "        self.count += 1\n",
        "        self.pbar.update(1)\n",
        "\n",
        "    def close(self):\n",
        "        self.pbar.close()\n",
        "\n",
        "progress_callback = TqdmCallback(total_combinations * tscv.n_splits)\n",
        "\n",
        "# Run the grid search with custom verbose callback\n",
        "with tqdm(total=1, desc=\"Overall Progress\") as pbar:\n",
        "    grid_obj = GridSearchCV(\n",
        "        rf_tuned,\n",
        "        parameters,\n",
        "        scoring='r2',\n",
        "        cv=tscv,\n",
        "        n_jobs=-1,\n",
        "        verbose=2\n",
        "    )\n",
        "\n",
        "    # Fit model with progress tracking\n",
        "    grid_obj.fit(X_train_selected, y_train)\n",
        "    pbar.update(1)\n",
        "\n",
        "# Get the best model\n",
        "rf_tuned_regressor = grid_obj.best_estimator_\n",
        "print(\"Best parameters:\", grid_obj.best_params_)\n",
        "tuned_val_performance = model_performance_regression(rf_tuned_regressor, X_val_selected, y_val)\n",
        "print(\"Tuned model validation performance:\", tuned_val_performance)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NBPNOPlH6xUc"
      },
      "outputs": [],
      "source": [
        "# Train final model on combined training and validation data\n",
        "X_train_val = pd.concat([X_train_selected, X_val_selected])\n",
        "y_train_val = pd.concat([y_train, y_val])\n",
        "\n",
        "final_rfmodel = RandomForestRegressor(**grid_obj.best_params_)\n",
        "final_rfmodel.fit(X_train_val, y_train_val)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m7gqeixBpmYL"
      },
      "source": [
        "Final test performance:     RMSE    MAE  R-squared  Adj. R-squared     MAPE\n",
        "0 1.2189 0.9298    -0.9563          8.5243 244.8741"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d5DUdiBn6xGg"
      },
      "outputs": [],
      "source": [
        "# Final evaluation on test set only\n",
        "final_test_performance = model_performance_regression(final_rfmodel, X_test_selected, y_test)\n",
        "print(\"\\nFinal test performance:\", final_test_performance)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1KebiUhp7arx"
      },
      "outputs": [],
      "source": [
        "# Plot actual vs predicted values on test set\n",
        "y_pred = final_rfmodel.predict(X_test_selected)\n",
        "plt.figure(figsize=(14, 6))\n",
        "plt.title('Bitcoin Price: Actual vs Predicted (Selected Features)', fontsize=15)\n",
        "plt.plot(y_test.index, y_test, label='Actual', color='blue')\n",
        "plt.plot(y_test.index, y_pred, label='Predicted', color='red', linestyle='--')\n",
        "plt.xlabel('Date', fontsize=12)\n",
        "plt.ylabel('Bitcoin Price % Change', fontsize=12)\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zyCJV1MffTxu"
      },
      "outputs": [],
      "source": [
        "# Function to generate predictions with confidence intervals using bootstrap\n",
        "def predict_with_confidence(model, X_data, n_bootstrap=1000):\n",
        "\n",
        "    predictions = []\n",
        "\n",
        "    # Get all individual tree predictions\n",
        "    for tree in tqdm(model.estimators_, desc=\"Building confidence intervals\"):\n",
        "        predictions.append(tree.predict(X_data))\n",
        "\n",
        "    # Convert to numpy array\n",
        "    predictions = np.array(predictions)\n",
        "\n",
        "    # Calculate mean prediction (overall prediction)\n",
        "    mean_prediction = np.mean(predictions, axis=0)\n",
        "\n",
        "    # Calculate confidence intervals\n",
        "    lower_bound = np.percentile(predictions, 2.5, axis=0)\n",
        "    upper_bound = np.percentile(predictions, 97.5, axis=0)\n",
        "\n",
        "    return mean_prediction, lower_bound, upper_bound"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-PKnupyPZWcR"
      },
      "outputs": [],
      "source": [
        "# Prediction with the model\n",
        "prediction = final_rfmodel.predict(X_test_selected)[0]  #the first prediction in test set\n",
        "\n",
        "mean_preds, lower_bounds, upper_bounds = predict_with_confidence(final_rfmodel, X_test_selected)\n",
        "margin = (upper_bounds[0] - lower_bounds[0]) / 2\n",
        "\n",
        "print(f\"\\nPredicted Bitcoin % change: {mean_preds[0]:.2f}% ± {margin:.2f}% based on model prediction with 95% confidence\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8X9WR8l-f4zg"
      },
      "source": [
        "## Model 4: Classification Tree & Random Forest (5104)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K-uhtPRaf_1b"
      },
      "source": [
        "Result:\n",
        "\n",
        "- 2D: Basic Decision Tree performance: {'Accuracy': 0.5134615384615384, 'Precision': 0.41935483870967744, 'Recall': 0.05241935483870968, 'F1 Score': 0.0931899641577061}\n",
        "- 7D:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DI7Dqgr3f6WO"
      },
      "outputs": [],
      "source": [
        "# Function to evaluate classification performance\n",
        "def model_performance_classification(model, X, y):\n",
        "    \"\"\"Evaluate classification model performance with comprehensive metrics\"\"\"\n",
        "    from sklearn.metrics import (accuracy_score, precision_score, recall_score,\n",
        "                               f1_score, roc_curve, auc, confusion_matrix)\n",
        "\n",
        "    # Generate predictions\n",
        "    y_pred = model.predict(X)\n",
        "\n",
        "    # Calculate confusion matrix values\n",
        "    tn, fp, fn, tp = confusion_matrix(y, y_pred).ravel()\n",
        "\n",
        "    # Get probability predictions if available\n",
        "    if hasattr(model, \"predict_proba\"):\n",
        "        y_prob = model.predict_proba(X)[:, 1]\n",
        "        fpr, tpr, _ = roc_curve(y == list(set(y))[1], y_prob)\n",
        "        roc_auc = auc(fpr, tpr)\n",
        "    else:\n",
        "        roc_auc = None\n",
        "\n",
        "    # Return all metrics\n",
        "    return {\n",
        "        'accuracy': accuracy_score(y, y_pred),\n",
        "        'precision': precision_score(y, y_pred),\n",
        "        'recall': recall_score(y, y_pred),\n",
        "        'f1': f1_score(y, y_pred),\n",
        "        'error_rate': 1 - accuracy_score(y, y_pred),\n",
        "        'specificity': tn / (tn + fp),\n",
        "        'roc_auc': roc_auc\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lHNps0suXQk-"
      },
      "outputs": [],
      "source": [
        "# Build Classification tree\n",
        "dt_model_2 = DecisionTreeClassifier(\n",
        "    max_depth=5,\n",
        "    min_samples_split=30,\n",
        "    min_samples_leaf=15,\n",
        "    criterion='gini',\n",
        "    class_weight='balanced',\n",
        "    splitter='best',\n",
        "    max_features='sqrt',\n",
        "    min_impurity_decrease=0.0001,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "dt_model_2.fit(X_train_2, y_train_2)\n",
        "dt_model_2.fit(X_train_2, y_train_2)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize decision tree\n",
        "plt.figure(figsize=(25, 20))\n",
        "plot_tree(dt_model_2, feature_names=X_train_2.columns, filled=True,\n",
        "          rounded=True, fontsize=10, class_names=['No Increase', 'Increase'])\n",
        "plt.title('Basic Decision Tree for Bitcoin Price Direction', fontsize=15)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Evaluate on validation\n",
        "dt_val_performance = model_performance_classification(dt_model_2, X_val_2, y_val_2)\n",
        "print(\"Basic Decision Tree performance:\", dt_val_performance)"
      ],
      "metadata": {
        "id": "xpDihOSQFnqS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KCg69edRgM_7"
      },
      "source": [
        "Validation performance: {'Accuracy': 0.47307692307692306, 'Precision': 0.47186147186147187, 'Recall': 0.8790322580645161, 'F1 Score': 0.6140845070422535}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2djGeVXPgVrh"
      },
      "outputs": [],
      "source": [
        "# Build random forest classifier\n",
        "classifier = RandomForestClassifier(\n",
        "    n_estimators=100,\n",
        "    max_depth=8,\n",
        "    min_samples_split=20,\n",
        "    min_samples_leaf=10,\n",
        "    max_features='sqrt',\n",
        "    bootstrap=True,\n",
        "    criterion='gini',\n",
        "    max_leaf_nodes=None,\n",
        "    min_impurity_decrease=0.0001,\n",
        "    class_weight='balanced',\n",
        "    max_samples=0.8,\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "\n",
        "with tqdm(total=1, desc=\"Fitting classifier\") as pbar:\n",
        "    classifier.fit(X_train_2, y_train_2)\n",
        "    pbar.update(1)\n",
        "\n",
        "# Evaluate on validation set\n",
        "val_performance = model_performance_classification(classifier, X_val_2, y_val_2)\n",
        "print(\"Validation performance:\", val_performance)\n",
        "\n",
        "# Calculate permutation importance on validation set with progress bar\n",
        "print(\"\\nCalculating feature importance...\")\n",
        "with tqdm(total=1, desc=\"Permutation importance\") as pbar:\n",
        "    perm_result = permutation_importance(\n",
        "        classifier, X_val_2, y_val_2,\n",
        "        n_repeats=10,\n",
        "        random_state=42,\n",
        "        n_jobs=-1\n",
        "    )\n",
        "    pbar.update(1)\n",
        "\n",
        "# Get importance scores and sort them\n",
        "importances = perm_result.importances_mean\n",
        "indices = np.argsort(importances)[::-1]  # Sort in descending order\n",
        "\n",
        "# Plot only top 15 feature importances\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.title('Top 15 Feature Importance for Direction Prediction (Permutation Method)', fontsize=15)\n",
        "\n",
        "# Select top 15 features\n",
        "top_n = 15\n",
        "top_indices = indices[:top_n]\n",
        "top_importances = importances[top_indices]\n",
        "feature_names = [X_train_2.columns[i] for i in top_indices]\n",
        "\n",
        "# Create the bar plot for top features\n",
        "plt.bar(range(len(top_indices)), top_importances, color='darkblue')\n",
        "plt.xticks(range(len(top_indices)), feature_names, rotation=90)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Select features with positive importance\n",
        "threshold = 0  # Only include features that improve predictions\n",
        "selected_features = [X_train_2.columns[i] for i in indices if importances[i] > threshold]\n",
        "print(f\"Selected {len(selected_features)} out of {X_train_2.shape[1]} features\")\n",
        "print(\"Selected features:\", selected_features)\n",
        "\n",
        "# Create datasets with only selected features\n",
        "X_train_2_selected = X_train_2[selected_features]\n",
        "X_val_2_selected = X_val_2[selected_features]\n",
        "X_test_2_selected = X_test_2[selected_features]\n",
        "\n",
        "# Hyperparameter tuning for classification\n",
        "rf_tuned = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Grid of parameters specifically for classification\n",
        "parameters = {\n",
        "    'n_estimators': [100, 200],\n",
        "    'max_depth': [None, 10, 20],\n",
        "    'max_features': ['sqrt', 'log2', 0.7],\n",
        "    'min_samples_split': [2, 5],\n",
        "    'min_samples_leaf': [1, 2, 4],\n",
        "    'class_weight': ['balanced', 'balanced_subsample', None]  # Important for imbalanced classes\n",
        "}\n",
        "\n",
        "# Use time series cross-validation\n",
        "tscv = TimeSeriesSplit(n_splits=5)\n",
        "\n",
        "# Calculate total parameter combinations\n",
        "total_combinations = (\n",
        "    len(parameters['n_estimators']) *\n",
        "    len(parameters['max_depth']) *\n",
        "    len(parameters['max_features']) *\n",
        "    len(parameters['min_samples_split']) *\n",
        "    len(parameters['min_samples_leaf']) *\n",
        "    len(parameters['class_weight'])\n",
        ")\n",
        "total_fits = total_combinations * tscv.n_splits\n",
        "\n",
        "# Run the grid search with progress tracking\n",
        "print(f\"\\nStarting grid search with {total_combinations} parameter combinations\")\n",
        "print(f\"Total model fits to perform: {total_fits}\\n\")\n",
        "\n",
        "# Create a wrapper to track progress\n",
        "class ProgressGridSearchCV(GridSearchCV):\n",
        "    def fit(self, X, y):\n",
        "        with tqdm(total=1, desc=\"Grid search progress\") as pbar:\n",
        "            result = super().fit(X, y)\n",
        "            pbar.update(1)\n",
        "        return result\n",
        "\n",
        "grid_obj = ProgressGridSearchCV(\n",
        "    rf_tuned,\n",
        "    parameters,\n",
        "    scoring='f1',  # F1 score is good for imbalanced classes\n",
        "    cv=tscv,\n",
        "    n_jobs=-1,     # Use all CPU cores\n",
        "    verbose=0      # Disable default verbose since we're using tqdm\n",
        ")\n",
        "\n",
        "print(\"Starting hyperparameter search...\")\n",
        "grid_obj.fit(X_train_2_selected, y_train_2)\n",
        "print(\"Grid search completed!\")\n",
        "\n",
        "# Get the best model\n",
        "rf_tuned_classifier = grid_obj.best_estimator_\n",
        "print(\"Best parameters:\", grid_obj.best_params_)\n",
        "tuned_val_performance = model_performance_classification(rf_tuned_classifier, X_val_2_selected, y_val_2)\n",
        "print(\"Tuned model validation performance:\", tuned_val_performance)\n",
        "\n",
        "# Show confusion matrix\n",
        "y_val_pred = rf_tuned_classifier.predict(X_val_2_selected)\n",
        "cm = confusion_matrix(y_val_2, y_val_pred)\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
        "plt.title('Confusion Matrix - Validation Set')\n",
        "plt.colorbar()\n",
        "tick_marks = np.arange(2)\n",
        "plt.xticks(tick_marks, ['No Increase', 'Increase'], rotation=45)\n",
        "plt.yticks(tick_marks, ['No Increase', 'Increase'])\n",
        "plt.tight_layout()\n",
        "plt.ylabel('True Label')\n",
        "plt.xlabel('Predicted Label')\n",
        "\n",
        "# Add text annotations\n",
        "thresh = cm.max() / 2\n",
        "for i in range(cm.shape[0]):\n",
        "    for j in range(cm.shape[1]):\n",
        "        plt.text(j, i, format(cm[i, j], 'd'),\n",
        "                 horizontalalignment=\"center\",\n",
        "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "plt.show()\n",
        "\n",
        "# Train final model on combined training and validation data\n",
        "print(\"\\nTraining final model on combined training and validation data...\")\n",
        "X_train_val_2 = pd.concat([X_train_2_selected, X_val_2_selected])\n",
        "y_train_val_2 = pd.concat([y_train_2, y_val_2])\n",
        "\n",
        "final_rfmodel_2 = RandomForestClassifier(**grid_obj.best_params_, random_state=1)\n",
        "with tqdm(total=1, desc=\"Training final model\") as pbar:\n",
        "    final_rfmodel_2.fit(X_train_val_2, y_train_val_2)\n",
        "    pbar.update(1)\n",
        "\n",
        "# Final evaluation on test set only once\n",
        "final_test_performance = model_performance_classification(final_rfmodel_2, X_test_2_selected, y_test_2)\n",
        "print(\"\\nFinal test performance:\", final_test_performance)\n",
        "\n",
        "# Plot prediction probabilities on test set\n",
        "y_test_proba = final_rfmodel_2.predict_proba(X_test_2_selected)[:, 1]  # Probability of class 1 (increase)\n",
        "plt.figure(figsize=(14, 6))\n",
        "plt.title('Bitcoin Direction: Probability of Price Increase', fontsize=15)\n",
        "plt.plot(y_test_2.index, y_test_proba, label='Probability of Increase', color='blue')\n",
        "plt.axhline(y=0.5, color='red', linestyle='--', label='Decision Threshold')\n",
        "# Add actual outcomes as points\n",
        "for idx, actual in zip(y_test_2.index, y_test_2):\n",
        "    marker = 'o' if actual == 1 else 'x'\n",
        "    color = 'green' if actual == 1 else 'red'\n",
        "    plt.scatter(idx, y_test_proba[y_test_2.index.get_loc(idx)], marker=marker, color=color, s=50)\n",
        "plt.xlabel('Date', fontsize=12)\n",
        "plt.ylabel('Probability', fontsize=12)\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Function to predict with confidence using prediction probabilities\n",
        "def predict_with_confidence_classification(model, X_data, n_bootstrap=100):\n",
        "    \"\"\"Generate predictions with confidence intervals for classification probabilities\"\"\"\n",
        "    all_proba = []\n",
        "\n",
        "    # Get all individual tree predictions (probabilities) with progress bar\n",
        "    for tree in tqdm(model.estimators_, desc=\"Building confidence intervals\"):\n",
        "        # For each tree, get the probability of class 1 (price increase)\n",
        "        tree_proba = tree.predict_proba(X_data)[:, 1]\n",
        "        all_proba.append(tree_proba)\n",
        "\n",
        "    # Convert to numpy array\n",
        "    all_proba = np.array(all_proba)\n",
        "\n",
        "    # Calculate mean probability (overall prediction)\n",
        "    mean_proba = np.mean(all_proba, axis=0)\n",
        "\n",
        "    # Calculate confidence intervals\n",
        "    lower_bound = np.percentile(all_proba, 2.5, axis=0)\n",
        "    upper_bound = np.percentile(all_proba, 97.5, axis=0)\n",
        "\n",
        "    # Predict labels based on mean probability\n",
        "    pred_labels = (mean_proba >= 0.5).astype(int)\n",
        "\n",
        "    return mean_proba, lower_bound, upper_bound, pred_labels\n",
        "\n",
        "# Generate predictions with confidence intervals (already has progress bar)\n",
        "print(\"\\nGenerating predictions with confidence intervals...\")\n",
        "mean_probs, lower_bounds, upper_bounds, pred_labels = predict_with_confidence_classification(\n",
        "    final_rfmodel_2, X_test_2_selected\n",
        ")\n",
        "\n",
        "# Display prediction for the first test sample\n",
        "first_idx = 0\n",
        "direction = \"UP\" if pred_labels[first_idx] == 1 else \"DOWN\"\n",
        "confidence = mean_probs[first_idx] if pred_labels[first_idx] == 1 else 1 - mean_probs[first_idx]\n",
        "margin = (upper_bounds[first_idx] - lower_bounds[first_idx]) / 2\n",
        "\n",
        "print(f\"\\nPredicted Bitcoin direction: {direction}\")\n",
        "print(f\"Confidence: {confidence*100:.2f}% ± {margin*100:.2f}% with 95% confidence interval\")\n",
        "print(f\"Probability range: [{lower_bounds[first_idx]*100:.2f}%, {upper_bounds[first_idx]*100:.2f}%]\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RS2LrpqWgcLa"
      },
      "outputs": [],
      "source": [
        "# Feature importance for final model\n",
        "final_importances = pd.DataFrame({\n",
        "    'Feature': selected_features,\n",
        "    'Importance': final_rfmodel_2.feature_importances_\n",
        "})\n",
        "final_importances = final_importances.sort_values('Importance', ascending=False)\n",
        "\n",
        "# Get only top 15 features\n",
        "top_15_features = final_importances.head(15)\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.title('Final Model: Top 15 Features for Direction Prediction', fontsize=15)\n",
        "plt.barh(range(len(top_15_features)), top_15_features['Importance'], color='darkblue')\n",
        "plt.yticks(range(len(top_15_features)), top_15_features['Feature'])\n",
        "plt.xlabel('Importance')\n",
        "plt.gca().invert_yaxis()  # To have the highest importance at the top\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nTop 15 most important features for predicting price direction:\")\n",
        "print(final_importances.head(15))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ACXOVxARF2Xr"
      },
      "source": [
        "##Model 5: Logistic Regression (5104)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gsmv-Wx-gviF"
      },
      "source": [
        "Process time around 80 mins"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TCWJ5-DWF__Q"
      },
      "outputs": [],
      "source": [
        "# Stepwise regression function\n",
        "def stepwise_logistic_regression(X, y, initial_features=None, p_value_threshold=0.05,\n",
        "                               direction='forward', max_iterations=50, max_features=15):\n",
        "\n",
        "    all_features = X.columns.tolist()\n",
        "\n",
        "    # Start with empty list for forward selection\n",
        "    current_features = [] if initial_features is None else [f for f in initial_features if f in all_features]\n",
        "\n",
        "    # Add constant for statsmodels\n",
        "    X_with_const = sm.add_constant(X)\n",
        "\n",
        "    # Initialize variables\n",
        "    best_features = current_features.copy()\n",
        "    iterations_log = []\n",
        "    iteration = 0\n",
        "    improved = True\n",
        "    best_aic = float('inf')\n",
        "    best_model = None\n",
        "\n",
        "    print(f\"Starting with {len(current_features)} features\")\n",
        "    if current_features:\n",
        "        print(f\"Initial features: {current_features}\")\n",
        "\n",
        "    # Start stepwise selection\n",
        "    while improved and iteration < max_iterations and len(current_features) < max_features:\n",
        "        improved = False\n",
        "        iteration += 1\n",
        "        print(f\"\\nIteration {iteration}:\")\n",
        "\n",
        "        # Define candidate features for forward selection\n",
        "        forward_candidates = [f for f in all_features if f not in current_features]\n",
        "\n",
        "        # Try adding one feature (forward selection)\n",
        "        best_forward_feature = None\n",
        "        best_forward_aic = float('inf')\n",
        "\n",
        "        if direction in ['forward', 'both']:\n",
        "            for feature in forward_candidates:\n",
        "                candidate_features = current_features + [feature]\n",
        "                try:\n",
        "                    # Prepare features including constant\n",
        "                    X_subset = X_with_const[['const'] + candidate_features]\n",
        "\n",
        "                    # Fit model and get AIC\n",
        "                    try:\n",
        "                        model = sm.Logit(y, X_subset).fit(disp=0, method='lbfgs', maxiter=200)\n",
        "                    except:\n",
        "                        model = sm.Logit(y, X_subset).fit(disp=0, method='bfgs', maxiter=200)\n",
        "\n",
        "                    aic = model.aic\n",
        "\n",
        "                    # Check if p-value for the newly added feature is significant\n",
        "                    p_value = model.pvalues.get(feature, 1.0)\n",
        "\n",
        "                    if p_value <= p_value_threshold and aic < best_forward_aic:\n",
        "                        best_forward_aic = aic\n",
        "                        best_forward_feature = feature\n",
        "                        best_forward_model = model\n",
        "                        print(f\"  Testing adding {feature}: AIC={aic:.2f}, p-value={p_value:.4f}\")\n",
        "                except Exception as e:\n",
        "                    print(f\"  Error testing {feature}: {str(e)[:60]}...\")\n",
        "                    continue\n",
        "\n",
        "        # Check if we can add the feature\n",
        "        if best_forward_feature and (best_forward_aic < best_aic):\n",
        "            # Add the best feature\n",
        "            print(f\"Adding feature: {best_forward_feature}, AIC: {best_forward_aic:.2f}\")\n",
        "            current_features.append(best_forward_feature)\n",
        "            best_aic = best_forward_aic\n",
        "            best_model = best_forward_model\n",
        "            improved = True\n",
        "            iterations_log.append({\n",
        "                'iteration': iteration,\n",
        "                'action': 'add',\n",
        "                'feature': best_forward_feature,\n",
        "                'aic': best_forward_aic,\n",
        "                'features': current_features.copy()\n",
        "            })\n",
        "\n",
        "        # If direction includes backward, check for insignificant features to remove\n",
        "        if direction in ['backward', 'both'] and len(current_features) > 1 and best_model is not None:\n",
        "            # Get p-values for all features\n",
        "            p_values = {col: best_model.pvalues.get(col, 1.0) for col in current_features}\n",
        "\n",
        "            # Find the most insignificant feature (if any)\n",
        "            insignificant_features = [f for f in current_features if p_values.get(f, 0) > p_value_threshold]\n",
        "\n",
        "            if insignificant_features:\n",
        "                # Sort by p-value (most insignificant first)\n",
        "                insignificant_features.sort(key=lambda f: p_values.get(f, 0), reverse=True)\n",
        "\n",
        "                # Remove the most insignificant feature and refit\n",
        "                feature_to_remove = insignificant_features[0]\n",
        "                candidate_features = [f for f in current_features if f != feature_to_remove]\n",
        "\n",
        "                try:\n",
        "                    # Prepare features including constant\n",
        "                    X_subset = X_with_const[['const'] + candidate_features]\n",
        "\n",
        "                    # Fit model and get AIC\n",
        "                    model = sm.Logit(y, X_subset).fit(disp=0, method='lbfgs', maxiter=200)\n",
        "                    aic = model.aic\n",
        "\n",
        "                    print(f\"Testing removal of {feature_to_remove}: AIC={aic:.2f}, p-value={p_values.get(feature_to_remove, 1):.4f}\")\n",
        "\n",
        "                    # If removing improves AIC, remove it\n",
        "                    if aic < best_aic:\n",
        "                        print(f\"Removing feature: {feature_to_remove}, AIC: {aic:.2f}\")\n",
        "                        current_features = candidate_features\n",
        "                        best_aic = aic\n",
        "                        best_model = model\n",
        "                        improved = True\n",
        "                        iterations_log.append({\n",
        "                            'iteration': iteration,\n",
        "                            'action': 'remove',\n",
        "                            'feature': feature_to_remove,\n",
        "                            'aic': aic,\n",
        "                            'features': current_features.copy()\n",
        "                        })\n",
        "                except Exception as e:\n",
        "                    print(f\"Error removing {feature_to_remove}: {str(e)[:60]}...\")\n",
        "\n",
        "        # Update best features if improved\n",
        "        if improved:\n",
        "            best_features = current_features.copy()\n",
        "\n",
        "        print(f\"Current features ({len(current_features)}): {current_features}\")\n",
        "        print(f\"Current AIC: {best_aic:.2f}\")\n",
        "\n",
        "    # Final model with best features\n",
        "    if best_features:\n",
        "        try:\n",
        "            X_final = X_with_const[['const'] + best_features]\n",
        "            final_model = sm.Logit(y, X_final).fit(disp=0, method='lbfgs', maxiter=200)\n",
        "            return best_features, final_model, iterations_log\n",
        "        except Exception as e:\n",
        "            print(f\"Error fitting final model: {str(e)}\")\n",
        "            return best_features, best_model, iterations_log\n",
        "\n",
        "    return [], None, iterations_log\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vLxUcwc4hJ8Z"
      },
      "outputs": [],
      "source": [
        "# Calculate comprehensive metrics\n",
        "def cal_metrics(y_true, y_pred):\n",
        "    \"\"\"Calculate detailed classification metrics\"\"\"\n",
        "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
        "    accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
        "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
        "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
        "    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
        "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
        "    false_negative_rate = fn / (tp + fn) if (tp + fn) > 0 else 0\n",
        "    error_rate = (fp + fn) / (tp + tn + fp + fn)\n",
        "\n",
        "    return {\n",
        "        'Accuracy': accuracy,\n",
        "        'Precision': precision,\n",
        "        'Recall': recall,\n",
        "        'Specificity': specificity,\n",
        "        'F1_Score': f1,\n",
        "        'False_Negative_Rate': false_negative_rate,\n",
        "        'Error_Rate': error_rate\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_l7fy4niKXaz"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "# Build stepwise logistic regression\n",
        "start_time = time.time()\n",
        "\n",
        "initial_features = []  # Start with empty set\n",
        "\n",
        "# Run stepwise regression\n",
        "final_features, final_model, iterations_log = stepwise_logistic_regression(\n",
        "    X_train_2,\n",
        "    y_train_2,\n",
        "    initial_features=initial_features,\n",
        "    p_value_threshold=0.05,\n",
        "    direction='both',  # Use both forward and backward steps\n",
        "    max_iterations=100,\n",
        "    max_features=300\n",
        ")\n",
        "\n",
        "elapsed_time = time.time() - start_time\n",
        "print(f\"\\nStepwise regression completed in {elapsed_time:.2f} seconds\")\n",
        "print(f\"Selected {len(final_features)} features out of {len(X_train_2.columns)} candidates\")\n",
        "print(\"Selected features:\", final_features)\n",
        "\n",
        "# Visualize the feature selection process\n",
        "iterations_df = pd.DataFrame(iterations_log)\n",
        "if not iterations_df.empty:\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.plot(iterations_df['iteration'], iterations_df['aic'], 'o-', color='blue')\n",
        "    plt.xlabel('Iteration')\n",
        "    plt.ylabel('AIC Score')\n",
        "    plt.title('AIC Score Improvement During Stepwise Selection')\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "    # Annotate add/remove actions\n",
        "    for i, row in iterations_df.iterrows():\n",
        "        action_color = 'green' if row['action'] == 'add' else 'red'\n",
        "        plt.annotate(\n",
        "            f\"{row['action']}: {row['feature'][:15]}\",\n",
        "            (row['iteration'], row['aic']),\n",
        "            textcoords=\"offset points\",\n",
        "            xytext=(0, 10 if i % 2 == 0 else -20),\n",
        "            ha='center',\n",
        "            color=action_color,\n",
        "            fontsize=8\n",
        "        )\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Feature importance from the final model\n",
        "if final_model is not None:\n",
        "    # Get model coefficients (excluding constant)\n",
        "    coefficients = final_model.params.drop('const', errors='ignore')\n",
        "\n",
        "    # Calculate odds ratios and confidence intervals\n",
        "    conf_int = final_model.conf_int()\n",
        "    odds_ratios = np.exp(coefficients)\n",
        "    odds_ratios_ci = np.exp(conf_int.drop('const', errors='ignore'))\n",
        "\n",
        "    # Create a DataFrame with the results\n",
        "    feature_importance = pd.DataFrame({\n",
        "        'Feature': coefficients.index,\n",
        "        'Coefficient': coefficients.values,\n",
        "        'Odds_Ratio': odds_ratios.values,\n",
        "        'CI_Lower': odds_ratios_ci[0].values,\n",
        "        'CI_Upper': odds_ratios_ci[1].values,\n",
        "        'P_Value': final_model.pvalues.drop('const', errors='ignore').values\n",
        "    })\n",
        "\n",
        "    # Sort by absolute coefficient value\n",
        "    feature_importance['Abs_Coefficient'] = feature_importance['Coefficient'].abs()\n",
        "    feature_importance = feature_importance.sort_values('Abs_Coefficient', ascending=False).reset_index(drop=True)\n",
        "\n",
        "    # Plot feature importance\n",
        "    plt.figure(figsize=(12, max(8, len(feature_importance) * 0.3)))\n",
        "\n",
        "    # Color based on positive/negative impact\n",
        "    colors = ['green' if c > 0 else 'red' for c in feature_importance['Coefficient']]\n",
        "\n",
        "    # Create the bar plot\n",
        "    plt.barh(\n",
        "        feature_importance['Feature'],\n",
        "        feature_importance['Coefficient'],\n",
        "        color=colors\n",
        "    )\n",
        "\n",
        "    plt.axvline(x=0, color='black', linestyle='-', alpha=0.3)\n",
        "    plt.title('Feature Importance for Bitcoin Price Direction')\n",
        "    plt.xlabel('Coefficient (Log Odds)')\n",
        "    plt.ylabel('Feature')\n",
        "    plt.grid(True, axis='x', alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Print detailed feature importance table\n",
        "    print(\"\\nFeature Importance:\")\n",
        "    pd.set_option('display.max_rows', None)\n",
        "    print(feature_importance[['Feature', 'Coefficient', 'Odds_Ratio', 'P_Value', 'CI_Lower', 'CI_Upper']])\n",
        "\n",
        "    # Print model summary\n",
        "    print(\"\\nFinal Model Summary:\")\n",
        "    print(final_model.summary())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k6EitHDvKY7r"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "# Evaluate on validation set\n",
        "print(\"\\nEvaluating on validation set...\")\n",
        "\n",
        "# Add constant for predictions\n",
        "X_val_selected = sm.add_constant(X_val_2[final_features])\n",
        "\n",
        "# Get validation predictions\n",
        "val_probs = final_model.predict(X_val_selected)\n",
        "val_preds = (val_probs > 0.5).astype(int)\n",
        "\n",
        "\n",
        "# Calculate detailed validation metrics\n",
        "val_metrics = cal_metrics(y_val_2, val_preds)\n",
        "val_auc = roc_auc_score(y_val_2, val_probs)\n",
        "\n",
        "print(\"\\nValidation Set Performance:\")\n",
        "print(f\"Accuracy: {val_metrics['Accuracy']:.4f}\")\n",
        "print(f\"AUC: {val_auc:.4f}\")\n",
        "print(f\"Recall (Sensitivity): {val_metrics['Recall']:.4f}\")\n",
        "print(f\"Specificity: {val_metrics['Specificity']:.4f}\")\n",
        "print(f\"Precision: {val_metrics['Precision']:.4f}\")\n",
        "print(f\"F1 Score: {val_metrics['F1_Score']:.4f}\")\n",
        "print(f\"False Negative Rate: {val_metrics['False_Negative_Rate']:.4f}\")\n",
        "print(f\"Error Rate: {val_metrics['Error_Rate']:.4f}\")\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_val_2, val_preds))\n",
        "\n",
        "# Plot confusion matrix\n",
        "plt.figure(figsize=(8, 6))\n",
        "cm = confusion_matrix(y_val_2, val_preds)\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
        "plt.title('Confusion Matrix (Validation)')\n",
        "plt.ylabel('True Label')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.xticks([0.5, 1.5], ['Decrease/No Change', 'Increase'])\n",
        "plt.yticks([0.5, 1.5], ['Decrease/No Change', 'Increase'])\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Find optimal threshold\n",
        "print(\"\\nFinding optimal threshold...\")\n",
        "# Calculate metrics at different thresholds\n",
        "thresholds = np.arange(0.01, 1, 0.01)\n",
        "accuracy_scores = []\n",
        "f1_scores = []\n",
        "precision_scores = []\n",
        "recall_scores = []\n",
        "specificity_scores = []\n",
        "\n",
        "for threshold in thresholds:\n",
        "    y_pred = (val_probs >= threshold).astype(int)\n",
        "    metrics = cal_metrics(y_val_2, y_pred)\n",
        "    accuracy_scores.append(metrics['Accuracy'])\n",
        "    f1_scores.append(metrics['F1_Score'])\n",
        "    precision_scores.append(metrics['Precision'])\n",
        "    recall_scores.append(metrics['Recall'])\n",
        "    specificity_scores.append(metrics['Specificity'])\n",
        "\n",
        "# Plot metrics vs threshold\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.plot(thresholds, accuracy_scores, 'b-', label='Accuracy')\n",
        "plt.plot(thresholds, f1_scores, 'g-', label='F1 Score')\n",
        "plt.plot(thresholds, precision_scores, 'r-', label='Precision')\n",
        "plt.plot(thresholds, recall_scores, 'y-', label='Recall (Sensitivity)')\n",
        "plt.plot(thresholds, specificity_scores, 'm-', label='Specificity')\n",
        "plt.xlabel('Threshold')\n",
        "plt.ylabel('Score')\n",
        "plt.title('Metrics vs. Threshold')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.legend()\n",
        "\n",
        "# Find optimal thresholds for different metrics\n",
        "opt_acc_threshold = thresholds[np.argmax(accuracy_scores)]\n",
        "opt_f1_threshold = thresholds[np.argmax(f1_scores)]\n",
        "\n",
        "# Find threshold where precision and recall are closest (balanced)\n",
        "pr_diff = np.abs(np.array(precision_scores) - np.array(recall_scores))\n",
        "opt_pr_threshold = thresholds[np.argmin(pr_diff)]\n",
        "\n",
        "plt.axvline(x=0.5, color='grey', linestyle='--', label='Default (0.5)')\n",
        "plt.axvline(x=opt_acc_threshold, color='blue', linestyle='--',\n",
        "            label=f'Optimal Accuracy ({opt_acc_threshold:.2f})')\n",
        "plt.axvline(x=opt_f1_threshold, color='green', linestyle='--',\n",
        "            label=f'Optimal F1 ({opt_f1_threshold:.2f})')\n",
        "plt.axvline(x=opt_pr_threshold, color='purple', linestyle='--',\n",
        "            label=f'Balanced P-R ({opt_pr_threshold:.2f})')\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"Optimal threshold for accuracy: {opt_acc_threshold:.2f}\")\n",
        "print(f\"Optimal threshold for F1 score: {opt_f1_threshold:.2f}\")\n",
        "print(f\"Balanced precision-recall threshold: {opt_pr_threshold:.2f}\")\n",
        "\n",
        "# Choose the threshold to use (you might prefer F1 for imbalanced data)\n",
        "chosen_threshold = opt_f1_threshold\n",
        "print(f\"Using threshold: {chosen_threshold:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oNHxTMf9KaeJ"
      },
      "outputs": [],
      "source": [
        "# Retrain on combined train+validation data with selected features\n",
        "print(\"\\nRetraining on combined training and validation data...\")\n",
        "X_train_val = pd.concat([X_train_2[final_features], X_val_2[final_features]])\n",
        "y_train_val = pd.concat([y_train_2, y_val_2])\n",
        "\n",
        "# Add constant for statsmodels\n",
        "X_train_val_sm = sm.add_constant(X_train_val)\n",
        "\n",
        "# Fit final statsmodels model\n",
        "try:\n",
        "    final_combined_model = sm.Logit(y_train_val, X_train_val_sm).fit(\n",
        "        disp=0,\n",
        "        method='lbfgs',\n",
        "        maxiter=200\n",
        "    )\n",
        "    print(\"\\nFinal Combined Model Summary:\")\n",
        "    print(final_combined_model.summary())\n",
        "except Exception as e:\n",
        "    print(f\"Error fitting combined model: {str(e)}\")\n",
        "    print(\"Using original model instead\")\n",
        "    final_combined_model = final_model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Add constant explicitly\n",
        "X_test_selected = sm.add_constant(X_test_2[final_features].copy())\n",
        "\n",
        "# Ensure test data columns exactly match model's expected columns\n",
        "model_columns = final_combined_model.params.index\n",
        "X_test_selected_lr = pd.DataFrame(index=X_test_2.index)\n",
        "X_test_selected_lr['const'] = 1.0\n",
        "\n",
        "# Add each feature in the exact order expected by the model\n",
        "for col in model_columns:\n",
        "    if col != 'const':\n",
        "        if col in X_test_2.columns:\n",
        "            X_test_selected_lr[col] = X_test_2[col]\n",
        "        else:\n",
        "            print(f\"Missing column in test data: {col}\")\n",
        "            # Fill with zeros as a fallback\n",
        "            X_test_selected_lr[col] = 0\n",
        "\n",
        "# Verify the alignment\n",
        "print(f\"Test data shape: {X_test_selected_lr.shape}\")\n",
        "print(f\"Model params shape: {final_combined_model.params.shape}\")\n",
        "print(f\"Columns match: {list(X_test_selected_lr.columns) == list(model_columns)}\")"
      ],
      "metadata": {
        "id": "0aSStkb7lU8q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bXQC91LjKc4F"
      },
      "outputs": [],
      "source": [
        "# Evaluate on test set\n",
        "print(\"\\nEvaluating on test set...\")\n",
        "\n",
        "# Get test predictions\n",
        "test_probs = final_combined_model.predict(X_test_selected_lr)\n",
        "test_preds = (test_probs >= chosen_threshold).astype(int)\n",
        "\n",
        "# Calculate detailed test metrics\n",
        "test_metrics = cal_metrics(y_test_2, test_preds)\n",
        "test_auc = roc_auc_score(y_test_2, test_probs)\n",
        "\n",
        "print(\"\\nTest Set Performance:\")\n",
        "print(f\"Accuracy: {test_metrics['Accuracy']:.4f}\")\n",
        "print(f\"AUC: {test_auc:.4f}\")\n",
        "print(f\"Recall (Sensitivity): {test_metrics['Recall']:.4f}\")\n",
        "print(f\"Specificity: {test_metrics['Specificity']:.4f}\")\n",
        "print(f\"Precision: {test_metrics['Precision']:.4f}\")\n",
        "print(f\"F1 Score: {test_metrics['F1_Score']:.4f}\")\n",
        "print(f\"False Negative Rate: {test_metrics['False_Negative_Rate']:.4f}\")\n",
        "print(f\"Error Rate: {test_metrics['Error_Rate']:.4f}\")\n",
        "print(\"\\nClassification Report:\")\n",
        "classification_report(y_test_2, test_preds)\n",
        "\n",
        "# Plot confusion matrix for test set\n",
        "plt.figure(figsize=(8, 6))\n",
        "cm_test = confusion_matrix(y_test_2, test_preds)\n",
        "sns.heatmap(cm_test, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
        "plt.title('Confusion Matrix (Test)')\n",
        "plt.ylabel('True Label')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.xticks([0.5, 1.5], ['Decrease/No Change', 'Increase'])\n",
        "plt.yticks([0.5, 1.5], ['Decrease/No Change', 'Increase'])\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Plot ROC curve\n",
        "fpr, tpr, _ = roc_curve(y_test_2, test_probs)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.3f})')\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC Curve for Bitcoin Price Direction Prediction')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xZbHS7qUqCKc"
      },
      "source": [
        "===== FINAL MODEL SUMMARY =====\n",
        "Selected 21 features: ['RSI_12_Overbought_x_RSI_12', 'ADX_Trend_Duration_x_Triangle_Completion', 'Yield_Spread', 'Yield_Spread_x_Fear_and_Greed_Index', 'MACD_sell_x_Volume', 'Consecutive_RSI_12_Overbought_x_RSI_Divergence', 'ADX_x_Triangle_Duration', 'US_10Y_x_Consecutive_Extreme_Greed', 'MACD_cum_sell_x_Volume_Ratio', 'MACD_cum_sell', 'MACD_cum_buy_x_MACD', 'ADX_Trend_Direction_x_ADX_Trend_Duration', 'ADX_Strength_x_ADX_Trend_Duration', 'OBV', 'MACD_cum_buy_x_Volume', 'ADX_Buy_Signal_x_ADX_Trend_Direction', 'Price_vs_MA200_x_OBV', 'MACD_x_Price_vs_MA50', 'Yield_Spread_x_Extreme_Fear', 'Triangle_Height', 'ADX_Strength_x_Ascending_Triangle_Breakout_With_Volume']\n",
        "\n",
        "Validation Performance:\n",
        "Accuracy: 0.5333\n",
        "AUC: 0.5933\n",
        "Precision: 0.4699\n",
        "Recall (Sensitivity): 0.6000\n",
        "Specificity: 0.4824\n",
        "F1 Score: 0.5270\n",
        "False Negative Rate: 0.4000\n",
        "Error Rate: 0.4667\n",
        "\n",
        "Test Performance:\n",
        "Accuracy: 0.6424\n",
        "AUC: 0.5000\n",
        "Precision: 0.6424\n",
        "Recall (Sensitivity): 1.0000\n",
        "Specificity: 0.0000\n",
        "F1 Score: 0.7823\n",
        "False Negative Rate: 0.0000\n",
        "Error Rate: 0.3576\n",
        "Optimal threshold: 0.18\n",
        "==============================="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0LDAyrcrKfFB"
      },
      "outputs": [],
      "source": [
        "# Print comprehensive final summary\n",
        "print(\"\\n===== FINAL MODEL SUMMARY =====\")\n",
        "print(f\"Selected {len(final_features)} features: {final_features}\")\n",
        "\n",
        "print(\"\\nValidation Performance:\")\n",
        "print(f\"Accuracy: {val_metrics['Accuracy']:.4f}\")\n",
        "print(f\"AUC: {val_auc:.4f}\")\n",
        "print(f\"Precision: {val_metrics['Precision']:.4f}\")\n",
        "print(f\"Recall (Sensitivity): {val_metrics['Recall']:.4f}\")\n",
        "print(f\"Specificity: {val_metrics['Specificity']:.4f}\")\n",
        "print(f\"F1 Score: {val_metrics['F1_Score']:.4f}\")\n",
        "print(f\"False Negative Rate: {val_metrics['False_Negative_Rate']:.4f}\")\n",
        "print(f\"Error Rate: {val_metrics['Error_Rate']:.4f}\")\n",
        "\n",
        "print(\"\\nTest Performance:\")\n",
        "print(f\"Accuracy: {test_metrics['Accuracy']:.4f}\")\n",
        "print(f\"AUC: {test_auc:.4f}\")\n",
        "print(f\"Precision: {test_metrics['Precision']:.4f}\")\n",
        "print(f\"Recall (Sensitivity): {test_metrics['Recall']:.4f}\")\n",
        "print(f\"Specificity: {test_metrics['Specificity']:.4f}\")\n",
        "print(f\"F1 Score: {test_metrics['F1_Score']:.4f}\")\n",
        "print(f\"False Negative Rate: {test_metrics['False_Negative_Rate']:.4f}\")\n",
        "print(f\"Error Rate: {test_metrics['Error_Rate']:.4f}\")\n",
        "\n",
        "print(f\"\\nOptimal threshold: {chosen_threshold:.2f}\")\n",
        "print(\"===============================\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CuEBLSrgQXMc"
      },
      "source": [
        "#Model Performance Comparison"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nvjy0AYxh-Cs"
      },
      "source": [
        "##5102"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IA7MTOQgiGEd"
      },
      "outputs": [],
      "source": [
        "# function to compute different metrics to check performance of a regression model\n",
        "def model_performance_regression(model, predictors, target):\n",
        "    \"\"\"\n",
        "    Function to compute different metrics to check regression model performance\n",
        "\n",
        "    model: regressor\n",
        "    predictors: independent variables\n",
        "    target: dependent variable\n",
        "    \"\"\"\n",
        "\n",
        "    # predicting using the independent variables\n",
        "    pred = model.predict(predictors)\n",
        "\n",
        "    r2 = r2_score(target, pred)  # to compute R-squared\n",
        "    adjr2 = adj_r2_score(predictors, target, pred)  # to compute adjusted R-squared\n",
        "    rmse = np.sqrt(mean_squared_error(target, pred))  # to compute RMSE\n",
        "    mae = mean_absolute_error(target, pred)  # to compute MAE\n",
        "    mape = mape_score(target, pred)  # to compute MAPE\n",
        "\n",
        "    # creating a dataframe of metrics\n",
        "    df_perf = pd.DataFrame(\n",
        "        {\n",
        "            \"RMSE\": rmse,\n",
        "            \"MAE\": mae,\n",
        "            \"R-squared\": r2,\n",
        "            \"Adj. R-squared\": adjr2,\n",
        "            \"MAPE\": mape,\n",
        "        },\n",
        "        index=[0],\n",
        "    )\n",
        "\n",
        "    return df_perf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KTQU4soz8Iqj"
      },
      "outputs": [],
      "source": [
        "# Linear regression model - baseline\n",
        "baseline_reg_perf_test = model_performance_regression(model, X_test, y_test)\n",
        "baseline_reg_perf_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yDrCYRVYfp8N"
      },
      "outputs": [],
      "source": [
        "# Linear regression model - Reduced model 1 (calculated VIF)\n",
        "model_reduce_vif_perf_test = model_performance_regression(model_reduce_vif, X_test[vifLess10_list], y_test)\n",
        "model_reduce_vif_perf_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uNiuyR1dfwFu"
      },
      "outputs": [],
      "source": [
        "# Linear regression model - Reduced model 2 (backward elimination)\n",
        "model_reduce_perf_test = model_performance_regression(model_reduce, X_test[beSelected], y_test)\n",
        "model_reduce_perf_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TazTF6Wof0gP"
      },
      "outputs": [],
      "source": [
        "# Linear regression model - Reduced model 3 (stepwise regression)\n",
        "model_step_perf_test = model_performance_regression(model_step, X_test[steplist], y_test)\n",
        "model_step_perf_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tHzOE755f6Re"
      },
      "outputs": [],
      "source": [
        "# Linear regression model - Reduced model 4 (PCA)\n",
        "pca_perf_test = model_performance_regression(model_pca, X_test, y_test)\n",
        "pca_perf_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DjzG6Kvz8Rui"
      },
      "outputs": [],
      "source": [
        "# ANN\n",
        "ann_perf_test = model_performance_regression(model_ann['best_test_model'], X_test_ann, y_test)\n",
        "ann_perf_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wljwowHh8Ufi"
      },
      "outputs": [],
      "source": [
        "# Random Forest\n",
        "regressor_perf_test = model_performance_regression(regressor, X_test, y_test)\n",
        "regressor_perf_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oOVXCaip8cTu"
      },
      "outputs": [],
      "source": [
        "# Tuned Random Forest Regressor\n",
        "rf_tuned_regressor_perf_test = model_performance_regression(rf_tuned_regressor, X_test_selected, y_test)\n",
        "rf_tuned_regressor_perf_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UUSeD7yM8js_"
      },
      "outputs": [],
      "source": [
        "models_test_comp_df = pd.concat(\n",
        "    [\n",
        "        baseline_reg_perf_test.T,\n",
        "        model_reduce_vif_perf_test.T,\n",
        "        model_reduce_perf_test.T,\n",
        "        model_step_perf_test.T,\n",
        "        pca_perf_test.T,\n",
        "        ann_perf_test.T,\n",
        "        regressor_perf_test.T,\n",
        "        rf_tuned_regressor_perf_test.T\n",
        "    ],\n",
        "    axis=1,\n",
        ")\n",
        "\n",
        "models_test_comp_df.columns = [\n",
        "    \"Baseline Linear Regression with all predictors\",\n",
        "    \"Reduced model 1 (calculated VIF)\",\n",
        "    \"Reduced model 2 (backward elimination)\",\n",
        "    \"Reduced model 3 (stepwise regression)\",\n",
        "    \"Reduced model 4 (PCA)\",\n",
        "    \"ANN\",\n",
        "    \"Random Forest regressor\",\n",
        "    \"Tuned Random Forest Regressor\"]\n",
        "\n",
        "print(\"Test performance comparison:\")\n",
        "models_test_comp_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tha53xHfiTWF"
      },
      "source": [
        "##5104"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to evaluate model performance\n",
        "def evaluate_all_models(models, X_test_dict, y_test, model_names):\n",
        "    \"\"\"\n",
        "    Parameters:\n",
        "    -----------\n",
        "    models : list\n",
        "        List of trained model objects\n",
        "    X_test_dict : dict\n",
        "        Dictionary mapping model names to their test features\n",
        "    y_test : array-like\n",
        "        Target values\n",
        "    model_names : list\n",
        "        List of model names corresponding to the models\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    metrics_df : DataFrame\n",
        "        DataFrame containing performance metrics for all models\n",
        "    \"\"\"\n",
        "    # First ensure y_test is binary (0/1)\n",
        "    print(f\"Original y_test shape: {y_test.shape}, dtype: {y_test.dtype}\")\n",
        "    print(f\"Unique values in y_test: {np.unique(y_test)}\")\n",
        "\n",
        "    # Convert y_test to binary if needed\n",
        "    if not np.array_equal(y_test, y_test.astype(bool).astype(int)):\n",
        "        print(\"Converting y_test to binary (0/1)\")\n",
        "        y_test_binary = (y_test > 0.5).astype(int)\n",
        "    else:\n",
        "        y_test_binary = y_test.copy()\n",
        "\n",
        "    print(f\"After conversion - unique values in y_test: {np.unique(y_test_binary)}\")\n",
        "\n",
        "    metrics = {\n",
        "        'Accuracy': [],\n",
        "        'Error_Rate': [],\n",
        "        'Precision': [],\n",
        "        'Recall': [],\n",
        "        'F1_Score': [],\n",
        "        'ROC_AUC': [],\n",
        "        'Specificity': []\n",
        "    }\n",
        "\n",
        "    all_predictions = {}\n",
        "    all_probabilities = {}\n",
        "\n",
        "    successful_models = []\n",
        "\n",
        "    for i, (model, name) in enumerate(zip(models, model_names)):\n",
        "        print(f\"\\nProcessing model: {name}\")\n",
        "\n",
        "        # Get the appropriate X_test for this model\n",
        "        if name in X_test_dict:\n",
        "            X_test_model = X_test_dict[name]\n",
        "        else:\n",
        "            print(f\"Warning: No specific X_test found for {name}. Using first available.\")\n",
        "            X_test_model = list(X_test_dict.values())[0]\n",
        "\n",
        "        try:\n",
        "            # Make predictions\n",
        "            y_pred_raw = model.predict(X_test_model)\n",
        "\n",
        "            # Convert predictions to binary if needed\n",
        "            if not np.array_equal(y_pred_raw, y_pred_raw.astype(bool).astype(int)):\n",
        "                print(f\"Converting predictions for {name} to binary (0/1)\")\n",
        "                y_pred = (y_pred_raw > 0.5).astype(int)\n",
        "            else:\n",
        "                y_pred = y_pred_raw\n",
        "\n",
        "            # Try to get probability scores\n",
        "            try:\n",
        "                y_prob = model.predict_proba(X_test_model)[:, 1]\n",
        "            except:\n",
        "                print(f\"No predict_proba for {name}, using alternative approach\")\n",
        "                # If predict_proba is not available, use predictions as probabilities\n",
        "                if np.max(y_pred_raw) > 1 or np.min(y_pred_raw) < 0:\n",
        "                    # For regression outputs, use sigmoid transformation\n",
        "                    y_prob = 1 / (1 + np.exp(-y_pred_raw))\n",
        "                else:\n",
        "                    y_prob = y_pred_raw\n",
        "\n",
        "            # Calculate confusion matrix elements\n",
        "            cm = confusion_matrix(y_test_binary, y_pred)\n",
        "            tn, fp, fn, tp = cm.ravel()\n",
        "\n",
        "            # Calculate metrics\n",
        "            accuracy = accuracy_score(y_test_binary, y_pred)\n",
        "            precision = precision_score(y_test_binary, y_pred)\n",
        "            recall = recall_score(y_test_binary, y_pred)\n",
        "            f1 = f1_score(y_test_binary, y_pred)\n",
        "\n",
        "            # ROC curve and AUC\n",
        "            fpr, tpr, _ = roc_curve(y_test_binary, y_prob)\n",
        "            roc_auc = auc(fpr, tpr)\n",
        "\n",
        "            # Specificity\n",
        "            specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
        "\n",
        "            # Store results\n",
        "            metrics['Accuracy'].append(accuracy)\n",
        "            metrics['Error_Rate'].append(1 - accuracy)\n",
        "            metrics['Precision'].append(precision)\n",
        "            metrics['Recall'].append(recall)\n",
        "            metrics['F1_Score'].append(f1)\n",
        "            metrics['ROC_AUC'].append(roc_auc)\n",
        "            metrics['Specificity'].append(specificity)\n",
        "\n",
        "            all_predictions[name] = y_pred\n",
        "            all_probabilities[name] = y_prob\n",
        "            successful_models.append(name)\n",
        "\n",
        "            print(f\"Successfully evaluated {name}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing model {name}: {e}\")\n",
        "            # Skip this model\n",
        "            continue\n",
        "\n",
        "    # Check if we have evaluated any models\n",
        "    if not successful_models:\n",
        "        print(\"No models were successfully evaluated!\")\n",
        "        return None\n",
        "\n",
        "    # Create DataFrame for metrics with only successful models\n",
        "    metrics_df = pd.DataFrame({k: v for k, v in metrics.items()}, index=successful_models)\n",
        "\n",
        "    # 1. Metrics Table\n",
        "    plt.figure(figsize=(15, 4))\n",
        "    sns.heatmap(metrics_df, annot=True, cmap='YlGnBu', fmt='.3f',\n",
        "                center=0.5, vmin=0, vmax=1)\n",
        "    plt.title('Model Performance Comparison')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # 2. ROC Curves\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    for name in successful_models:\n",
        "        fpr, tpr, _ = roc_curve(y_test_binary, all_probabilities[name])\n",
        "        roc_auc = auc(fpr, tpr)\n",
        "        plt.plot(fpr, tpr, label=f'{name} (AUC = {roc_auc:.3f})')\n",
        "\n",
        "    plt.plot([0, 1], [0, 1], 'k--')\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title('ROC Curves Comparison')\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    plt.show()\n",
        "\n",
        "    # 3. Confusion Matrices with percentages\n",
        "    fig, axes = plt.subplots(1, len(successful_models), figsize=(5*len(successful_models), 4))\n",
        "    if len(successful_models) == 1:\n",
        "        axes = [axes]\n",
        "\n",
        "    for ax, name in zip(axes, successful_models):\n",
        "        cm = confusion_matrix(y_test_binary, all_predictions[name])\n",
        "        cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "\n",
        "        # Plot both counts and percentages\n",
        "        sns.heatmap(cm, annot=True, fmt='d', ax=ax, cmap='Blues')\n",
        "        sns.heatmap(cm_normalized, annot=True, fmt='.2%', ax=ax, cmap='Blues', alpha=0.3)\n",
        "        ax.set_title(f'Confusion Matrix\\n{name}\\n(count and percentage)')\n",
        "        ax.set_ylabel('True Label')\n",
        "        ax.set_xlabel('Predicted Label')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # 4. Print detailed reports\n",
        "    for name in successful_models:\n",
        "        print(f\"\\nDetailed Performance Report for {name}:\")\n",
        "        print(\"-\" * 50)\n",
        "        print(\"Classification Report:\")\n",
        "        print(classification_report(y_test_binary, all_predictions[name]))\n",
        "\n",
        "        # Calculate additional metrics for detailed reporting\n",
        "        tn, fp, fn, tp = confusion_matrix(y_test_binary, all_predictions[name]).ravel()\n",
        "        print(\"\\nAdditional Metrics:\")\n",
        "        print(f\"Error Rate: {(1 - accuracy_score(y_test_binary, all_predictions[name])):.3f}\")\n",
        "        print(f\"Specificity: {tn/(tn+fp) if (tn+fp) > 0 else 0:.3f}\")\n",
        "\n",
        "    return metrics_df"
      ],
      "metadata": {
        "id": "_xTyXIiUwL9O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a dictionary mapping model names to their corresponding test data\n",
        "X_test_dict = {\n",
        "    'ANN': X_test_2,\n",
        "    'Decision Tree': X_test_2,\n",
        "    'Random Forest': X_test_2,\n",
        "    'Tuned Random Forest': X_test_2_selected,\n",
        "    'Logistic Regression': X_test_selected_lr\n",
        "}\n",
        "\n",
        "# Set up the models list with the extracted ANN model\n",
        "models = [ann_model, dt_model_2, classifier, final_rfmodel_2, final_combined_model]\n",
        "model_names = ['ANN', 'Decision Tree', 'Random Forest', 'Tuned Random Forest', 'Logistic Regression']\n",
        "\n",
        "# Evaluate all models using the dictionary of test datasets\n",
        "metrics_df = evaluate_all_models(models, X_test_dict, y_test_2, model_names)"
      ],
      "metadata": {
        "id": "wFG9S99hwYwJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wrIIM5ZIXJN9"
      },
      "source": [
        "#Final Model Evaluation and Backtesting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Xll_G0linR5"
      },
      "source": [
        "Final Model selected: pending"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3XG2IYv7xjUA"
      },
      "source": [
        "## Back Transformation After prediction (for interpretation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SGRTZdzJuoiP"
      },
      "outputs": [],
      "source": [
        "# Recreate PowerTransformers with stored lambdas for back-transformation\n",
        "power_transformer_price.lambdas_ = [lambdas['Price_lambda']]\n",
        "power_transformer_change.lambdas_ = [lambdas['Change_lambda']]\n",
        "power_transformer_7d_pct_change.lambdas_ = [lambdas['Pct_Change_lambda_7D']]\n",
        "power_transformer_2d_pct_change.lambdas_ = [lambdas['Pct_Change_lambda_2D']]\n",
        "\n",
        "# Step 1: De-standardize the standardized values to get Yeo-Johnson transformed values\n",
        "scaled_btc_data['YeoJohnson_Price_Back'] = (\n",
        "    scaled_btc_data['YeoJohnson_Price'] * transformed_stds['Price_std']\n",
        ") + transformed_means['Price_mean']\n",
        "\n",
        "scaled_btc_data['YeoJohnson_Change_Back'] = (\n",
        "    scaled_btc_data['YeoJohnson_Change'] * transformed_stds['Change_std']\n",
        ") + transformed_means['Change_mean']\n",
        "\n",
        "scaled_btc_data['YeoJohnson_7D_Pct_Change_Back'] = (\n",
        "    scaled_btc_data['YeoJohnson_7D_Pct_Change'] * transformed_stds['Pct_Change_std_7D']\n",
        ") + transformed_means['Pct_Change_mean_7D']\n",
        "\n",
        "scaled_btc_data['YeoJohnson_2D_Pct_Change_Back'] = (\n",
        "    scaled_btc_data['YeoJohnson_2D_Pct_Change'] * transformed_stds['Pct_Change_std_2D']\n",
        ") + transformed_means['Pct_Change_mean_2D']\n",
        "\n",
        "# Step 2: Create temporary DataFrames with the exact column names used during fit\n",
        "price_back_df = scaled_btc_data[['YeoJohnson_Price_Back']].rename(columns={'YeoJohnson_Price_Back': 'Price'})\n",
        "change_back_df = scaled_btc_data[['YeoJohnson_Change_Back']].rename(columns={'YeoJohnson_Change_Back': 'Change'})\n",
        "pct_change_7d_back_df = scaled_btc_data[['YeoJohnson_7D_Pct_Change_Back']].rename(columns={'YeoJohnson_7D_Pct_Change_Back': 'Pct_Change_7D'})\n",
        "pct_change_2d_back_df = scaled_btc_data[['YeoJohnson_2D_Pct_Change_Back']].rename(columns={'YeoJohnson_2D_Pct_Change_Back': 'Pct_Change_2D'})\n",
        "\n",
        "# Step 3: Apply the inverse transformation to recover the original values\n",
        "scaled_btc_data['Original_Price'] = power_transformer_price.inverse_transform(price_back_df)\n",
        "scaled_btc_data['Original_Change'] = power_transformer_change.inverse_transform(change_back_df)\n",
        "scaled_btc_data['Original_7D_Pct_Change'] = power_transformer_7d_pct_change.inverse_transform(pct_change_7d_back_df)\n",
        "scaled_btc_data['Original_2D_Pct_Change'] = power_transformer_2d_pct_change.inverse_transform(pct_change_2d_back_df)\n",
        "\n",
        "# Step 4: Display the back-transformed data\n",
        "print(\"\\nBack-Transformed Data:\")\n",
        "print(scaled_btc_data[['Original_Price', 'Original_Change', 'Original_7D_Pct_Change', 'Original_2D_Pct_Change']].head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hwHPX6pZil4j"
      },
      "outputs": [],
      "source": [
        "# Recreate PowerTransformers with stored lambdas for back-transformation\n",
        "power_transformer_price.lambdas_ = [lambdas['Price_lambda']]\n",
        "power_transformer_change.lambdas_ = [lambdas['Change_lambda']]\n",
        "power_transformer_7d_pct_change.lambdas_ = [lambdas['Pct_Change_lambda_7D']]\n",
        "power_transformer_2d_pct_change.lambdas_ = [lambdas['Pct_Change_lambda_2D']]\n",
        "\n",
        "# De-standardize the standardized values in final_df to get Yeo-Johnson transformed values\n",
        "final_df['YeoJohnson_Price_Back'] = (\n",
        "    final_df['YeoJohnson_Price'] * transformed_stds['Price_std']\n",
        ") + transformed_means['Price_mean']\n",
        "\n",
        "final_df['YeoJohnson_Change_Back'] = (\n",
        "    final_df['YeoJohnson_Change'] * transformed_stds['Change_std']\n",
        ") + transformed_means['Change_mean']\n",
        "\n",
        "final_df['YeoJohnson_7D_Pct_Change_Back'] = (\n",
        "    final_df['YeoJohnson_7D_Pct_Change'] * transformed_stds['Pct_Change_std_7D']\n",
        ") + transformed_means['Pct_Change_mean_7D']\n",
        "\n",
        "final_df['YeoJohnson_2D_Pct_Change_Back'] = (\n",
        "    final_df['YeoJohnson_2D_Pct_Change'] * transformed_stds['Pct_Change_std_2D']\n",
        ") + transformed_means['Pct_Change_mean_2D']\n",
        "\n",
        "# Create temporary DataFrames with the exact column names used during fit\n",
        "price_back_df = final_df[['YeoJohnson_Price_Back']].rename(columns={'YeoJohnson_Price_Back': 'Price'})\n",
        "change_back_df = final_df[['YeoJohnson_Change_Back']].rename(columns={'YeoJohnson_Change_Back': 'Change'})\n",
        "pct_change_7d_back_df = final_df[['YeoJohnson_7D_Pct_Change_Back']].rename(columns={'YeoJohnson_7D_Pct_Change_Back': 'Pct_Change_7D'})\n",
        "pct_change_2d_back_df = final_df[['YeoJohnson_2D_Pct_Change_Back']].rename(columns={'YeoJohnson_2D_Pct_Change_Back': 'Pct_Change_2D'})\n",
        "\n",
        "# Apply the inverse transformation to recover the original values\n",
        "final_df['Original_Price'] = power_transformer_price.inverse_transform(price_back_df)\n",
        "final_df['Original_Change'] = power_transformer_change.inverse_transform(change_back_df)\n",
        "final_df['Original_7D_Pct_Change'] = power_transformer_7d_pct_change.inverse_transform(pct_change_7d_back_df)\n",
        "final_df['Original_2D_Pct_Change'] = power_transformer_2d_pct_change.inverse_transform(pct_change_2d_back_df)\n",
        "\n",
        "# Verify that the original data in final_df matches the original data in scaled_btc_data\n",
        "if not all(final_df.index == scaled_btc_data.index):\n",
        "    print(\"Warning: Indices don't match between final_df and scaled_btc_data\")\n",
        "\n",
        "    # Align the dataframes if needed\n",
        "    common_dates = final_df.index.intersection(scaled_btc_data.index)\n",
        "    print(f\"Common dates count: {len(common_dates)}\")\n",
        "\n",
        "    # Create a new column in scaled_btc_data for verification\n",
        "    scaled_btc_data_aligned = scaled_btc_data.loc[common_dates]\n",
        "    final_df_aligned = final_df.loc[common_dates]\n",
        "\n",
        "    # Verify values match (sample check)\n",
        "    price_diff = (scaled_btc_data_aligned['Original_Price'] - final_df_aligned['Original_Price']).abs().mean()\n",
        "    print(f\"Average absolute difference in Original_Price between dataframes: {price_diff:.8f}\")\n",
        "else:\n",
        "    print(\"Dataframe indices match perfectly\")\n",
        "\n",
        "    # Verify values match\n",
        "    price_diff = (scaled_btc_data['Original_Price'] - final_df['Original_Price']).abs().mean()\n",
        "    print(f\"Average absolute difference in Original_Price between dataframes: {price_diff:.8f}\")\n",
        "\n",
        "# Create results DataFrame with predictions from each model\n",
        "results = pd.DataFrame({\n",
        "    'actual': y_test,  # actual test values\n",
        "    'predicted': regressor.predict(X_test)  # regressor model (can change model here)\n",
        "})\n",
        "\n",
        "print(results[['actual', 'predicted']].head(10))\n",
        "print(results['actual'].isna().sum(), \"actual NaNs\")\n",
        "print(results['actual'].unique())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bBE5yCTxpZc8"
      },
      "source": [
        "## Backtesting and Trading Strategy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M1dRIiS0JKOi"
      },
      "outputs": [],
      "source": [
        "# Clone model\n",
        "def clone_and_predict(model, X_train, y_train, X_pred):\n",
        "\n",
        "    # Check if it's a statsmodels model\n",
        "    if isinstance(model, sm.regression.linear_model.RegressionResultsWrapper):\n",
        "        # For statsmodels models\n",
        "        X_train_sm = sm.add_constant(X_train)\n",
        "        X_pred_sm = sm.add_constant(X_pred)\n",
        "\n",
        "        # Create a new model with the same formula\n",
        "        new_model = sm.OLS(y_train, X_train_sm).fit()\n",
        "        return new_model.predict(X_pred_sm)\n",
        "\n",
        "    # For scikit-learn models\n",
        "    elif hasattr(model, 'fit') and hasattr(model, 'predict'):\n",
        "        # Create a clone\n",
        "        if hasattr(model, 'get_params'):\n",
        "            model_clone = type(model)(**model.get_params())\n",
        "        else:\n",
        "            model_clone = deepcopy(model)\n",
        "\n",
        "        # Fit and predict\n",
        "        model_clone.fit(X_train, y_train)\n",
        "        return model_clone.predict(X_pred)\n",
        "\n",
        "    else:\n",
        "        raise TypeError(\"Unsupported model type. Model must be either a statsmodels result or have fit/predict methods.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xuVQyyQhJKCv"
      },
      "outputs": [],
      "source": [
        "# Generate predictions using walk-forward validation\n",
        "\n",
        "def generate_predictions(model, X, target_col, test_size=0.3, step_size=0.05):\n",
        "    \"\"\"\n",
        "    Parameters:\n",
        "    model : model with fit and predict methods or statsmodels result\n",
        "    X : DataFrame with features and target\n",
        "    target_col : target column name\n",
        "    test_size : fraction of data to use for testing\n",
        "    step_size : fraction of data to move forward in each step\n",
        "    \"\"\"\n",
        "    # Extract features and target\n",
        "    feature_cols = [col for col in X.columns if not any(x in col for x in ['YeoJohnson_', 'Original_'])]\n",
        "    X_features = X[feature_cols].copy()\n",
        "    target_series = X[target_col].copy()\n",
        "\n",
        "    # Calculate indices\n",
        "    data_size = len(X_features)\n",
        "    test_start_idx = int(data_size * (1 - test_size))\n",
        "    step_idx = max(1, int(data_size * step_size))\n",
        "\n",
        "    # Create results DataFrame\n",
        "    results = pd.DataFrame(index=X_features.index[test_start_idx:])\n",
        "    results['actual'] = target_series.iloc[test_start_idx:].values\n",
        "    results['predicted'] = np.nan\n",
        "\n",
        "    # Add price if available\n",
        "    if 'Original_Price' in X.columns:\n",
        "        results['price'] = X['Original_Price'].iloc[test_start_idx:].values\n",
        "\n",
        "    # Walk-forward testing\n",
        "    train_end_idx = test_start_idx\n",
        "    with tqdm(total=data_size - test_start_idx) as pbar:\n",
        "        while train_end_idx < data_size:\n",
        "            # Define train set and prediction window\n",
        "            X_train = X_features.iloc[:train_end_idx]\n",
        "            y_train = target_series.iloc[:train_end_idx]\n",
        "            pred_end_idx = min(train_end_idx + step_idx, data_size)\n",
        "            X_pred = X_features.iloc[train_end_idx:pred_end_idx]\n",
        "\n",
        "            if len(X_pred) == 0:\n",
        "                break\n",
        "\n",
        "            # Handle different model types for prediction\n",
        "            predictions = clone_and_predict(model, X_train, y_train, X_pred)\n",
        "\n",
        "            # Store predictions\n",
        "            pred_indices = X_features.index[train_end_idx:pred_end_idx]\n",
        "            results.loc[pred_indices, 'predicted'] = predictions\n",
        "\n",
        "            # Move forward\n",
        "            train_end_idx = pred_end_idx\n",
        "            pbar.update(min(step_idx, pred_end_idx - (train_end_idx - step_idx)))\n",
        "\n",
        "    # Calculate metrics\n",
        "    mse = mean_squared_error(results['actual'].dropna(), results['predicted'].dropna())\n",
        "    rmse = np.sqrt(mse)\n",
        "    r2 = r2_score(results['actual'].dropna(), results['predicted'].dropna())\n",
        "\n",
        "    print(f\"Prediction Metrics: MSE={mse:.4f}, RMSE={rmse:.4f}, R²={r2:.4f}\")\n",
        "\n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aF1FylX9JJ6A"
      },
      "outputs": [],
      "source": [
        "# More Conservative Trading Strategy\n",
        "def apply_trading_strategy(prediction_results, target_col,\n",
        "                          initial_capital=10000, position_size=0.95,\n",
        "                          stop_loss=None, take_profit=None):\n",
        "    \"\"\"Apply a more conservative trading strategy to predictions\"\"\"\n",
        "    results = prediction_results.copy()\n",
        "\n",
        "    # Calculate prediction reliability metrics\n",
        "    results['pred_error'] = results['actual'] - results['predicted']\n",
        "    results['rolling_error'] = results['pred_error'].rolling(window=10, min_periods=1).mean()\n",
        "    results['error_std'] = results['pred_error'].rolling(window=10, min_periods=1).std()\n",
        "\n",
        "    # Create improved signal with stricter criteria and more conservatism\n",
        "    # Only take positions when predicted value exceeds a threshold (not just > 0)\n",
        "    threshold = 4.0  # Only trade when predicted change is significant (3%)\n",
        "    confidence_margin = 1.0  # Additional margin to reduce false signals\n",
        "\n",
        "    # Create directional signal (1 for strong buy, -1 for strong sell, 0 otherwise)\n",
        "    results['raw_signal'] = np.where(results['predicted'] > threshold + confidence_margin, 1,\n",
        "                           np.where(results['predicted'] < -threshold - confidence_margin, -1, 0))\n",
        "\n",
        "    # Add trend filter - only trade when prediction aligns with recent trend\n",
        "    results['price_ma5'] = results['price'].rolling(window=5, min_periods=1).mean() if 'price' in results.columns else 0\n",
        "    results['price_ma20'] = results['price'].rolling(window=20, min_periods=1).mean() if 'price' in results.columns else 0\n",
        "\n",
        "    # Trade only if price is above 20-day MA for buys or below for sells (trend confirmation)\n",
        "    if 'price' in results.columns:\n",
        "        trend_aligned = ((results['raw_signal'] == 1) & (results['price'] > results['price_ma20'])) | \\\n",
        "                        ((results['raw_signal'] == -1) & (results['price'] < results['price_ma20'])) | \\\n",
        "                        (results['raw_signal'] == 0)\n",
        "        results['signal'] = np.where(trend_aligned, results['raw_signal'], 0)\n",
        "    else:\n",
        "        results['signal'] = results['raw_signal']\n",
        "\n",
        "    # Filter out signals when error is high (model is less reliable)\n",
        "    error_threshold = results['error_std'].mean() * 2  # Dynamic threshold based on typical error\n",
        "    results['signal'] = np.where(results['error_std'] > error_threshold, 0, results['signal'])\n",
        "\n",
        "    # Add position sizing based on conviction (stronger signals get larger positions)\n",
        "    results['conviction'] = np.abs(results['predicted']) / (results['error_std'] + 0.1)  # Prevent div by zero\n",
        "    results['conviction'] = results['conviction'] / results['conviction'].rolling(window=30, min_periods=1).max().fillna(1)\n",
        "    results['position_pct'] = results['signal'] * np.minimum(results['conviction'] * 0.8, 1.0) * position_size\n",
        "\n",
        "    # Shift signal to avoid look-ahead bias\n",
        "    results['signal'] = results['signal'].shift(1).fillna(0)\n",
        "    results['position_pct'] = results['position_pct'].shift(1).fillna(0)\n",
        "\n",
        "    # Calculate actual returns based on target type\n",
        "    if '7D_Pct_Change' in target_col or '2D_Pct_Change' in target_col:\n",
        "        results['actual_return'] = results['actual'] / 100  # Convert % to decimal\n",
        "    else:\n",
        "        # For price or change targets\n",
        "        if 'price' in results.columns:\n",
        "            results['actual_return'] = results['price'].pct_change()\n",
        "        else:\n",
        "            results['actual_return'] = results['actual']\n",
        "\n",
        "    # Portfolio simulation\n",
        "    results['capital'] = initial_capital\n",
        "    results['position'] = 0\n",
        "    results['shares'] = 0\n",
        "    results['cash'] = initial_capital\n",
        "\n",
        "    # Set up for price tracking\n",
        "    if 'price' in results.columns:\n",
        "        price_col = 'price'\n",
        "    else:\n",
        "        # Create synthetic price starting at 100\n",
        "        results['synth_price'] = 100\n",
        "        for i in range(1, len(results)):\n",
        "            results['synth_price'].iloc[i] = results['synth_price'].iloc[i-1] * (1 + results['actual_return'].iloc[i])\n",
        "        price_col = 'synth_price'\n",
        "\n",
        "    # Add volatility-based position sizing and dynamic stops\n",
        "    if 'price' in results.columns:\n",
        "        # Calculate volatility metric (ATR-like)\n",
        "        results['daily_range'] = results['price'].rolling(window=14).std()\n",
        "        results['volatility_factor'] = 1.0 - np.minimum(results['daily_range'] / results['price'] * 20, 0.5)\n",
        "    else:\n",
        "        results['volatility_factor'] = 0.8  # Default conservative factor\n",
        "\n",
        "    # Implement trading logic\n",
        "    entry_price = 0\n",
        "    current_position = 0\n",
        "    days_in_position = 0\n",
        "    max_position_days = 15  # Maximum days to hold a position\n",
        "\n",
        "    # Conservative money management\n",
        "    max_trades_open = 3  # Maximum number of trades to have open at once\n",
        "    trades_today = 0  # Count of trades opened today\n",
        "    max_daily_trades = 1  # Maximum trades to open per day\n",
        "    open_trades = 0  # Number of currently open trades\n",
        "\n",
        "    for i in range(1, len(results)):\n",
        "        prev_idx = results.index[i-1]\n",
        "        curr_idx = results.index[i]\n",
        "\n",
        "        # Get values from previous day\n",
        "        prev_signal = results.loc[prev_idx, 'signal']\n",
        "        prev_position_pct = results.loc[prev_idx, 'position_pct']\n",
        "        prev_cash = results.loc[prev_idx, 'cash']\n",
        "        prev_shares = results.loc[prev_idx, 'shares']\n",
        "        volatility_factor = results.loc[prev_idx, 'volatility_factor']\n",
        "\n",
        "        # Current price\n",
        "        curr_price = results.loc[curr_idx, price_col]\n",
        "\n",
        "        # Increment days in position if we have one\n",
        "        if current_position != 0:\n",
        "            days_in_position += 1\n",
        "\n",
        "        # Force close if in position too long (prevents holding losing trades)\n",
        "        if days_in_position > max_position_days and current_position != 0:\n",
        "            if current_position == 1:  # Close long\n",
        "                cash_value = prev_shares * curr_price\n",
        "                prev_cash = cash_value\n",
        "                prev_shares = 0\n",
        "            elif current_position == -1:  # Close short\n",
        "                cash_value = prev_cash + (entry_price - curr_price) * abs(prev_shares)\n",
        "                prev_cash = cash_value\n",
        "                prev_shares = 0\n",
        "\n",
        "            current_position = 0\n",
        "            entry_price = 0\n",
        "            days_in_position = 0\n",
        "            open_trades -= 1\n",
        "            continue\n",
        "\n",
        "        # Dynamic stop loss/take profit based on volatility\n",
        "        if stop_loss is not None:\n",
        "            dynamic_stop = stop_loss * (1 / volatility_factor)  # Tighter stops in high volatility\n",
        "        else:\n",
        "            dynamic_stop = 0.03  # Default 3% stop if none specified\n",
        "\n",
        "        if take_profit is not None:\n",
        "            dynamic_tp = take_profit * volatility_factor  # Adjust TP based on volatility\n",
        "        else:\n",
        "            dynamic_tp = 0.06  # Default 6% take profit\n",
        "\n",
        "        # Check stop-loss/take-profit\n",
        "        if current_position != 0 and entry_price > 0:\n",
        "            if current_position == 1:  # Long position\n",
        "                current_return = (curr_price / entry_price) - 1\n",
        "                # Stop loss\n",
        "                if current_return <= -dynamic_stop:\n",
        "                    cash_value = prev_shares * curr_price\n",
        "                    current_position = 0\n",
        "                    entry_price = 0\n",
        "                    prev_signal = 0\n",
        "                    prev_cash = cash_value\n",
        "                    prev_shares = 0\n",
        "                    days_in_position = 0\n",
        "                    open_trades -= 1\n",
        "                # Take profit\n",
        "                elif current_return >= dynamic_tp:\n",
        "                    cash_value = prev_shares * curr_price\n",
        "                    current_position = 0\n",
        "                    entry_price = 0\n",
        "                    prev_signal = 0\n",
        "                    prev_cash = cash_value\n",
        "                    prev_shares = 0\n",
        "                    days_in_position = 0\n",
        "                    open_trades -= 1\n",
        "\n",
        "            elif current_position == -1:  # Short position\n",
        "                current_return = 1 - (curr_price / entry_price)\n",
        "                # Stop loss\n",
        "                if current_return <= -dynamic_stop:\n",
        "                    cash_value = prev_cash + (entry_price - curr_price) * abs(prev_shares)\n",
        "                    current_position = 0\n",
        "                    entry_price = 0\n",
        "                    prev_signal = 0\n",
        "                    prev_cash = cash_value\n",
        "                    prev_shares = 0\n",
        "                    days_in_position = 0\n",
        "                    open_trades -= 1\n",
        "                # Take profit\n",
        "                elif current_return >= dynamic_tp:\n",
        "                    cash_value = prev_cash + (entry_price - curr_price) * abs(prev_shares)\n",
        "                    current_position = 0\n",
        "                    entry_price = 0\n",
        "                    prev_signal = 0\n",
        "                    prev_cash = cash_value\n",
        "                    prev_shares = 0\n",
        "                    days_in_position = 0\n",
        "                    open_trades -= 1\n",
        "\n",
        "        # Reset daily trade counter if it's a new day\n",
        "        if i > 1 and results.index[i].date() != results.index[i-1].date():\n",
        "            trades_today = 0\n",
        "\n",
        "        # Process trading signal if different from current position\n",
        "        if prev_signal != current_position and trades_today < max_daily_trades and open_trades < max_trades_open:\n",
        "            # Close existing position\n",
        "            if current_position == 1:  # Close long\n",
        "                cash_value = prev_shares * curr_price\n",
        "                prev_cash = cash_value\n",
        "                prev_shares = 0\n",
        "                current_position = 0\n",
        "                entry_price = 0\n",
        "                days_in_position = 0\n",
        "                open_trades -= 1\n",
        "            elif current_position == -1:  # Close short\n",
        "                cash_value = prev_cash + (entry_price - curr_price) * abs(prev_shares)\n",
        "                prev_cash = cash_value\n",
        "                prev_shares = 0\n",
        "                current_position = 0\n",
        "                entry_price = 0\n",
        "                days_in_position = 0\n",
        "                open_trades -= 1\n",
        "\n",
        "            # Open new position with adjusted position size\n",
        "            if prev_signal != 0:\n",
        "                # Adjust position size based on volatility factor and conviction\n",
        "                adjusted_position_size = abs(prev_position_pct) * volatility_factor\n",
        "                position_value = prev_cash * adjusted_position_size\n",
        "\n",
        "                if position_value > 0:  # Only trade if position value is positive\n",
        "                    if prev_signal == 1:  # Long\n",
        "                        prev_shares = position_value / curr_price\n",
        "                        prev_cash -= position_value\n",
        "                        current_position = 1\n",
        "                        entry_price = curr_price\n",
        "                        days_in_position = 0\n",
        "                        trades_today += 1\n",
        "                        open_trades += 1\n",
        "                    elif prev_signal == -1:  # Short\n",
        "                        prev_shares = -position_value / curr_price\n",
        "                        current_position = -1\n",
        "                        entry_price = curr_price\n",
        "                        days_in_position = 0\n",
        "                        trades_today += 1\n",
        "                        open_trades += 1\n",
        "\n",
        "        # Calculate current capital\n",
        "        if current_position == 1:  # Long\n",
        "            curr_capital = prev_cash + (prev_shares * curr_price)\n",
        "        elif current_position == -1:  # Short\n",
        "            short_pnl = (entry_price - curr_price) * abs(prev_shares)\n",
        "            curr_capital = prev_cash + short_pnl\n",
        "        else:  # No position\n",
        "            curr_capital = prev_cash\n",
        "\n",
        "        # Update results\n",
        "        results.loc[curr_idx, 'capital'] = curr_capital\n",
        "        results.loc[curr_idx, 'position'] = current_position\n",
        "        results.loc[curr_idx, 'shares'] = prev_shares\n",
        "        results.loc[curr_idx, 'cash'] = prev_cash\n",
        "\n",
        "    # Calculate performance metrics\n",
        "    results['daily_return'] = results['capital'].pct_change()\n",
        "    results['cum_return'] = (1 + results['daily_return']).cumprod() - 1\n",
        "\n",
        "    # Market returns (buy & hold)\n",
        "    results['market_return'] = results[price_col].pct_change()\n",
        "    results['cum_market_return'] = (1 + results['market_return']).cumprod() - 1\n",
        "\n",
        "    # Calculate drawdowns\n",
        "    results['peak_capital'] = results['capital'].cummax()\n",
        "    results['drawdown'] = (results['capital'] - results['peak_capital']) / results['peak_capital']\n",
        "\n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GR9TaJaiJJwh"
      },
      "outputs": [],
      "source": [
        "# Backtesting - Performance Analysis\n",
        "def analyze_backtest(backtest_results):\n",
        "    \"\"\"Calculate performance metrics and display results\"\"\"\n",
        "    results = backtest_results.copy()\n",
        "\n",
        "    # Calculate basic metrics\n",
        "    total_return = results['cum_return'].iloc[-1]\n",
        "    market_return = results['cum_market_return'].iloc[-1]\n",
        "    max_drawdown = results['drawdown'].min()\n",
        "\n",
        "    # Calculate annualized return\n",
        "    days = (results.index[-1] - results.index[0]).days\n",
        "    years = max(days / 365, 0.01)\n",
        "    annual_return = (1 + total_return) ** (1/years) - 1\n",
        "    market_annual = (1 + market_return) ** (1/years) - 1\n",
        "\n",
        "    # Calculate Sharpe ratio (annualized)\n",
        "    risk_free_rate = 0.02  # 2% annual risk-free rate\n",
        "    excess_returns = results['daily_return'] - ((1 + risk_free_rate) ** (1/252) - 1)\n",
        "    sharpe_ratio = np.sqrt(252) * excess_returns.mean() / excess_returns.std() if excess_returns.std() > 0 else 0\n",
        "\n",
        "    # Calculate win rate and trades\n",
        "    trade_changes = results['position'].diff().fillna(0)\n",
        "    total_trades = (trade_changes != 0).sum() / 2\n",
        "    winning_days = (results['daily_return'] > 0).sum()\n",
        "    losing_days = (results['daily_return'] < 0).sum()\n",
        "    win_rate = winning_days / (winning_days + losing_days) if (winning_days + losing_days) > 0 else 0\n",
        "\n",
        "    # Print results\n",
        "    print(\"\\nPerformance Summary:\")\n",
        "    print(f\"Total Return: {total_return:.2%} (Market: {market_return:.2%})\")\n",
        "    print(f\"Annual Return: {annual_return:.2%} (Market: {market_annual:.2%})\")\n",
        "    print(f\"Sharpe Ratio: {sharpe_ratio:.2f}\")\n",
        "    print(f\"Max Drawdown: {max_drawdown:.2%}\")\n",
        "    print(f\"Total Trades: {total_trades:.0f}\")\n",
        "    print(f\"Win Rate: {win_rate:.2%} ({winning_days}/{winning_days+losing_days})\")\n",
        "\n",
        "    return {\n",
        "        'total_return': total_return,\n",
        "        'annual_return': annual_return,\n",
        "        'sharpe_ratio': sharpe_ratio,\n",
        "        'max_drawdown': max_drawdown,\n",
        "        'total_trades': total_trades,\n",
        "        'win_rate': win_rate\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gE8cH2xgJJuY"
      },
      "outputs": [],
      "source": [
        "# Visualization\n",
        "def visualize_backtest(backtest_results, target_col):\n",
        "    \"\"\"Create visualization of backtest results\"\"\"\n",
        "    results = backtest_results.copy()\n",
        "\n",
        "    # Plot 1: Actual vs Predicted\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.plot(results.index, results['actual'], 'b-', label='Actual')\n",
        "    plt.plot(results.index, results['predicted'], 'r-', label='Predicted')\n",
        "\n",
        "    if '7D_Pct_Change' in target_col:\n",
        "        title = '7-Day Percentage Change'\n",
        "        ylabel = '7-Day % Change'\n",
        "    elif '2D_Pct_Change' in target_col:\n",
        "        title = '2-Day Percentage Change'\n",
        "        ylabel = '2-Day % Change'\n",
        "    else:\n",
        "        title = 'Value'\n",
        "        ylabel = 'Value'\n",
        "\n",
        "    plt.title(f'Actual vs Predicted {title}')\n",
        "    plt.xlabel('Date')\n",
        "    plt.ylabel(ylabel)\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Plot 2: Capital Growth\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.plot(results.index, results['capital'], 'g-', label='Strategy Capital')\n",
        "\n",
        "    # Add buy-and-hold for comparison\n",
        "    price_col = 'price' if 'price' in results.columns else 'synth_price'\n",
        "    initial_capital = results['capital'].iloc[0]\n",
        "    initial_price = results[price_col].iloc[0]\n",
        "    initial_shares = initial_capital / initial_price\n",
        "    buy_hold_capital = initial_shares * results[price_col]\n",
        "    plt.plot(results.index, buy_hold_capital, 'b-', label='Buy & Hold')\n",
        "\n",
        "    plt.title('Capital Growth')\n",
        "    plt.xlabel('Date')\n",
        "    plt.ylabel('Capital ($)')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Plot 3: Cumulative Returns\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.plot(results.index, results['cum_return'], 'g-', label='Strategy')\n",
        "    plt.plot(results.index, results['cum_market_return'], 'b-', label='Market')\n",
        "    plt.title('Cumulative Returns')\n",
        "    plt.xlabel('Date')\n",
        "    plt.ylabel('Return')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Plot 4: Drawdowns\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.plot(results.index, results['drawdown'], 'r-')\n",
        "    plt.title('Strategy Drawdowns')\n",
        "    plt.xlabel('Date')\n",
        "    plt.ylabel('Drawdown')\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Plot 5: Position over time\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.plot(results.index, results['position'], 'b-')\n",
        "    plt.title('Position Over Time')\n",
        "    plt.xlabel('Date')\n",
        "    plt.ylabel('Position (1=Long, -1=Short, 0=Cash)')\n",
        "    plt.yticks([-1, 0, 1])\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-CsjRRsNJJrW"
      },
      "outputs": [],
      "source": [
        "def fixed_backtest_model(model, X, target_col, test_size=0.25, step_size=0.02,\n",
        "                       initial_capital=10000, position_size=1.0,\n",
        "                       stop_loss=None, take_profit=None, plot=False):\n",
        "    \"\"\"\n",
        "    A fixed version that handles the formula model correctly\n",
        "    \"\"\"\n",
        "    # Check if it's a formula-based model\n",
        "    is_formula_model = isinstance(model, sm.regression.linear_model.RegressionResultsWrapper) and hasattr(model.model, 'formula')\n",
        "\n",
        "    if is_formula_model:\n",
        "        print(\"Using formula-based model...\")\n",
        "        # Get the formula\n",
        "        formula = model.model.formula\n",
        "        print(f\"Model formula: {formula}\")\n",
        "\n",
        "        # Step 1: Generate predictions using the formula directly\n",
        "        print(\"Step 1: Generating predictions...\")\n",
        "\n",
        "        # Simplified approach - recreate model for each prediction\n",
        "        # Instead of using walk-forward validation, we'll just split the data\n",
        "        # This is less accurate but easier to implement\n",
        "        data_size = len(X)\n",
        "        test_start_idx = int(data_size * (1 - test_size))\n",
        "\n",
        "        # Train data\n",
        "        train_data = X.iloc[:test_start_idx].copy()\n",
        "        # Test data\n",
        "        test_data = X.iloc[test_start_idx:].copy()\n",
        "\n",
        "        # Re-fit model on training data\n",
        "        import statsmodels.formula.api as smf\n",
        "        train_model = smf.ols(formula=formula, data=train_data).fit()\n",
        "\n",
        "        # Make predictions\n",
        "        test_predictions = train_model.predict(test_data)\n",
        "\n",
        "        # Create prediction results dataframe\n",
        "        prediction_results = pd.DataFrame(index=test_data.index)\n",
        "        prediction_results['actual'] = test_data[target_col].values\n",
        "        prediction_results['predicted'] = test_predictions\n",
        "\n",
        "        if 'Original_Price' in X.columns:\n",
        "            prediction_results['price'] = X['Original_Price'].iloc[test_start_idx:].values\n",
        "    else:\n",
        "        # For non-formula models, use your original approach\n",
        "        print(\"Using standard model...\")\n",
        "        prediction_results = generate_predictions(\n",
        "            model=model, X=X, target_col=target_col,\n",
        "            test_size=test_size, step_size=step_size\n",
        "        )\n",
        "\n",
        "    # Continue with trading strategy as before\n",
        "    print(\"\\nStep 2: Applying trading strategy...\")\n",
        "    backtest_results = apply_trading_strategy(\n",
        "        prediction_results=prediction_results,\n",
        "        target_col=target_col,\n",
        "        initial_capital=initial_capital,\n",
        "        position_size=position_size,\n",
        "        stop_loss=stop_loss,\n",
        "        take_profit=take_profit\n",
        "    )\n",
        "\n",
        "    # Analysis and visualization remain the same\n",
        "    print(\"\\nStep 3: Analyzing performance...\")\n",
        "    performance = analyze_backtest(backtest_results)\n",
        "\n",
        "    if plot:\n",
        "        print(\"\\nStep 4: Generating visualizations...\")\n",
        "        visualize_backtest(backtest_results, target_col)\n",
        "\n",
        "    return backtest_results, performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OrxF0IB4JJi4"
      },
      "outputs": [],
      "source": [
        "# Create backtest df\n",
        "backtest_df = final_df.copy()\n",
        "\n",
        "# Run backtest with clean DataFrame\n",
        "backtest_results_basic, performance_basic = fixed_backtest_model(\n",
        "    model=model,\n",
        "    X=backtest_df,\n",
        "    target_col='Original_7D_Pct_Change',\n",
        "    test_size=0.25,\n",
        "    step_size=0.02,\n",
        "    initial_capital=25000,\n",
        "    position_size=0.8,\n",
        "    stop_loss=0.02,\n",
        "    take_profit=0.08,\n",
        "    plot=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GXJ7zIOBJGdY"
      },
      "source": [
        "backup (del later)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B8unzWbtixv7"
      },
      "outputs": [],
      "source": [
        "# Clone model\n",
        "def clone_and_predict(model, X_train, y_train, X_pred):\n",
        "\n",
        "    # Check if it's a statsmodels model\n",
        "    if isinstance(model, sm.regression.linear_model.RegressionResultsWrapper):\n",
        "        # For statsmodels models\n",
        "        X_train_sm = sm.add_constant(X_train)\n",
        "        X_pred_sm = sm.add_constant(X_pred)\n",
        "\n",
        "        # Create a new model with the same formula\n",
        "        new_model = sm.OLS(y_train, X_train_sm).fit()\n",
        "        return new_model.predict(X_pred_sm)\n",
        "\n",
        "    # For scikit-learn models\n",
        "    elif hasattr(model, 'fit') and hasattr(model, 'predict'):\n",
        "        # Create a clone\n",
        "        if hasattr(model, 'get_params'):\n",
        "            model_clone = type(model)(**model.get_params())\n",
        "        else:\n",
        "            model_clone = deepcopy(model)\n",
        "\n",
        "        # Fit and predict\n",
        "        model_clone.fit(X_train, y_train)\n",
        "        return model_clone.predict(X_pred)\n",
        "\n",
        "    else:\n",
        "        raise TypeError(\"Unsupported model type. Model must be either a statsmodels result or have fit/predict methods.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WIlNnZ8Ti8L0"
      },
      "outputs": [],
      "source": [
        "# Generate predictions using walk-forward validation\n",
        "\n",
        "def generate_predictions(model, X, target_col, test_size=0.3, step_size=0.05):\n",
        "    \"\"\"\n",
        "    Parameters:\n",
        "    model : model with fit and predict methods or statsmodels result\n",
        "    X : DataFrame with features and target\n",
        "    target_col : target column name\n",
        "    test_size : fraction of data to use for testing\n",
        "    step_size : fraction of data to move forward in each step\n",
        "    \"\"\"\n",
        "    # Extract features and target\n",
        "    feature_cols = [col for col in X.columns if not any(x in col for x in ['YeoJohnson_', 'Original_'])]\n",
        "    X_features = X[feature_cols].copy()\n",
        "    target_series = X[target_col].copy()\n",
        "\n",
        "    # Calculate indices\n",
        "    data_size = len(X_features)\n",
        "    test_start_idx = int(data_size * (1 - test_size))\n",
        "    step_idx = max(1, int(data_size * step_size))\n",
        "\n",
        "    # Create results DataFrame\n",
        "    results = pd.DataFrame(index=X_features.index[test_start_idx:])\n",
        "    results['actual'] = target_series.iloc[test_start_idx:].values\n",
        "    results['predicted'] = np.nan\n",
        "\n",
        "    # Add price if available\n",
        "    if 'Original_Price' in X.columns:\n",
        "        results['price'] = X['Original_Price'].iloc[test_start_idx:].values\n",
        "\n",
        "    # Walk-forward testing\n",
        "    train_end_idx = test_start_idx\n",
        "    with tqdm(total=data_size - test_start_idx) as pbar:\n",
        "        while train_end_idx < data_size:\n",
        "            # Define train set and prediction window\n",
        "            X_train = X_features.iloc[:train_end_idx]\n",
        "            y_train = target_series.iloc[:train_end_idx]\n",
        "            pred_end_idx = min(train_end_idx + step_idx, data_size)\n",
        "            X_pred = X_features.iloc[train_end_idx:pred_end_idx]\n",
        "\n",
        "            if len(X_pred) == 0:\n",
        "                break\n",
        "\n",
        "            # Handle different model types for prediction\n",
        "            predictions = clone_and_predict(model, X_train, y_train, X_pred)\n",
        "\n",
        "            # Store predictions\n",
        "            pred_indices = X_features.index[train_end_idx:pred_end_idx]\n",
        "            results.loc[pred_indices, 'predicted'] = predictions\n",
        "\n",
        "            # Move forward\n",
        "            train_end_idx = pred_end_idx\n",
        "            pbar.update(min(step_idx, pred_end_idx - (train_end_idx - step_idx)))\n",
        "\n",
        "    # Calculate metrics\n",
        "    mse = mean_squared_error(results['actual'].dropna(), results['predicted'].dropna())\n",
        "    rmse = np.sqrt(mse)\n",
        "    r2 = r2_score(results['actual'].dropna(), results['predicted'].dropna())\n",
        "\n",
        "    print(f\"Prediction Metrics: MSE={mse:.4f}, RMSE={rmse:.4f}, R²={r2:.4f}\")\n",
        "\n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GfWoP5c3jDVO"
      },
      "outputs": [],
      "source": [
        "# Trading Strategy\n",
        "def apply_trading_strategy(prediction_results, target_col,\n",
        "                          initial_capital=10000, position_size=0.95,\n",
        "                          stop_loss=None, take_profit=None):\n",
        "    \"\"\"Apply trading strategy to predictions\"\"\"\n",
        "    results = prediction_results.copy()\n",
        "\n",
        "    # Create signal: 1 for buy (predicted > 0), -1 for sell (predicted < 0)\n",
        "    results['signal'] = np.sign(results['predicted'])\n",
        "\n",
        "    # Calculate actual returns based on target type\n",
        "    if '7D_Pct_Change' in target_col or '2D_Pct_Change' in target_col:\n",
        "        results['actual_return'] = results['actual'] / 100  # Convert % to decimal\n",
        "    else:\n",
        "        # For price or change targets\n",
        "        if 'price' in results.columns:\n",
        "            results['actual_return'] = results['price'].pct_change()\n",
        "        else:\n",
        "            results['actual_return'] = results['actual']\n",
        "\n",
        "    # Shift signal to avoid look-ahead bias\n",
        "    results['signal'] = results['signal'].shift(1)\n",
        "    results['signal'].iloc[0] = 0  # No position on first day\n",
        "\n",
        "    # Portfolio simulation\n",
        "    results['capital'] = initial_capital\n",
        "    results['position'] = 0\n",
        "    results['shares'] = 0\n",
        "    results['cash'] = initial_capital\n",
        "\n",
        "    # Set up for price tracking\n",
        "    if 'price' in results.columns:\n",
        "        price_col = 'price'\n",
        "    else:\n",
        "        # Create synthetic price starting at 100\n",
        "        results['synth_price'] = 100\n",
        "        for i in range(1, len(results)):\n",
        "            results['synth_price'].iloc[i] = results['synth_price'].iloc[i-1] * (1 + results['actual_return'].iloc[i])\n",
        "        price_col = 'synth_price'\n",
        "\n",
        "    # Implement trading logic\n",
        "    entry_price = 0\n",
        "    current_position = 0\n",
        "\n",
        "    for i in range(1, len(results)):\n",
        "        prev_idx = results.index[i-1]\n",
        "        curr_idx = results.index[i]\n",
        "\n",
        "        # Get values from previous day\n",
        "        prev_signal = results.loc[prev_idx, 'signal']\n",
        "        prev_cash = results.loc[prev_idx, 'cash']\n",
        "        prev_shares = results.loc[prev_idx, 'shares']\n",
        "\n",
        "        # Current price\n",
        "        curr_price = results.loc[curr_idx, price_col]\n",
        "\n",
        "        # Check stop-loss/take-profit\n",
        "        if current_position != 0 and entry_price > 0:\n",
        "            if current_position == 1:  # Long position\n",
        "                current_return = (curr_price / entry_price) - 1\n",
        "                # Stop loss\n",
        "                if stop_loss is not None and current_return <= -stop_loss:\n",
        "                    cash_value = prev_shares * curr_price\n",
        "                    current_position = 0\n",
        "                    entry_price = 0\n",
        "                    prev_signal = 0\n",
        "                    prev_cash = cash_value\n",
        "                    prev_shares = 0\n",
        "                # Take profit\n",
        "                elif take_profit is not None and current_return >= take_profit:\n",
        "                    cash_value = prev_shares * curr_price\n",
        "                    current_position = 0\n",
        "                    entry_price = 0\n",
        "                    prev_signal = 0\n",
        "                    prev_cash = cash_value\n",
        "                    prev_shares = 0\n",
        "\n",
        "            elif current_position == -1:  # Short position\n",
        "                current_return = 1 - (curr_price / entry_price)\n",
        "                # Stop loss\n",
        "                if stop_loss is not None and current_return <= -stop_loss:\n",
        "                    cash_value = prev_cash + (entry_price - curr_price) * abs(prev_shares)\n",
        "                    current_position = 0\n",
        "                    entry_price = 0\n",
        "                    prev_signal = 0\n",
        "                    prev_cash = cash_value\n",
        "                    prev_shares = 0\n",
        "                # Take profit\n",
        "                elif take_profit is not None and current_return >= take_profit:\n",
        "                    cash_value = prev_cash + (entry_price - curr_price) * abs(prev_shares)\n",
        "                    current_position = 0\n",
        "                    entry_price = 0\n",
        "                    prev_signal = 0\n",
        "                    prev_cash = cash_value\n",
        "                    prev_shares = 0\n",
        "\n",
        "        # Process trading signal if different from current position\n",
        "        if prev_signal != current_position:\n",
        "            # Close existing position\n",
        "            if current_position == 1:  # Close long\n",
        "                cash_value = prev_shares * curr_price\n",
        "                prev_cash = cash_value\n",
        "                prev_shares = 0\n",
        "            elif current_position == -1:  # Close short\n",
        "                cash_value = prev_cash + (entry_price - curr_price) * abs(prev_shares)\n",
        "                prev_cash = cash_value\n",
        "                prev_shares = 0\n",
        "\n",
        "            # Open new position\n",
        "            if prev_signal != 0:\n",
        "                position_value = prev_cash * position_size\n",
        "\n",
        "                if prev_signal == 1:  # Long\n",
        "                    prev_shares = position_value / curr_price\n",
        "                    prev_cash -= position_value\n",
        "                    current_position = 1\n",
        "                    entry_price = curr_price\n",
        "                elif prev_signal == -1:  # Short\n",
        "                    prev_shares = -position_value / curr_price\n",
        "                    current_position = -1\n",
        "                    entry_price = curr_price\n",
        "\n",
        "        # Calculate current capital\n",
        "        if current_position == 1:  # Long\n",
        "            curr_capital = prev_cash + (prev_shares * curr_price)\n",
        "        elif current_position == -1:  # Short\n",
        "            short_pnl = (entry_price - curr_price) * abs(prev_shares)\n",
        "            curr_capital = prev_cash + short_pnl\n",
        "        else:  # No position\n",
        "            curr_capital = prev_cash\n",
        "\n",
        "        # Update results\n",
        "        results.loc[curr_idx, 'capital'] = curr_capital\n",
        "        results.loc[curr_idx, 'position'] = current_position\n",
        "        results.loc[curr_idx, 'shares'] = prev_shares\n",
        "        results.loc[curr_idx, 'cash'] = prev_cash\n",
        "\n",
        "    # Calculate performance metrics\n",
        "    results['daily_return'] = results['capital'].pct_change()\n",
        "    results['cum_return'] = (1 + results['daily_return']).cumprod() - 1\n",
        "\n",
        "    # Market returns (buy & hold)\n",
        "    results['market_return'] = results[price_col].pct_change()\n",
        "    results['cum_market_return'] = (1 + results['market_return']).cumprod() - 1\n",
        "\n",
        "    # Calculate drawdowns\n",
        "    results['peak_capital'] = results['capital'].cummax()\n",
        "    results['drawdown'] = (results['capital'] - results['peak_capital']) / results['peak_capital']\n",
        "\n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yZzmiv4ijJqx"
      },
      "outputs": [],
      "source": [
        "# Backtesting - Performance Analysis\n",
        "def analyze_backtest(backtest_results):\n",
        "    \"\"\"Calculate performance metrics and display results\"\"\"\n",
        "    results = backtest_results.copy()\n",
        "\n",
        "    # Calculate basic metrics\n",
        "    total_return = results['cum_return'].iloc[-1]\n",
        "    market_return = results['cum_market_return'].iloc[-1]\n",
        "    max_drawdown = results['drawdown'].min()\n",
        "\n",
        "    # Calculate annualized return\n",
        "    days = (results.index[-1] - results.index[0]).days\n",
        "    years = max(days / 365, 0.01)\n",
        "    annual_return = (1 + total_return) ** (1/years) - 1\n",
        "    market_annual = (1 + market_return) ** (1/years) - 1\n",
        "\n",
        "    # Calculate Sharpe ratio (annualized)\n",
        "    risk_free_rate = 0.02  # 2% annual risk-free rate\n",
        "    excess_returns = results['daily_return'] - ((1 + risk_free_rate) ** (1/252) - 1)\n",
        "    sharpe_ratio = np.sqrt(252) * excess_returns.mean() / excess_returns.std() if excess_returns.std() > 0 else 0\n",
        "\n",
        "    # Calculate win rate and trades\n",
        "    trade_changes = results['position'].diff().fillna(0)\n",
        "    total_trades = (trade_changes != 0).sum() / 2\n",
        "    winning_days = (results['daily_return'] > 0).sum()\n",
        "    losing_days = (results['daily_return'] < 0).sum()\n",
        "    win_rate = winning_days / (winning_days + losing_days) if (winning_days + losing_days) > 0 else 0\n",
        "\n",
        "    # Print results\n",
        "    print(\"\\nPerformance Summary:\")\n",
        "    print(f\"Total Return: {total_return:.2%} (Market: {market_return:.2%})\")\n",
        "    print(f\"Annual Return: {annual_return:.2%} (Market: {market_annual:.2%})\")\n",
        "    print(f\"Sharpe Ratio: {sharpe_ratio:.2f}\")\n",
        "    print(f\"Max Drawdown: {max_drawdown:.2%}\")\n",
        "    print(f\"Total Trades: {total_trades:.0f}\")\n",
        "    print(f\"Win Rate: {win_rate:.2%} ({winning_days}/{winning_days+losing_days})\")\n",
        "\n",
        "    return {\n",
        "        'total_return': total_return,\n",
        "        'annual_return': annual_return,\n",
        "        'sharpe_ratio': sharpe_ratio,\n",
        "        'max_drawdown': max_drawdown,\n",
        "        'total_trades': total_trades,\n",
        "        'win_rate': win_rate\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LU03zfWjjOe1"
      },
      "outputs": [],
      "source": [
        "# Visualization\n",
        "def visualize_backtest(backtest_results, target_col):\n",
        "    \"\"\"Create visualization of backtest results\"\"\"\n",
        "    results = backtest_results.copy()\n",
        "\n",
        "    # Plot 1: Actual vs Predicted\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.plot(results.index, results['actual'], 'b-', label='Actual')\n",
        "    plt.plot(results.index, results['predicted'], 'r-', label='Predicted')\n",
        "\n",
        "    if '7D_Pct_Change' in target_col:\n",
        "        title = '7-Day Percentage Change'\n",
        "        ylabel = '7-Day % Change'\n",
        "    elif '2D_Pct_Change' in target_col:\n",
        "        title = '2-Day Percentage Change'\n",
        "        ylabel = '2-Day % Change'\n",
        "    else:\n",
        "        title = 'Value'\n",
        "        ylabel = 'Value'\n",
        "\n",
        "    plt.title(f'Actual vs Predicted {title}')\n",
        "    plt.xlabel('Date')\n",
        "    plt.ylabel(ylabel)\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Plot 2: Capital Growth\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.plot(results.index, results['capital'], 'g-', label='Strategy Capital')\n",
        "\n",
        "    # Add buy-and-hold for comparison\n",
        "    price_col = 'price' if 'price' in results.columns else 'synth_price'\n",
        "    initial_capital = results['capital'].iloc[0]\n",
        "    initial_price = results[price_col].iloc[0]\n",
        "    initial_shares = initial_capital / initial_price\n",
        "    buy_hold_capital = initial_shares * results[price_col]\n",
        "    plt.plot(results.index, buy_hold_capital, 'b-', label='Buy & Hold')\n",
        "\n",
        "    plt.title('Capital Growth')\n",
        "    plt.xlabel('Date')\n",
        "    plt.ylabel('Capital ($)')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Plot 3: Cumulative Returns\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.plot(results.index, results['cum_return'], 'g-', label='Strategy')\n",
        "    plt.plot(results.index, results['cum_market_return'], 'b-', label='Market')\n",
        "    plt.title('Cumulative Returns')\n",
        "    plt.xlabel('Date')\n",
        "    plt.ylabel('Return')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Plot 4: Drawdowns\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.plot(results.index, results['drawdown'], 'r-')\n",
        "    plt.title('Strategy Drawdowns')\n",
        "    plt.xlabel('Date')\n",
        "    plt.ylabel('Drawdown')\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Plot 5: Position over time\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.plot(results.index, results['position'], 'b-')\n",
        "    plt.title('Position Over Time')\n",
        "    plt.xlabel('Date')\n",
        "    plt.ylabel('Position (1=Long, -1=Short, 0=Cash)')\n",
        "    plt.yticks([-1, 0, 1])\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AY-5RAM1jVmt"
      },
      "outputs": [],
      "source": [
        "# Overall Model Prediction\n",
        "def backtest_model(model, X, target_col='Original_7D_Pct_Change',\n",
        "                  test_size=0.3, step_size=0.05,\n",
        "                  initial_capital=10000, position_size=0.95,\n",
        "                  stop_loss=None, take_profit=None,\n",
        "                  plot=True):\n",
        "    \"\"\"Main function to run backtest with model predictions\"\"\"\n",
        "\n",
        "    # Step 1: Generate predictions\n",
        "    print(\"Step 1: Generating predictions...\")\n",
        "    prediction_results = generate_predictions(\n",
        "        model=model, X=X, target_col=target_col,\n",
        "        test_size=test_size, step_size=step_size\n",
        "    )\n",
        "\n",
        "    # Step 2: Apply trading strategy\n",
        "    print(\"\\nStep 2: Applying trading strategy...\")\n",
        "    backtest_results = apply_trading_strategy(\n",
        "        prediction_results=prediction_results,\n",
        "        target_col=target_col,\n",
        "        initial_capital=initial_capital,\n",
        "        position_size=position_size,\n",
        "        stop_loss=stop_loss,\n",
        "        take_profit=take_profit\n",
        "    )\n",
        "\n",
        "    # Step 3: Analyze results\n",
        "    print(\"\\nStep 3: Analyzing performance...\")\n",
        "    performance = analyze_backtest(backtest_results)\n",
        "\n",
        "    # Step 4: Visualize results\n",
        "    if plot:\n",
        "        print(\"\\nStep 4: Generating visualizations...\")\n",
        "        visualize_backtest(backtest_results, target_col)\n",
        "\n",
        "    return backtest_results, performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8U0XHP6gjgd-"
      },
      "outputs": [],
      "source": [
        "feature_cols = [col for col in final_df.columns if not any(x in col for x in [\n",
        "    'YeoJohnson_', 'Original_']+ exclude_list)]\n",
        "\n",
        "# Add the necessary columns for backtesting\n",
        "backtest_df = final_df[feature_cols].copy()\n",
        "backtest_df['Original_7D_Pct_Change'] = final_df['Original_7D_Pct_Change']  # target\n",
        "backtest_df['Original_Price'] = final_df['Original_Price']  # for calculating returns\n",
        "\n",
        "# Run backtest with clean DataFrame\n",
        "backtest_results_basic, performance_basic = backtest_model(\n",
        "    model=regressor,\n",
        "    X=backtest_df,\n",
        "    target_col='Original_7D_Pct_Change',\n",
        "    test_size=0.25,\n",
        "    step_size=0.02,\n",
        "    initial_capital=25000,\n",
        "    position_size=0.8,\n",
        "    stop_loss=0.04,\n",
        "    take_profit=0.08,\n",
        "    plot=True\n",
        ")\n",
        "\n",
        "\n",
        "'''# Example 1: Random Forest model with default settings\n",
        "backtest_results_rf, performance_rf = backtest_model(\n",
        "    model=rf_tuned_regressor,\n",
        "    X=backtest_df,\n",
        "    target_col='Original_7D_Pct_Change'\n",
        ")'''\n",
        "\n",
        "'''# Example 2: Baseline OLS model with modified test size and step size\n",
        "backtest_results_baseline, performance_baseline = backtest_model(\n",
        "    model=model, # baseline OLS model\n",
        "    X=backtest_df,\n",
        "    target_col='Original_7D_Pct_Change',\n",
        "    test_size=0.2,  # Use only 20% of data for testing\n",
        "    step_size=0.1,   # Take larger steps in walk-forward validation\n",
        "    initial_capital=10000,\n",
        "    position_size=0.95,\n",
        "    stop_loss=0.05,\n",
        "    take_profit=0.05,)'''\n",
        "\n",
        "'''# Example 3: Reduced model with different capital and position size\n",
        "backtest_results_reduced, performance_reduced = backtest_model(\n",
        "    model=model_reduce,\n",
        "    X=backtest_df,\n",
        "    target_col='Original_7D_Pct_Change',\n",
        "    initial_capital=50000,  # Start with $50,000\n",
        "    position_size=0.75      # Use 75% of capital per trade\n",
        ")\n",
        "\n",
        "# Example 4: VIF-reduced model with stop loss and take profit\n",
        "backtest_results_vif, performance_vif = backtest_model(\n",
        "    model=model_reduce_vif,\n",
        "    X=backtest_df,\n",
        "    target_col='Original_7D_Pct_Change',\n",
        "    stop_loss=0.03,     # 3% stop loss\n",
        "    take_profit=0.05    # 5% take profit\n",
        ")\n",
        "\n",
        "# Example 5: Final RF model with custom settings\n",
        "backtest_results_final_rf, performance_final_rf = backtest_model(\n",
        "    model=final_rfmodel,\n",
        "    X=backtest_df,\n",
        "    target_col='Original_7D_Pct_Change',\n",
        "    test_size=0.25,\n",
        "    step_size=0.02,\n",
        "    initial_capital=25000,\n",
        "    position_size=0.8,\n",
        "    stop_loss=0.04,\n",
        "    take_profit=0.08,\n",
        "    plot=True\n",
        ")\n",
        "\n",
        "# Example 6: Basic regressor\n",
        "backtest_results_basic, performance_basic = backtest_model(\n",
        "    model=regressor,\n",
        "    X=scaled_btc_data,\n",
        "    target_col='Original_7D_Pct_Change',\n",
        "    test_size=0.25,\n",
        "    step_size=0.02,\n",
        "    initial_capital=25000,\n",
        "    position_size=0.8,\n",
        "    stop_loss=0.04,\n",
        "    take_profit=0.08,\n",
        "    plot=True\n",
        ")\n",
        "\n",
        "# Example 7: Testing different target column with RF model\n",
        "backtest_results_2d, performance_2d = backtest_model(\n",
        "    model=rf_tuned_regressor,\n",
        "    X=scaled_btc_data,\n",
        "    target_col='Original_2D_Pct_Change',  # Predict 2-day changes instead\n",
        "    test_size=0.3,\n",
        "    step_size=0.05\n",
        ")'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mDJkRvMpt4zU"
      },
      "source": [
        "##Predicting % of Bitcoin Price Change in 7 Days"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cP89-VPUjnSi"
      },
      "outputs": [],
      "source": [
        "def predict_future_bitcoin_change(model, current_data, confidence_level=0.95):\n",
        "    \"\"\"\n",
        "    Predict the 7-day percentage change in Bitcoin price from today.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    model : trained model\n",
        "        The trained prediction model\n",
        "    current_data : DataFrame\n",
        "        Contains the latest feature values\n",
        "    confidence_level : float\n",
        "        Confidence level for prediction intervals\n",
        "    \"\"\"\n",
        "    # Extract only features the model was trained with\n",
        "    if hasattr(model, 'feature_names_in_'):\n",
        "        model_features = list(model.feature_names_in_)\n",
        "    else:\n",
        "        model_features = [col for col in current_data.columns\n",
        "                          if not any(x in col for x in ['YeoJohnson_', 'Original_'])]\n",
        "\n",
        "    # Get current date and price\n",
        "    current_date = current_data.index[-1]\n",
        "    current_price = current_data['Original_Price'].iloc[-1] if 'Original_Price' in current_data.columns else None\n",
        "    target_date = current_date + pd.Timedelta(days=7)\n",
        "\n",
        "    # Make prediction\n",
        "    latest_features = current_data[model_features].iloc[-1:].copy()\n",
        "    prediction = model.predict(latest_features)[0]\n",
        "\n",
        "    # Calculate confidence interval (for RandomForest models)\n",
        "    if hasattr(model, 'estimators_'):\n",
        "        tree_predictions = [tree.predict(latest_features)[0] for tree in model.estimators_]\n",
        "        alpha = 1 - confidence_level\n",
        "        lower_bound = np.percentile(tree_predictions, (alpha/2)*100)\n",
        "        upper_bound = np.percentile(tree_predictions, (1-alpha/2)*100)\n",
        "        std_dev = np.std(tree_predictions)\n",
        "    else:\n",
        "        std_dev = abs(prediction) * 0.2\n",
        "        lower_bound = prediction - 1.96 * std_dev\n",
        "        upper_bound = prediction + 1.96 * std_dev\n",
        "\n",
        "    # Determine signal\n",
        "    if prediction > 2.0:\n",
        "        signal = \"BUY\"\n",
        "    elif prediction > 0:\n",
        "        signal = \"WEAK BUY\"\n",
        "    elif prediction < -2.0:\n",
        "        signal = \"SELL\"\n",
        "    elif prediction < 0:\n",
        "        signal = \"WEAK SELL\"\n",
        "    else:\n",
        "        signal = \"NEUTRAL\"\n",
        "\n",
        "    # Calculate price targets if price data is available\n",
        "    if current_price:\n",
        "        predicted_price = current_price * (1 + prediction/100)\n",
        "        lower_price = current_price * (1 + lower_bound/100)\n",
        "        upper_price = current_price * (1 + upper_bound/100)\n",
        "\n",
        "        # Calculate stop loss and take profit\n",
        "        if signal in [\"BUY\", \"WEAK BUY\"]:\n",
        "            stop_loss = current_price * 0.95\n",
        "            take_profit = current_price * (1 + max(prediction/100 * 1.5, 0.1))\n",
        "        elif signal in [\"SELL\", \"WEAK SELL\"]:\n",
        "            stop_loss = current_price * 1.05\n",
        "            take_profit = current_price * (1 + min(prediction/100 * 1.5, -0.1))\n",
        "        else:\n",
        "            stop_loss = take_profit = None\n",
        "\n",
        "    # Display results\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"BITCOIN 7-DAY PRICE PREDICTION ({confidence_level*100:.0f}% CONFIDENCE)\")\n",
        "    print(f\"{'='*60}\")\n",
        "    print(f\"Current Date: {current_date.strftime('%Y-%m-%d')}\")\n",
        "    print(f\"Target Date:  {target_date.strftime('%Y-%m-%d')}\")\n",
        "\n",
        "    if current_price:\n",
        "        print(f\"\\nCurrent Price: ${current_price:,.2f}\")\n",
        "        print(f\"Predicted Price: ${predicted_price:,.2f}\")\n",
        "        print(f\"Price Range: ${lower_price:,.2f} to ${upper_price:,.2f}\")\n",
        "\n",
        "    print(f\"\\nPredicted 7-Day Change: {prediction:.2f}%\")\n",
        "    print(f\"Range: {lower_bound:.2f}% to {upper_bound:.2f}%\")\n",
        "    print(f\"\\nSIGNAL: {signal}\")\n",
        "\n",
        "    # Show trading advice\n",
        "    print(\"\\nTrading Advice:\", end=\" \")\n",
        "    if signal == \"BUY\":\n",
        "        print(\"Strong buying opportunity.\")\n",
        "        if current_price:\n",
        "            print(f\"Stop Loss: ${stop_loss:,.2f} (-5.00%)\")\n",
        "            print(f\"Take Profit: ${take_profit:,.2f} (+{(take_profit/current_price-1)*100:.2f}%)\")\n",
        "    elif signal == \"WEAK BUY\":\n",
        "        print(\"Mild bullish signal.\")\n",
        "    elif signal == \"SELL\":\n",
        "        print(\"Strong selling opportunity.\")\n",
        "        if current_price:\n",
        "            print(f\"Stop Loss: ${stop_loss:,.2f} (+5.00%)\")\n",
        "            print(f\"Take Profit: ${take_profit:,.2f} ({(take_profit/current_price-1)*100:.2f}%)\")\n",
        "    elif signal == \"WEAK SELL\":\n",
        "        print(\"Mild bearish signal.\")\n",
        "    else:\n",
        "        print(\"Hold current positions or stay out of the market.\")\n",
        "\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    # Visualize prediction\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    if current_price:\n",
        "        dates = [current_date, target_date]\n",
        "        plt.scatter([current_date], [current_price], color='blue', s=100, label='Current Price')\n",
        "        plt.scatter([target_date], [predicted_price], color='red', s=100, label='Predicted Price')\n",
        "        plt.plot(dates, [current_price, predicted_price], 'r--')\n",
        "\n",
        "        # Show confidence interval\n",
        "        plt.fill_between(\n",
        "            [target_date-pd.Timedelta(days=0.5), target_date+pd.Timedelta(days=0.5)],\n",
        "            [lower_price, lower_price],\n",
        "            [upper_price, upper_price],\n",
        "            color='red', alpha=0.2\n",
        "        )\n",
        "        plt.title(f'Bitcoin 7-Day Price Prediction: {prediction:.2f}%')\n",
        "        plt.ylabel('Price ($)')\n",
        "    else:\n",
        "        # Just show percentage change\n",
        "        plt.bar(['Predicted Change'], [prediction], color='red')\n",
        "        plt.axhline(y=0, color='black', linestyle='-', alpha=0.3)\n",
        "        plt.title(f'Predicted 7-Day Change: {prediction:.2f}%')\n",
        "        plt.ylabel('Percent Change (%)')\n",
        "\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    return {\n",
        "        'current_date': current_date,\n",
        "        'target_date': target_date,\n",
        "        'prediction_percent': prediction,\n",
        "        'lower_bound': lower_bound,\n",
        "        'upper_bound': upper_bound,\n",
        "        'signal': signal,\n",
        "        'current_price': current_price if current_price else None,\n",
        "        'predicted_price': predicted_price if current_price else None\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bnwf7QE4jwgy"
      },
      "outputs": [],
      "source": [
        "# Predict % of bitcoin price change in 7 days (assuming the dataset is up-to-date)\n",
        "prediction = predict_future_bitcoin_change(\n",
        "    model=regressor,\n",
        "    current_data=final_df,\n",
        "    confidence_level=0.95\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
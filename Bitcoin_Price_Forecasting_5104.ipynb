{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ArthurChan-1111/Bitcoin_prediction/blob/main/Bitcoin_Price_Forecasting_5104.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JlqCL3Z5-Lpg"
      },
      "source": [
        "#Import Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "fe_jdv1pJbmt",
        "outputId": "24e4ed96-6afb-43ad-ed00-d547dd294921"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pandas_ta\n",
            "  Downloading pandas_ta-0.3.14b.tar.gz (115 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.1/115.1 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from pandas_ta) (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas->pandas_ta) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->pandas_ta) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->pandas_ta) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->pandas_ta) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->pandas_ta) (1.17.0)\n",
            "Building wheels for collected packages: pandas_ta\n",
            "  Building wheel for pandas_ta (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pandas_ta: filename=pandas_ta-0.3.14b0-py3-none-any.whl size=218910 sha256=4703cb1e22a20d1cb127b2f313fc23c8906d2e53057d7c06ed97b4f0a9edf4c2\n",
            "  Stored in directory: /root/.cache/pip/wheels/7f/33/8b/50b245c5c65433cd8f5cb24ac15d97e5a3db2d41a8b6ae957d\n",
            "Successfully built pandas_ta\n",
            "Installing collected packages: pandas_ta\n",
            "Successfully installed pandas_ta-0.3.14b0\n",
            "Collecting numpy==1.23.5\n",
            "  Downloading numpy-1.23.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.3 kB)\n",
            "Downloading numpy-1.23.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.1/17.1 MB\u001b[0m \u001b[31m46.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 1.23.5 which is incompatible.\n",
            "chex 0.1.89 requires numpy>=1.24.1, but you have numpy 1.23.5 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.23.5 which is incompatible.\n",
            "scikit-image 0.25.2 requires numpy>=1.24, but you have numpy 1.23.5 which is incompatible.\n",
            "blosc2 3.3.1 requires numpy>=1.26, but you have numpy 1.23.5 which is incompatible.\n",
            "albucore 0.0.23 requires numpy>=1.24.4, but you have numpy 1.23.5 which is incompatible.\n",
            "xarray 2025.3.1 requires numpy>=1.24, but you have numpy 1.23.5 which is incompatible.\n",
            "imbalanced-learn 0.13.0 requires numpy<3,>=1.24.3, but you have numpy 1.23.5 which is incompatible.\n",
            "albumentations 2.0.5 requires numpy>=1.24.4, but you have numpy 1.23.5 which is incompatible.\n",
            "jax 0.5.2 requires numpy>=1.25, but you have numpy 1.23.5 which is incompatible.\n",
            "treescope 0.1.9 requires numpy>=1.25.2, but you have numpy 1.23.5 which is incompatible.\n",
            "jaxlib 0.5.1 requires numpy>=1.25, but you have numpy 1.23.5 which is incompatible.\n",
            "bigframes 2.1.0 requires numpy>=1.24.0, but you have numpy 1.23.5 which is incompatible.\n",
            "pymc 5.22.0 requires numpy>=1.25.0, but you have numpy 1.23.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-1.23.5\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              },
              "id": "71bb133fd8164df1b5350b913b58f5d1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (1.23.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (0.13.2)\n",
            "Requirement already satisfied: numpy!=1.24.0,>=1.20 in /usr/local/lib/python3.11/dist-packages (from seaborn) (1.23.5)\n",
            "Requirement already satisfied: pandas>=1.2 in /usr/local/lib/python3.11/dist-packages (from seaborn) (2.2.2)\n",
            "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in /usr/local/lib/python3.11/dist-packages (from seaborn) (3.10.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.2->seaborn) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.2->seaborn) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn) (1.17.0)\n",
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.11/dist-packages (2.1.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from xgboost) (1.23.5)\n",
            "Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.11/dist-packages (from xgboost) (2.21.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from xgboost) (1.15.2)\n",
            "Cloning into 'Bitcoin_prediction'...\n",
            "remote: Enumerating objects: 622, done.\u001b[K\n",
            "remote: Counting objects: 100% (87/87), done.\u001b[K\n",
            "remote: Compressing objects: 100% (69/69), done.\u001b[K\n",
            "remote: Total 622 (delta 66), reused 22 (delta 18), pack-reused 535 (from 2)\u001b[K\n",
            "Receiving objects: 100% (622/622), 218.65 MiB | 27.05 MiB/s, done.\n",
            "Resolving deltas: 100% (339/339), done.\n",
            "/content/Bitcoin_prediction\n",
            "Requirement already satisfied: pandas_ta in /usr/local/lib/python3.11/dist-packages (0.3.14b0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from pandas_ta) (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas->pandas_ta) (1.23.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->pandas_ta) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->pandas_ta) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->pandas_ta) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->pandas_ta) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "# Check if running in Colab\n",
        "try:\n",
        "    import google.colab\n",
        "    IN_COLAB = True\n",
        "except:\n",
        "    IN_COLAB = False\n",
        "\n",
        "if IN_COLAB:\n",
        "    !pip install pandas_ta\n",
        "    !pip install numpy==1.23.5\n",
        "    !pip install pandas\n",
        "    !pip install seaborn\n",
        "    !pip install xgboost\n",
        "\n",
        "    !git clone https://github.com/ArthurChan-1111/Bitcoin_prediction.git\n",
        "\n",
        "    %cd Bitcoin_prediction\n",
        "    %pip install pandas_ta\n",
        "else:\n",
        "    print(\"Not running in Colab, skipping git clone and directory change\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RfF2sFtH-Xjn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "89ca721f-ad60-4b27-8b28-9a1ad6690344"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-e3e3b9a6b570>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# General Libraries\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas_ta\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mta\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcsv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     35\u001b[0m     ) from _err\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m from pandas._config import (\n\u001b[0m\u001b[1;32m     38\u001b[0m     \u001b[0mget_option\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mset_option\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/_config/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;34m\"warn_copy_on_write\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m ]\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_config\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_config\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdates\u001b[0m  \u001b[0;31m# pyright: ignore[reportUnusedImport]  # noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m from pandas._config.config import (\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/_config/config.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m from pandas._typing import (\n\u001b[0m\u001b[1;32m     69\u001b[0m     \u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/_typing.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 198\u001b[0;31m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGenerator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBitGenerator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRandomState\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numpy/__init__.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(attr)\u001b[0m\n\u001b[1;32m    335\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m2.0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1e-5\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAssertionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 337\u001b[0;31m         \u001b[0;32mexcept\u001b[0m \u001b[0mAssertionError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    338\u001b[0m             msg = (\"The current Numpy installation ({!r}) fails to \"\n\u001b[1;32m    339\u001b[0m                    \u001b[0;34m\"pass simple sanity checks. This can be caused for example \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numpy/random/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[0;31m# add these for module-freeze analysis (like PyInstaller)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_pickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_common\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_bounded_integers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numpy/random/_pickle.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmtrand\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRandomState\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_philox\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPhilox\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_pcg64\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPCG64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPCG64DXSM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_sfc64\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSFC64\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mmtrand.pyx\u001b[0m in \u001b[0;36minit numpy.random.mtrand\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject"
          ]
        }
      ],
      "source": [
        "# General Libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pandas_ta as ta\n",
        "import csv\n",
        "import warnings\n",
        "from tqdm import tqdm\n",
        "from tqdm.auto import tqdm\n",
        "from scipy import stats\n",
        "from scipy.stats import zscore\n",
        "\n",
        "# Visualization Libraries\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import pylab as py\n",
        "\n",
        "# Data Preprocessing\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "\n",
        "# General Model Selection and Metrics\n",
        "from sklearn.model_selection import (\n",
        "    train_test_split,\n",
        "    GridSearchCV,\n",
        "    RandomizedSearchCV,\n",
        "    TimeSeriesSplit,\n",
        "    RepeatedKFold,\n",
        "    cross_val_score,\n",
        "    StratifiedKFold\n",
        ")\n",
        "\n",
        "from sklearn.metrics import (\n",
        "    mean_squared_error,\n",
        "    mean_absolute_error,\n",
        "    r2_score\n",
        ")\n",
        "\n",
        "# Regression Models and Tools\n",
        "from sklearn.linear_model import (\n",
        "    LinearRegression,\n",
        "    Ridge,\n",
        "    Lasso,\n",
        "    ElasticNet,\n",
        "    RidgeCV,\n",
        "    LassoCV,\n",
        "    ElasticNetCV\n",
        ")\n",
        "\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import (\n",
        "    BaggingRegressor,\n",
        "    RandomForestRegressor,\n",
        "    GradientBoostingRegressor,\n",
        "    AdaBoostRegressor,\n",
        "    StackingRegressor\n",
        ")\n",
        "from xgboost import XGBRegressor\n",
        "\n",
        "# Stats Models\n",
        "import statsmodels.api as sm\n",
        "import statsmodels.formula.api as smf\n",
        "import statsmodels.stats.api as sms\n",
        "from statsmodels.stats.outliers_influence import (\n",
        "    variance_inflation_factor,\n",
        "    summary_table,\n",
        "    OLSInfluence\n",
        ")\n",
        "from statsmodels.regression.linear_model import OLSResults\n",
        "from statsmodels.stats.stattools import durbin_watson as dwtest\n",
        "from statsmodels.sandbox.stats.runs import runstest_1samp\n",
        "from statsmodels.stats.diagnostic import het_white\n",
        "from statsmodels.compat import lzip\n",
        "\n",
        "# Classification Models and Metrics\n",
        "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.inspection import permutation_importance\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score,\n",
        "    precision_score,\n",
        "    recall_score,\n",
        "    f1_score,\n",
        "    confusion_matrix,\n",
        "    classification_report,\n",
        "    roc_curve,\n",
        "    auc,\n",
        "    roc_auc_score\n",
        ")\n",
        "\n",
        "# Settings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "pd.set_option(\"display.float_format\", lambda x: \"%.4f\" % x)\n",
        "pd.set_option(\"display.max_columns\", None)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k5CupF2I1WTP"
      },
      "source": [
        "#Load dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CTX7TsDYc3wx"
      },
      "outputs": [],
      "source": [
        "btc_data = pd.read_csv('Bitcoin Historical Data.csv', on_bad_lines='skip', lineterminator='\\n')\n",
        "# Check if 'Change %\\r' exists and rename it to 'Change'\n",
        "if 'Change %\\r' in btc_data.columns:\n",
        "    btc_data.rename(columns={'Change %\\r': 'Change'}, inplace=True)\n",
        "btc_data.tail(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XWH7v2wHeHJn"
      },
      "outputs": [],
      "source": [
        "# Convert the dictionary to a DataFrame\n",
        "btc_data = pd.DataFrame(btc_data)\n",
        "\n",
        "# Parse the \"Date\" column into datetime format\n",
        "btc_data[\"Date\"] = pd.to_datetime(btc_data[\"Date\"], format=\"%m/%d/%Y\")\n",
        "\n",
        "# Sort the data by date in ascending order\n",
        "btc_data.sort_values(by=\"Date\", ascending=True, inplace=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0uoXtNE4qYsG"
      },
      "outputs": [],
      "source": [
        "gold_price_data = pd.read_csv('gold_price_data.csv', on_bad_lines='skip', lineterminator='\\n', sep=';')\n",
        "# Check if 'Gold_Volume %\\r' exists and rename it to 'Gold_Volume %'\n",
        "if 'Gold_Volume\\r' in gold_price_data.columns:\n",
        "    gold_price_data.rename(columns={'Gold_Volume\\r': 'Gold_Volume'}, inplace=True)\n",
        "gold_price_data.tail(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wtDmOlHZrsXk"
      },
      "outputs": [],
      "source": [
        "risk_free_data = pd.read_csv('risk_free_data.csv', on_bad_lines='skip', lineterminator='\\n', sep=',')\n",
        "# rename the Yield_Spread\\r to Yield_Spread\n",
        "risk_free_data.rename(columns={'Yield_Spread\\r': 'Yield_Spread'}, inplace=True)\n",
        "risk_free_data.tail(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pTZaq_uv-GPH"
      },
      "outputs": [],
      "source": [
        "spy_data = pd.read_csv('spy_data.csv', on_bad_lines='skip', lineterminator='\\n', sep=',')\n",
        "# Check if 'SPY_Volume\\r' exists and rename it to 'SPY_Volume'\n",
        "if 'SPY_Volume\\r' in spy_data.columns:\n",
        "    spy_data.rename(columns={'SPY_Volume\\r': 'SPY_Volume'}, inplace=True)\n",
        "spy_data.tail(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wM2GhEuUFNUq"
      },
      "outputs": [],
      "source": [
        "eth_data = pd.read_csv('eth_data.csv', on_bad_lines='skip', lineterminator='\\n', sep=',')\n",
        "\n",
        "# Remove the dollar sign and convert 'ETH_Price' to a numeric type\n",
        "eth_data['ETH_Price'] = eth_data['ETH_Price'].replace('[\\$,]', '', regex=True).astype(float)\n",
        "\n",
        "# Check if 'ETH_Volume %\\r' exists and rename it to 'ETH_Volume %'\n",
        "if 'ETH_Volume\\r' in eth_data.columns:\n",
        "    eth_data.rename(columns={'ETH_Volume\\r': 'ETH_Volume'}, inplace=True)\n",
        "\n",
        "eth_data.tail(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yRmPrKkqrw-6"
      },
      "outputs": [],
      "source": [
        "#Change the 'Date' to datetime format\n",
        "gold_price_data['Date'] = pd.to_datetime(gold_price_data['Date'])\n",
        "risk_free_data['Date'] = pd.to_datetime(risk_free_data['Date'])\n",
        "spy_data['Date'] = pd.to_datetime(spy_data['Date'])\n",
        "eth_data['Date'] = pd.to_datetime(eth_data['Date'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KgU2-lQ2Awx1"
      },
      "source": [
        "#Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uImVKfuKAp7F"
      },
      "outputs": [],
      "source": [
        "#Function to clean and convert volume data\n",
        "def clean_volume(volume):\n",
        "    if isinstance(volume, str):  # Check if the value is a string\n",
        "        volume = volume.replace(',', '')  # Remove commas\n",
        "        if 'B' in volume:  # If the value contains 'B' (billions)\n",
        "            return float(volume.replace('B', '')) * 1_000_000_000\n",
        "        elif 'M' in volume:  # If the value contains 'M' (millions)\n",
        "            return float(volume.replace('M', '')) * 1_000_000\n",
        "        elif 'K' in volume:  # If the value contains 'K' (thousands)\n",
        "            return float(volume.replace('K', '')) * 1_000\n",
        "        else:  # If no suffix is present, convert to float directly\n",
        "            return float(volume)\n",
        "    return np.nan  # Handle unexpected cases\n",
        "\n",
        "# Apply the cleaning function to the volume column\n",
        "btc_data[\"Volume\"] = btc_data[\"Vol. ('000)\"].apply(clean_volume)\n",
        "btc_data = btc_data.drop(\"Vol. ('000)\", axis=1)\n",
        "\n",
        "# 4. Remove '%' from \"Change %\" and convert to numeric\n",
        "btc_data.rename(columns={\"Change %\\r\": \"Change\"}, inplace=True)\n",
        "btc_data[\"Change\"] = btc_data[\"Change\"].str.replace(\"%\", \"\").str.strip().astype(float)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o7RHHNjbHqiC"
      },
      "outputs": [],
      "source": [
        "# Define the missing data as a dictionary\n",
        "missing_data = {\n",
        "    'Date': '01/12/2025',\n",
        "    'Price': 94541.8,\n",
        "    'Open': 94600.0,\n",
        "    'High': 95384.3,\n",
        "    'Low': 93711.2,\n",
        "    'Volume': 17.60,  # Assuming 'Vol. (\\'000)' is in thousands\n",
        "    'Change': -0.07  # Assuming 'Change %' is already converted to a float\n",
        "}\n",
        "\n",
        "# Convert the dictionary to a DataFrame\n",
        "missing_data_df = pd.DataFrame([missing_data])\n",
        "\n",
        "# Ensure the 'Date' column is in datetime format\n",
        "missing_data_df['Date'] = pd.to_datetime(missing_data_df['Date'])\n",
        "\n",
        "# Append the missing data to btc_data\n",
        "btc_data = pd.concat([btc_data, missing_data_df], ignore_index=True)\n",
        "\n",
        "# Filter the data for the date range 01/10/2025 to 01/20/2025\n",
        "filtered_data = btc_data[(btc_data['Date'] >= '2025-01-10') & (btc_data['Date'] <= '2025-01-20')]\n",
        "\n",
        "# Display the filtered data\n",
        "print(filtered_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kOl-feBeiLbJ"
      },
      "outputs": [],
      "source": [
        "# Day of the week (0=Monday, 6=Sunday)\n",
        "btc_data[\"Day_of_Week\"] = btc_data[\"Date\"].dt.dayofweek\n",
        "\n",
        "# Week of the year\n",
        "btc_data[\"Week_of_Year\"] = btc_data[\"Date\"].dt.isocalendar().week\n",
        "\n",
        "# Month of the year\n",
        "btc_data[\"Month\"] = btc_data[\"Date\"].dt.month\n",
        "\n",
        "# Quarter of the year\n",
        "btc_data[\"Quarter\"] = btc_data[\"Date\"].dt.quarter\n",
        "\n",
        "# Year\n",
        "btc_data[\"Year\"] = btc_data[\"Date\"].dt.year"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tzdpX4IHvXDW"
      },
      "source": [
        "## Signal Detection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hYeEyw86J_lU"
      },
      "outputs": [],
      "source": [
        "# Calculate RSI, EMA, SMA, and MACD\n",
        "btc_data[\"RSI_6\"] = ta.rsi(btc_data[\"Price\"], length=6)  # Relative Strength Index\n",
        "btc_data[\"RSI_12\"] = ta.rsi(btc_data[\"Price\"], length=12)  # Relative Strength Index\n",
        "btc_data[\"EMA_14\"] = ta.ema(btc_data[\"Price\"], length=14)  # Exponential Moving Average\n",
        "btc_data[\"SMA_14\"] = ta.sma(btc_data[\"Price\"], length=14)  # Simple Moving Average\n",
        "\n",
        "# Add On-Balance Volume (OBV)\n",
        "btc_data[\"OBV\"] = ta.obv(btc_data[\"Price\"], btc_data[\"Volume\"])\n",
        "\n",
        "# Calculate MACD --------------------------------------------------------------------------------------\n",
        "macd = ta.macd(btc_data[\"Price\"], fast=12, slow=26, signal=9)\n",
        "btc_data[\"MACD\"] = macd[\"MACD_12_26_9\"]\n",
        "btc_data[\"MACD_Signal\"] = macd[\"MACDs_12_26_9\"]\n",
        "\n",
        "# Create MACD_buy: 1 if MACD crosses above MACD_Signal, otherwise 0\n",
        "btc_data[\"MACD_buy\"] = 0  # Default to 0 (no buy signal)\n",
        "btc_data.loc[\n",
        "    (btc_data[\"MACD\"] > btc_data[\"MACD_Signal\"]) &  # MACD is above Signal\n",
        "    (btc_data[\"MACD\"].shift(1) <= btc_data[\"MACD_Signal\"].shift(1)),  # Previous MACD was below or equal to Signal\n",
        "    \"MACD_buy\"] = 1  # Buy signal\n",
        "\n",
        "# Create MACD_sell: 1 if MACD crosses below MACD_Signal, otherwise 0\n",
        "btc_data[\"MACD_sell\"] = 0  # Default to 0 (no sell signal)\n",
        "btc_data.loc[\n",
        "    (btc_data[\"MACD\"] <= btc_data[\"MACD_Signal\"]) &  # MACD is below or equal to Signal\n",
        "    (btc_data[\"MACD\"].shift(1) > btc_data[\"MACD_Signal\"].shift(1)),  # Previous MACD was above Signal\n",
        "    \"MACD_sell\"] = 1  # Sell signal\n",
        "\n",
        "# New Variable: MACD_above_signal (1 if MACD > MACD_Signal, otherwise 0)\n",
        "btc_data[\"MACD_above_signal\"] = (btc_data[\"MACD\"] > btc_data[\"MACD_Signal\"]).astype(int)\n",
        "\n",
        "# New Variable: MACD_below_signal (1 if MACD <= MACD_Signal, otherwise 0)\n",
        "btc_data[\"MACD_below_signal\"] = (btc_data[\"MACD\"] <= btc_data[\"MACD_Signal\"]).astype(int)\n",
        "\n",
        "# Create MACD_cum_buy: Accumulate consecutive days where MACD_above_signal is 1\n",
        "btc_data[\"MACD_cum_buy\"] = (\n",
        "    btc_data[\"MACD_above_signal\"]\n",
        "    .groupby((btc_data[\"MACD_above_signal\"] == 0).cumsum())  # Group by resets when MACD_above_signal is 0\n",
        "    .cumcount()  # Count consecutive days starting from 1\n",
        ")\n",
        "btc_data.loc[btc_data[\"MACD_above_signal\"] == 0, \"MACD_cum_buy\"] = 0  # Reset to 0 where the condition is not met\n",
        "\n",
        "# Create MACD_cum_sell: Accumulate consecutive days where MACD_below_signal is 1\n",
        "btc_data[\"MACD_cum_sell\"] = (\n",
        "    btc_data[\"MACD_below_signal\"]\n",
        "    .groupby((btc_data[\"MACD_below_signal\"] == 0).cumsum())  # Group by resets when MACD_below_signal is 0\n",
        "    .cumcount()  # Count consecutive days starting from 1\n",
        ")\n",
        "btc_data.loc[btc_data[\"MACD_below_signal\"] == 0, \"MACD_cum_sell\"] = 0  # Reset to 0\n",
        "\n",
        "\n",
        "# Calculate Bollinger Bands --------------------------------------------------------------------------------------\n",
        "bb = ta.bbands(btc_data[\"Price\"], length=20, std=2)  # 20-period BB with 2 standard deviations\n",
        "btc_data[\"BB_upper\"] = bb.get(\"BBU_20_2.0\")  # Upper Bollinger Band\n",
        "btc_data[\"BB_lower\"] = bb.get(\"BBL_20_2.0\")  # Lower Bollinger Band\n",
        "\n",
        "# Create BB_Buy: 1 if Price crosses below the Lower Band, otherwise 0\n",
        "btc_data[\"BB_Buy\"] = 0  # Default to 0 (no buy signal)\n",
        "btc_data.loc[\n",
        "    (btc_data[\"Price\"] < btc_data[\"BB_lower\"]) &  # Price is below the Lower Band\n",
        "    (btc_data[\"Price\"].shift(1) >= btc_data[\"BB_lower\"].shift(1)),  # Previous Price was above or equal to Lower Band\n",
        "    \"BB_Buy\"] = 1  # Buy signal\n",
        "\n",
        "# Create BB_Sell: 1 if Price crosses above the Upper Band, otherwise 0\n",
        "btc_data[\"BB_Sell\"] = 0  # Default to 0 (no sell signal)\n",
        "btc_data.loc[\n",
        "    (btc_data[\"Price\"] > btc_data[\"BB_upper\"]) &  # Price is above the Upper Band\n",
        "    (btc_data[\"Price\"].shift(1) <= btc_data[\"BB_upper\"].shift(1)),  # Previous Price was below or equal to Upper Band\n",
        "    \"BB_Sell\"] = 1  # Sell signal\n",
        "\n",
        "# New Variable: Price_above_lower_band (1 if Price > BB_lower, otherwise 0)\n",
        "btc_data[\"Price_below_BB_lower\"] = (btc_data[\"Price\"] < btc_data[\"BB_lower\"]).astype(int)\n",
        "\n",
        "# New Variable: Price_below_upper_band (1 if Price < BB_upper, otherwise 0)\n",
        "btc_data[\"Price_above_BB_upper\"] = (btc_data[\"Price\"] > btc_data[\"BB_upper\"]).astype(int)\n",
        "\n",
        "# Create BB_cum_Buy: Accumulate consecutive days where Price_below_BB_lower is 1\n",
        "btc_data[\"BB_cum_Buy\"] = (\n",
        "    btc_data[\"Price_below_BB_lower\"]\n",
        "    .groupby((btc_data[\"Price_below_BB_lower\"] == 0).cumsum())  # Group by resets when Price_below_BB_lower is 0\n",
        "    .cumcount()  # Count consecutive days starting from 1\n",
        ")\n",
        "btc_data.loc[btc_data[\"Price_below_BB_lower\"] == 0, \"BB_cum_Buy\"] = 0  # Reset to 0 where the condition is not met\n",
        "\n",
        "# Create BB_cum_Sell: Accumulate consecutive days where Price_above_BB_upper is 1\n",
        "btc_data[\"BB_cum_Sell\"] = (\n",
        "    btc_data[\"Price_above_BB_upper\"]\n",
        "    .groupby((btc_data[\"Price_above_BB_upper\"] == 0).cumsum())  # Group by resets when Price_above_BB_upper is 0\n",
        "    .cumcount()  # Count consecutive days starting from 1\n",
        ")\n",
        "btc_data.loc[btc_data[\"Price_above_BB_upper\"] == 0, \"BB_cum_Sell\"] = 0  # Reset to 0 where the condition is not met\n",
        "\n",
        "# Calculate ATR --------------------------------------------------------------------------------------\n",
        "btc_data[\"ATR\"] = ta.atr(btc_data[\"High\"], btc_data[\"Low\"], btc_data[\"Price\"], length=14)\n",
        "\n",
        "# Set Stop-Loss Levels (Example with Long Trade)\n",
        "atr_multiplier = 2\n",
        "btc_data[\"Stop_Loss_Long\"] = btc_data[\"Price\"] - (btc_data[\"ATR\"] * atr_multiplier) #Stop-Loss for Buy (Long) Trade\n",
        "btc_data[\"Stop_Loss_Short\"] = btc_data[\"Price\"] + (btc_data[\"ATR\"] * atr_multiplier) #Stop-Loss for Sell (Short) Trade\n",
        "\n",
        "# Calculate VWAP --------------------------------------------------------------------------------------\n",
        "btc_data.set_index(\"Date\", inplace=True) # Set the \"Date\" column as the index\n",
        "btc_data[\"VWAP\"] = ta.vwap(btc_data[\"High\"], btc_data[\"Low\"], btc_data[\"Price\"], btc_data[\"Volume\"])\n",
        "\n",
        "# Create VWAP_Buy: 1 if Price crosses above VWAP, otherwise 0\n",
        "btc_data[\"VWAP_Buy\"] = 0  # Default to 0 (no buy signal)\n",
        "btc_data.loc[\n",
        "    (btc_data[\"Price\"] > btc_data[\"VWAP\"]) &  # Price is above VWAP\n",
        "    (btc_data[\"Price\"].shift(1) <= btc_data[\"VWAP\"].shift(1)),  # Previous Price was below or equal to VWAP\n",
        "    \"VWAP_Buy\"] = 1  # Buy signal\n",
        "\n",
        "# Create VWAP_Sell: 1 if Price crosses below VWAP, otherwise 0\n",
        "btc_data[\"VWAP_Sell\"] = 0  # Default to 0 (no sell signal)\n",
        "btc_data.loc[\n",
        "    (btc_data[\"Price\"] < btc_data[\"VWAP\"]) &  # Price is below VWAP\n",
        "    (btc_data[\"Price\"].shift(1) >= btc_data[\"VWAP\"].shift(1)),  # Previous Price was above or equal to VWAP\n",
        "    \"VWAP_Sell\"] = 1  # Sell signal\n",
        "\n",
        "# Fear and greed index\n",
        "fear_and_greed_index = pd.read_csv('fear_and_greed_index.csv')\n",
        "# rename the date column in fear_and_greed_index to Date\n",
        "fear_and_greed_index.rename(columns={\"date\": \"Date\", \"value\": \"Fear_and_Greed_Index\"}, inplace=True)\n",
        "# convert the date column to datetime\n",
        "fear_and_greed_index[\"Date\"] = pd.to_datetime(fear_and_greed_index[\"Date\"], format=\"%Y-%m-%d\")\n",
        "# dropping the timestamp, value_classification, time_until_update columns\n",
        "fear_and_greed_index.drop(columns=[\"timestamp\", \"value_classification\", \"time_until_update\"], inplace=True)\n",
        "fear_and_greed_index.head()\n",
        "\n",
        "# Join the two dataframes on the \"Date\" column, dropping\n",
        "btc_data = btc_data.merge(fear_and_greed_index, on=\"Date\", how=\"left\")\n",
        "btc_data.head(20)\n",
        "\n",
        "# Support and Resistance Levels\n",
        "btc_data[\"Support\"] = btc_data[\"Low\"].rolling(window=20).min()  # Lowest low in the past 20 days\n",
        "btc_data[\"Resistance\"] = btc_data[\"High\"].rolling(window=20).max()  # Highest high in the past 20 days\n",
        "\n",
        "# Add two columns for extreme fear and extreme greed\n",
        "btc_data[\"Extreme_Fear\"] = np.where(btc_data[\"Fear_and_Greed_Index\"] < 20, 1, 0)\n",
        "btc_data[\"Extreme_Greed\"] = np.where(btc_data[\"Fear_and_Greed_Index\"] > 80, 1, 0)\n",
        "btc_data.describe().T\n",
        "\n",
        "# Use 70 in RSI to classify overbought and 30 to classify oversold\n",
        "btc_data[\"RSI_12_Oversold\"] = np.where(btc_data[\"RSI_12\"] < 30, 1, 0)  # Create RSI_12_Oversold column\n",
        "btc_data.loc[btc_data[\"RSI_12_Oversold\"] == 0, \"Consecutive_RSI_12_Oversold\"] = 0  # Reset to 0 where the condition is not met\n",
        "btc_data[\"RSI_12_Overbought\"] = np.where(btc_data[\"RSI_12\"] > 70, 1, 0)\n",
        "\n",
        "# Adding RSI_6 greater than RSI_12 to show bullish divergence and conversely\n",
        "btc_data[\"RSI_Divergence\"] = np.where(btc_data[\"RSI_6\"] > btc_data[\"RSI_12\"], 1, 0)\n",
        "btc_data.describe().T\n",
        "\n",
        "\n",
        "# Count consecutive appearances of Extreme_Fear\n",
        "btc_data[\"Consecutive_Extreme_Fear\"] = (\n",
        "    btc_data[\"Extreme_Fear\"]\n",
        "    .groupby((btc_data[\"Extreme_Fear\"] == 0).cumsum())  # Group by resets when Extreme_Fear is 0\n",
        "    .cumcount()  # Count consecutive days starting from 1\n",
        ")\n",
        "btc_data.loc[btc_data[\"Extreme_Fear\"] == 0, \"Consecutive_Extreme_Fear\"] = 0  # Reset to 0 where the condition is not met\n",
        "\n",
        "# Count consecutive appearances of Extreme_Greed\n",
        "btc_data[\"Consecutive_Extreme_Greed\"] = (\n",
        "    btc_data[\"Extreme_Greed\"]\n",
        "    .groupby((btc_data[\"Extreme_Greed\"] == 0).cumsum())  # Group by resets when Extreme_Greed is 0\n",
        "    .cumcount()  # Count consecutive days starting from 1\n",
        ")\n",
        "btc_data.loc[btc_data[\"Extreme_Greed\"] == 0, \"Consecutive_Extreme_Greed\"] = 0  # Reset to 0 where the condition is not met\n",
        "\n",
        "# Forward fill the missing value in 'Fear_and_Greed_Index'\n",
        "btc_data['Fear_and_Greed_Index'] = btc_data['Fear_and_Greed_Index'].fillna(method='ffill')\n",
        "\n",
        "# Count consecutive appearances of RSI_12_Overbought\n",
        "btc_data[\"Consecutive_RSI_12_Overbought\"] = (\n",
        "    btc_data[\"RSI_12_Overbought\"]\n",
        "    .groupby((btc_data[\"RSI_12_Overbought\"] == 0).cumsum())  # Group by resets when RSI_12_Overbought is 0\n",
        "    .cumcount()  # Count consecutive days starting from 1\n",
        ")\n",
        "btc_data.loc[btc_data[\"RSI_12_Overbought\"] == 0, \"Consecutive_RSI_12_Overbought\"] = 0  # Reset to 0 where the condition is not met\n",
        "\n",
        "# Count consecutive appearances of RSI_12_Oversold\n",
        "btc_data[\"Consecutive_RSI_12_Oversold\"] = (\n",
        "    btc_data[\"RSI_12_Oversold\"]\n",
        "    .groupby((btc_data[\"RSI_12_Oversold\"] == 0).cumsum())  # Group by resets when RSI_12_Oversold is 0\n",
        "    .cumcount()  # Count consecutive days starting from 1\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VwCi0T_SmYTy"
      },
      "outputs": [],
      "source": [
        "#Add continous variables\n",
        "\n",
        "#ATR ratio\n",
        "btc_data['ATR_Ratio'] = (btc_data['ATR'] / btc_data['Price']) * 100\n",
        "\n",
        "# Calculate 50-day and 200-day moving averages\n",
        "btc_data['MA50'] = btc_data['Price'].rolling(window=50).mean()\n",
        "btc_data['MA200'] = btc_data['Price'].rolling(window=200).mean()\n",
        "\n",
        "# Calculate Price relative to 50-day MA (%)\n",
        "btc_data['Price_vs_MA50'] = ((btc_data['Price'] - btc_data['MA50']) / btc_data['MA50']) * 100\n",
        "\n",
        "# Calculate Price relative to 200-day MA (%)\n",
        "btc_data['Price_vs_MA200'] = ((btc_data['Price'] - btc_data['MA200']) / btc_data['MA200']) * 100\n",
        "\n",
        "# Calculate MA Distance Ratio (Ratio of MA50 to MA200 distance)\n",
        "btc_data['MA_Distance_Ratio'] = ((btc_data['MA50'] - btc_data['MA200']) / btc_data['Price']) * 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Td80ru1Z-ulJ"
      },
      "outputs": [],
      "source": [
        "from scipy.stats import linregress\n",
        "\n",
        "# Ensure btc_data is sorted by Date\n",
        "btc_data = btc_data.sort_values('Date')\n",
        "\n",
        "# 1. Volume_Ratio: Current volume / 20-day avg volume\n",
        "btc_data['Avg_Volume_20'] = btc_data['Volume'].rolling(window=20).mean()\n",
        "btc_data['Volume_Ratio'] = btc_data['Volume'] / btc_data['Avg_Volume_20']\n",
        "\n",
        "# 2. OBV_Slope: 5-day slope of On-Balance Volume (OBV)\n",
        "# Calculate OBV\n",
        "btc_data['OBV'] = (np.sign(btc_data['Price'].diff()) * btc_data['Volume']).fillna(0).cumsum()\n",
        "\n",
        "# Calculate 5-day slope of OBV\n",
        "def calculate_slope(series):\n",
        "    x = np.arange(len(series))\n",
        "    slope, _, _, _, _ = linregress(x, series)\n",
        "    return slope\n",
        "\n",
        "btc_data['OBV_Slope'] = btc_data['OBV'].rolling(window=5).apply(calculate_slope, raw=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Y-jkoPrKL_7"
      },
      "outputs": [],
      "source": [
        "# Calculate the percentage price change over the next 7-day and 2-day (Target Variable for Regression)\n",
        "btc_data[\"Pct_Change_7D\"] = ((btc_data[\"Price\"].shift(-7) - btc_data[\"Price\"]) / btc_data[\"Price\"]) * 100\n",
        "btc_data[\"Pct_Change_2D\"] = ((btc_data[\"Price\"].shift(-2) - btc_data[\"Price\"]) / btc_data[\"Price\"]) * 100\n",
        "\n",
        "# Create binary variables for 7-day and 2-day percentage changes\n",
        "btc_data[\"Positive_7D\"] = (btc_data[\"Pct_Change_7D\"] > 0).astype(int)  # 1 if positive, 0 if negative\n",
        "btc_data[\"Positive_2D\"] = (btc_data[\"Pct_Change_2D\"] > 0).astype(int)  # 1 if positive, 0 if negative"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yj_e_l3XvMv7"
      },
      "source": [
        "## Pattern Detection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Uvu3PlfvlHX"
      },
      "outputs": [],
      "source": [
        "# Asending Triangle\n",
        "# Sort data by Date\n",
        "btc_data = btc_data.sort_values('Date')\n",
        "\n",
        "# Calculate exponential moving average for Resistance_Level\n",
        "btc_data['Resistance_Level'] = btc_data['High'].ewm(span=7, adjust=False).mean()\n",
        "\n",
        "# Identify rising trendline (higher lows)\n",
        "btc_data['Low_Shifted'] = btc_data['Low'].shift(1)\n",
        "btc_data['Higher_Low'] = (btc_data['Low'] > btc_data['Low_Shifted']).astype(int)\n",
        "\n",
        "# Detect breakout using rolling mean of Volume\n",
        "btc_data['Breakout'] = btc_data['Price'] > btc_data['Resistance_Level']\n",
        "\n",
        "# Add ascending triangle detection signal\n",
        "btc_data['Ascending_Triangle_Breakout'] = btc_data['Breakout']\n",
        "\n",
        "# Ensure Breakout is binary (1 or 0)\n",
        "btc_data['Breakout'] = btc_data['Breakout'].astype(int)\n",
        "\n",
        "# Ensure Ascending_Triangle_Breakout is binary (1 or 0)\n",
        "btc_data['Ascending_Triangle_Breakout'] = btc_data['Ascending_Triangle_Breakout'].astype(int)\n",
        "\n",
        "# Calculate the rolling mean of Volume over 7 days\n",
        "btc_data['Avg_Volume'] = btc_data['Volume'].rolling(window=7).mean()\n",
        "\n",
        "# Detect breakout with volume confirmation\n",
        "btc_data['Breakout_With_Volume'] = ((btc_data['High'] > btc_data['Resistance_Level']) &\n",
        "                                    (btc_data['Volume'] > btc_data['Avg_Volume'])).astype(int)\n",
        "\n",
        "# Add ascending triangle detection signal with volume\n",
        "btc_data['Ascending_Triangle_Breakout_With_Volume'] = btc_data['Breakout_With_Volume']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RpkSoHAQvk98"
      },
      "outputs": [],
      "source": [
        "# Count the number of breakouts\n",
        "breakout_count = btc_data['Ascending_Triangle_Breakout_With_Volume'].sum()\n",
        "print(f\"Number of breakouts: {breakout_count}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lAQMsKoemCuP"
      },
      "outputs": [],
      "source": [
        "btc_data['Triangle_Completion'] = btc_data['Higher_Low'].rolling(window=7).mean()  # Simulate completion as a rolling mean of Higher_Low\n",
        "btc_data['Triangle_Height'] = ((btc_data['High'] - btc_data['Low']) / btc_data['Price']).rolling(window=7).mean()  # Height as % of price\n",
        "btc_data['Triangle_Duration'] = btc_data['Higher_Low'].rolling(window=7).sum()  # Duration as the sum of Higher_Low over 7 days\n",
        "btc_data['Breakout_Strength'] = ((btc_data['High'] - btc_data['Resistance_Level']) * btc_data['Volume']).rolling(window=7).mean()  # Volume-adjusted breakout strength"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ebGGzgsO7qr3"
      },
      "outputs": [],
      "source": [
        "#ADX Pattern (Show the strength of the trend, please only use ADX_Buy_Signal)\n",
        "!pip install ta\n",
        "import ta\n",
        "# Calculate ADX, +DI, and -DI using the ta library\n",
        "btc_data['ADX'] = ta.trend.adx(btc_data['High'], btc_data['Low'], btc_data['Price'], window=14)\n",
        "btc_data['Positive_DI'] = ta.trend.adx_pos(btc_data['High'], btc_data['Low'], btc_data['Price'], window=14)\n",
        "btc_data['Negative_DI'] = ta.trend.adx_neg(btc_data['High'], btc_data['Low'], btc_data['Price'], window=14)\n",
        "\n",
        "# Define the ADX threshold for a strong trend\n",
        "adx_threshold = 25\n",
        "\n",
        "# Generate trade signals\n",
        "btc_data['ADX_Buy_Signal'] = ((btc_data['ADX'] > adx_threshold) & (btc_data['Positive_DI'] > btc_data['Negative_DI'])).astype(int)\n",
        "btc_data['ADX_Sell_Signal'] = ((btc_data['ADX'] > adx_threshold) & (btc_data['Negative_DI'] > btc_data['Positive_DI'])).astype(int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UxsP_-CmH2rN"
      },
      "outputs": [],
      "source": [
        "# Create columns for consecutive ADX Buy and Sell signals\n",
        "btc_data['Consecutive_ADX_Buy'] = (\n",
        "    btc_data['ADX_Buy_Signal']\n",
        "    .groupby((btc_data['ADX_Buy_Signal'] == 0).cumsum())\n",
        "    .cumcount()\n",
        ")\n",
        "btc_data.loc[btc_data['ADX_Buy_Signal'] == 0, 'Consecutive_ADX_Buy'] = 0\n",
        "\n",
        "btc_data['Consecutive_ADX_Sell'] = (\n",
        "    btc_data['ADX_Sell_Signal']\n",
        "    .groupby((btc_data['ADX_Sell_Signal'] == 0).cumsum())\n",
        "    .cumcount()\n",
        ")\n",
        "btc_data.loc[btc_data['ADX_Sell_Signal'] == 0, 'Consecutive_ADX_Sell'] = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ejD4BOm-7qdN"
      },
      "outputs": [],
      "source": [
        "# Count the number of breakout signals\n",
        "breakout_count = btc_data['ADX_Buy_Signal'].sum()\n",
        "print(f\"Number of breakout signals: {breakout_count}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cguOIO8WmK5Z"
      },
      "outputs": [],
      "source": [
        "# Add continuous features\n",
        "# ADX_Strength: How far ADX is above the threshold (ADX - 25, 0 if below)\n",
        "btc_data['ADX_Strength'] = (btc_data['ADX'] - adx_threshold).clip(lower=0)\n",
        "\n",
        "# ADX_Trend_Duration: Number of consecutive days ADX > 25\n",
        "btc_data['ADX_Trend_Duration'] = (btc_data['ADX'] > adx_threshold).astype(int).groupby((btc_data['ADX'] <= adx_threshold).astype(int).cumsum()).cumsum()\n",
        "\n",
        "# ADX_Trend_Direction: +DI minus -DI (positive for uptrend, negative for downtrend)\n",
        "btc_data['ADX_Trend_Direction'] = btc_data['Positive_DI'] - btc_data['Negative_DI']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eU5mhlpV-ntq"
      },
      "outputs": [],
      "source": [
        "# Calculate Stochastic Oscillator (%K and %D)\n",
        "btc_data['Stochastic_perK'] = ta.momentum.stoch(\n",
        "    btc_data['High'], btc_data['Low'], btc_data['Price'], window=14, smooth_window=3\n",
        ")\n",
        "btc_data['Stochastic_perD'] = ta.momentum.stoch_signal(\n",
        "    btc_data['High'], btc_data['Low'], btc_data['Price'], window=14, smooth_window=3\n",
        ")\n",
        "\n",
        "# Define Stochastic Buy and Sell Signals\n",
        "btc_data['Stochastic_Buy_Signal'] = np.where(\n",
        "    (btc_data['Stochastic_perK'] < 20) & (btc_data['Stochastic_perK'] > btc_data['Stochastic_perD']),\n",
        "    1,  # Buy Signal\n",
        "    0   # No Signal\n",
        ")\n",
        "\n",
        "btc_data['Stochastic_Sell_Signal'] = np.where(\n",
        "    (btc_data['Stochastic_perK'] > 80) & (btc_data['Stochastic_perK'] < btc_data['Stochastic_perD']),\n",
        "    1,  # Sell Signal\n",
        "    0   # No Signal\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6GmYoGZNH9Xj"
      },
      "outputs": [],
      "source": [
        "# Add consecutive features for Stochastic signals\n",
        "btc_data['Consecutive_Stochastic_Buy'] = (\n",
        "    btc_data['Stochastic_Buy_Signal']\n",
        "    .groupby((btc_data['Stochastic_Buy_Signal'] == 0).cumsum())\n",
        "    .cumcount()\n",
        ")\n",
        "\n",
        "btc_data['Consecutive_Stochastic_Sell'] = (\n",
        "    btc_data['Stochastic_Sell_Signal']\n",
        "    .groupby((btc_data['Stochastic_Sell_Signal'] == 0).cumsum())\n",
        "    .cumcount()\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bIdDYQw1r8PZ"
      },
      "outputs": [],
      "source": [
        "#Bullish and Bearish Engulfing\n",
        "def detect_bullish_engulfing(data):\n",
        "    return (\n",
        "        (data['Price'] > data['Open']) &  # Current candle is bullish\n",
        "        (data['Price'].shift(1) < data['Open'].shift(1)) &  # Previous candle is bearish\n",
        "        (data['Open'] < data['Price'].shift(1)) &  # Current open is below previous close\n",
        "        (data['Price'] > data['Open'].shift(1))    # Current close is above previous open\n",
        "    )\n",
        "\n",
        "def detect_bearish_engulfing(data):\n",
        "    return (\n",
        "        (data['Price'] < data['Open']) &  # Current candle is bearish\n",
        "        (data['Price'].shift(1) > data['Open'].shift(1)) &  # Previous candle is bullish\n",
        "        (data['Open'] > data['Price'].shift(1)) &  # Current open is above previous close\n",
        "        (data['Price'] < data['Open'].shift(1))    # Current close is below previous open\n",
        "    )\n",
        "\n",
        "# Apply the detection functions to btc_data\n",
        "btc_data['Bullish_Engulfing'] = detect_bullish_engulfing(btc_data).astype(int)\n",
        "btc_data['Bearish_Engulfing'] = detect_bearish_engulfing(btc_data).astype(int)\n",
        "\n",
        "# Display the rows where patterns are detected\n",
        "bullish_patterns = btc_data[btc_data['Bullish_Engulfing'] == 1]\n",
        "bearish_patterns = btc_data[btc_data['Bearish_Engulfing'] == 1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ow8D8L0Sr8H0"
      },
      "outputs": [],
      "source": [
        "breakout_count = btc_data['Bullish_Engulfing'].sum()\n",
        "print(f\"Number of breakout signals: {breakout_count}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tJ6CxrGWr8Ad"
      },
      "outputs": [],
      "source": [
        "breakout_count = btc_data['Bearish_Engulfing'].sum()\n",
        "print(f\"Number of breakout signals: {breakout_count}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bKw4kLG8lAuV"
      },
      "outputs": [],
      "source": [
        "def detect_hammer(data):\n",
        "    \"\"\"\n",
        "    Detects hammer candlestick patterns.\n",
        "    A hammer has:\n",
        "    1. A small body at the top\n",
        "    2. A long lower shadow (at least 2x the body)\n",
        "    3. Little or no upper shadow\n",
        "    \"\"\"\n",
        "\n",
        "    # Calculate body and shadows\n",
        "    data['Body'] = data['Price'] - data['Open']\n",
        "    data['Upper_Shadow'] = data['High'] - data[['Open', 'Price']].max(axis=1)\n",
        "    data['Lower_Shadow'] = data[['Open', 'Price']].min(axis=1) - data['Low']\n",
        "\n",
        "    # Define hammer criteria\n",
        "    data['Hammer'] = (\n",
        "        # Body is small (using 0.3 times high-low range as threshold)\n",
        "        (abs(data['Body']) <= 0.3 * (data['High'] - data['Low'])) &\n",
        "        # Lower shadow is at least 2x the body length\n",
        "        (data['Lower_Shadow'] >= 2 * abs(data['Body'])) &\n",
        "        # Upper shadow is relatively small\n",
        "        (data['Upper_Shadow'] <= 0.1 * (data['High'] - data['Low']))\n",
        "    ).astype(int)\n",
        "\n",
        "    # Clean up temporary columns\n",
        "    data.drop(['Body', 'Upper_Shadow', 'Lower_Shadow'], axis=1, inplace=True)\n",
        "\n",
        "    # Count consecutive hammers\n",
        "    data['Consecutive_Hammer'] = (\n",
        "        data['Hammer']\n",
        "        .groupby((data['Hammer'] == 0).cumsum())\n",
        "        .cumcount()\n",
        "    )\n",
        "    data.loc[data['Hammer'] == 0, 'Consecutive_Hammer'] = 0\n",
        "\n",
        "    return data\n",
        "\n",
        "# Apply hammer detection to the data\n",
        "btc_data = detect_hammer(btc_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Tde3F5WlAuV"
      },
      "outputs": [],
      "source": [
        "hammer_count = btc_data['Hammer'].sum()\n",
        "print(f\"Total number of hammer patterns detected: {hammer_count}\")\n",
        "\n",
        "# Also get the number of cases where we had consecutive hammers\n",
        "consecutive_hammers = btc_data[btc_data['Consecutive_Hammer'] > 0]\n",
        "print(f\"Number of cases with consecutive hammers: {len(consecutive_hammers)}\")\n",
        "print(f\"Maximum number of consecutive hammers: {btc_data['Consecutive_Hammer'].max()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Q8lSznMlAuV",
        "vscode": {
          "languageId": "ruby"
        }
      },
      "outputs": [],
      "source": [
        "def detect_three_white_soldiers(data, min_body_ratio=0.5):\n",
        "    # Create candle body and shadow lengths\n",
        "    data['Body'] = data['Price'] - data['Open']\n",
        "    data['Upper_Shadow'] = data['High'] - data[['Price', 'Open']].max(axis=1)\n",
        "    data['Lower_Shadow'] = data[['Price', 'Open']].min(axis=1) - data['Low']\n",
        "\n",
        "    # Initialize Three White Soldiers pattern array\n",
        "    three_white_soldiers = np.zeros(len(data))\n",
        "\n",
        "    for i in range(2, len(data)):\n",
        "        # Check if we have 3 consecutive bullish candles\n",
        "        bullish_candles = (data['Body'].iloc[i] > 0 and\n",
        "                          data['Body'].iloc[i-1] > 0 and\n",
        "                          data['Body'].iloc[i-2] > 0)\n",
        "\n",
        "        if bullish_candles:\n",
        "            # Calculate total candle lengths\n",
        "            candle_lengths = [\n",
        "                data['High'].iloc[i] - data['Low'].iloc[i],\n",
        "                data['High'].iloc[i-1] - data['Low'].iloc[i-1],\n",
        "                data['High'].iloc[i-2] - data['Low'].iloc[i-2]\n",
        "            ]\n",
        "\n",
        "            # Calculate body ratios\n",
        "            body_ratios = [\n",
        "                data['Body'].iloc[i] / candle_lengths[0],\n",
        "                data['Body'].iloc[i-1] / candle_lengths[1],\n",
        "                data['Body'].iloc[i-2] / candle_lengths[2]\n",
        "            ]\n",
        "\n",
        "            # Check if each candle has a sufficient body ratio\n",
        "            strong_bodies = all(ratio >= min_body_ratio for ratio in body_ratios)\n",
        "\n",
        "            # Check if each candle opens within previous candle's body\n",
        "            progressive_opens = (\n",
        "                data['Open'].iloc[i] > data['Open'].iloc[i-1] and\n",
        "                data['Open'].iloc[i-1] > data['Open'].iloc[i-2]\n",
        "            )\n",
        "\n",
        "            # Check if each candle closes higher than previous candle\n",
        "            progressive_closes = (\n",
        "                data['Price'].iloc[i] > data['Price'].iloc[i-1] and\n",
        "                data['Price'].iloc[i-1] > data['Price'].iloc[i-2]\n",
        "            )\n",
        "\n",
        "            # If all conditions are met, mark as Three White Soldiers\n",
        "            if strong_bodies and progressive_opens and progressive_closes:\n",
        "                three_white_soldiers[i] = 1\n",
        "\n",
        "    # Add the pattern to the dataframe\n",
        "    data['Three_White_Soldiers'] = three_white_soldiers\n",
        "\n",
        "    # Count the occurrences\n",
        "    pattern_count = int(three_white_soldiers.sum())\n",
        "\n",
        "    return pattern_count, data['Three_White_Soldiers']\n",
        "\n",
        "# Apply the detection\n",
        "pattern_count, three_white_soldiers = detect_three_white_soldiers(btc_data)\n",
        "print(f\"Number of Three White Soldiers patterns detected: {pattern_count}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i8HYxhux9X3X"
      },
      "outputs": [],
      "source": [
        "def detect_three_black_crows(data, min_body_ratio=0.5):\n",
        "    # Create candle body and shadow lengths\n",
        "    data['Body'] = data['Price'] - data['Open']\n",
        "    data['Upper_Shadow'] = data['High'] - data[['Price', 'Open']].max(axis=1)\n",
        "    data['Lower_Shadow'] = data[['Price', 'Open']].min(axis=1) - data['Low']\n",
        "\n",
        "    # Initialize Three Black Crows pattern array\n",
        "    three_black_crows = np.zeros(len(data))\n",
        "\n",
        "    for i in range(2, len(data)):\n",
        "        # Check if we have 3 consecutive bearish candles\n",
        "        bearish_candles = (data['Body'].iloc[i] < 0 and\n",
        "                          data['Body'].iloc[i-1] < 0 and\n",
        "                          data['Body'].iloc[i-2] < 0)\n",
        "\n",
        "        if bearish_candles:\n",
        "            # Calculate total candle lengths\n",
        "            candle_lengths = [\n",
        "                data['High'].iloc[i] - data['Low'].iloc[i],\n",
        "                data['High'].iloc[i-1] - data['Low'].iloc[i-1],\n",
        "                data['High'].iloc[i-2] - data['Low'].iloc[i-2]\n",
        "            ]\n",
        "\n",
        "            # Calculate body ratios\n",
        "            body_ratios = [\n",
        "                abs(data['Body'].iloc[i]) / candle_lengths[0],\n",
        "                abs(data['Body'].iloc[i-1]) / candle_lengths[1],\n",
        "                abs(data['Body'].iloc[i-2]) / candle_lengths[2]\n",
        "            ]\n",
        "\n",
        "            # Check if each candle has a sufficient body ratio\n",
        "            strong_bodies = all(ratio >= min_body_ratio for ratio in body_ratios)\n",
        "\n",
        "            # Check if each candle opens within previous candle's body\n",
        "            progressive_opens = (\n",
        "                data['Open'].iloc[i] < data['Open'].iloc[i-1] and\n",
        "                data['Open'].iloc[i-1] < data['Open'].iloc[i-2]\n",
        "            )\n",
        "\n",
        "            # Check if each candle closes lower than previous candle\n",
        "            progressive_closes = (\n",
        "                data['Price'].iloc[i] < data['Price'].iloc[i-1] and\n",
        "                data['Price'].iloc[i-1] < data['Price'].iloc[i-2]\n",
        "            )\n",
        "\n",
        "            # If all conditions are met, mark as Three Black Crows\n",
        "            if strong_bodies and progressive_opens and progressive_closes:\n",
        "                three_black_crows[i] = 1\n",
        "\n",
        "    # Add the pattern to the dataframe\n",
        "    data['Three_Black_Crows'] = three_black_crows\n",
        "\n",
        "    # Count the occurrences\n",
        "    pattern_count = int(three_black_crows.sum())\n",
        "\n",
        "    return pattern_count, data['Three_Black_Crows']\n",
        "\n",
        "# Apply the detection\n",
        "pattern_count, three_black_crows = detect_three_black_crows(btc_data)\n",
        "print(f\"Number of Three Black Crows patterns detected: {pattern_count}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YLR8IOQVveOv"
      },
      "source": [
        "##Join External Data Source"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wdcMrJgHr0ag"
      },
      "outputs": [],
      "source": [
        "#Left join gold_price_data, risk_free_data, spy500 to btc_data\n",
        "btc_data = pd.merge(btc_data, gold_price_data, on='Date', how='left') #till 2025-02-03\n",
        "btc_data = pd.merge(btc_data, risk_free_data, on='Date', how='left')\n",
        "btc_data = pd.merge(btc_data, spy_data, on='Date', how='left') #till 2025-04-11\n",
        "btc_data = pd.merge(btc_data, eth_data, on='Date', how='left') #from 2015-08-07\n",
        "#As other external data have market closeure, all na filled by previous day close data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "St8vfp29ZukO"
      },
      "outputs": [],
      "source": [
        "btc_data.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "78r4m6m2wyCg"
      },
      "outputs": [],
      "source": [
        "# Plot Bitcoin Volume over date\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(btc_data['Date'], btc_data['Volume'])\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Bitcoin Volume')\n",
        "plt.ylim(bottom=-2)  # Set the lower limit to 0\n",
        "plt.title('Bitcoin Volume Over Time')\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mP5ErQLgwyCh"
      },
      "outputs": [],
      "source": [
        "btc_data['Volume'].describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LkVC0-VBwyCh"
      },
      "source": [
        "Since we can observe the spikes happened from 2018 to mid-2022, we try to fit the model avoiding the spikes here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GCleimScwyCh"
      },
      "outputs": [],
      "source": [
        "# Find the min and max of date with volume greater than 1\n",
        "btc_data[btc_data['Volume'] > 1]['Date'].min(), btc_data[btc_data['Volume'] > 1]['Date'].max()\n",
        "# Print the min and max date with volume greater than 1\n",
        "print(\"Min date with volume > 1:\", btc_data[btc_data['Volume'] > 1]['Date'].min())\n",
        "print(\"Max date with volume > 1:\", btc_data[btc_data['Volume'] > 1]['Date'].max())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iPtOUvLuwyCh"
      },
      "outputs": [],
      "source": [
        "# Drop the rows with date before 2022-05-09\n",
        "btc_data = btc_data[btc_data['Date'] >= '2022-05-09']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UA1mIe1ZwyCh"
      },
      "outputs": [],
      "source": [
        "# Plot Bitcoin Volume over date\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(btc_data['Date'], btc_data['Volume'])\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Bitcoin Volume')\n",
        "plt.ylim(bottom=-2)  # Set the lower limit to 0\n",
        "plt.title('Bitcoin Volume Over Time')\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AA0o_13tnTZW"
      },
      "source": [
        "## **Re-scale and Transformation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mjJ0Usfy00v7"
      },
      "source": [
        "**Yeo-Johnson Transformation** for Price, Change, Pct_Change"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yKJWVEl9nN7f"
      },
      "outputs": [],
      "source": [
        "# For Price, Change, 7D_Pct_Change, 2D_Pct_Change (use yeo-johnson)\n",
        "from sklearn.preprocessing import PowerTransformer\n",
        "import pandas as pd\n",
        "\n",
        "# Initialize PowerTransformer for each column\n",
        "power_transformer_price = PowerTransformer(method='yeo-johnson', standardize=False)  # Disable standardization\n",
        "power_transformer_change = PowerTransformer(method='yeo-johnson', standardize=False)\n",
        "power_transformer_7d_pct_change = PowerTransformer(method='yeo-johnson', standardize=False)\n",
        "power_transformer_2d_pct_change = PowerTransformer(method='yeo-johnson', standardize=False)\n",
        "\n",
        "# Apply the Yeo-Johnson transformation to each column\n",
        "btc_data['YeoJohnson_Price'] = power_transformer_price.fit_transform(btc_data[['Price']])\n",
        "btc_data['YeoJohnson_Change'] = power_transformer_change.fit_transform(btc_data[['Change']])\n",
        "btc_data['YeoJohnson_7D_Pct_Change'] = power_transformer_7d_pct_change.fit_transform(btc_data[['Pct_Change_7D']])\n",
        "btc_data['YeoJohnson_2D_Pct_Change'] = power_transformer_2d_pct_change.fit_transform(btc_data[['Pct_Change_2D']])\n",
        "\n",
        "# Store lambda values for each column\n",
        "lambdas = {\n",
        "    'Price_lambda': power_transformer_price.lambdas_[0],\n",
        "    'Change_lambda': power_transformer_change.lambdas_[0],\n",
        "    'Pct_Change_lambda_7D': power_transformer_7d_pct_change.lambdas_[0],\n",
        "    'Pct_Change_lambda_2D': power_transformer_2d_pct_change.lambdas_[0]\n",
        "}\n",
        "\n",
        "# Calculate mean and standard deviation for each transformed column\n",
        "transformed_means = {\n",
        "    'Price_mean': btc_data['YeoJohnson_Price'].mean(),\n",
        "    'Change_mean': btc_data['YeoJohnson_Change'].mean(),\n",
        "    'Pct_Change_mean_7D': btc_data['YeoJohnson_7D_Pct_Change'].mean(),\n",
        "    'Pct_Change_mean_2D': btc_data['YeoJohnson_2D_Pct_Change'].mean()\n",
        "}\n",
        "\n",
        "transformed_stds = {\n",
        "    'Price_std': btc_data['YeoJohnson_Price'].std(),\n",
        "    'Change_std': btc_data['YeoJohnson_Change'].std(),\n",
        "    'Pct_Change_std_7D': btc_data['YeoJohnson_7D_Pct_Change'].std(),\n",
        "    'Pct_Change_std_2D': btc_data['YeoJohnson_2D_Pct_Change'].std()\n",
        "}\n",
        "\n",
        "# Print the collected lambdas, means, and standard deviations\n",
        "print(f\"Lambdas: {lambdas}\")\n",
        "print(f\"Means: {transformed_means}\")\n",
        "print(f\"Standard Deviations: {transformed_stds}\")\n",
        "\n",
        "### Step 2: Back-Transformation ###\n",
        "# Recreate PowerTransformers with stored lambdas for back-transformation\n",
        "power_transformer_price.lambdas_ = [lambdas['Price_lambda']]\n",
        "power_transformer_change.lambdas_ = [lambdas['Change_lambda']]\n",
        "power_transformer_7d_pct_change.lambdas_ = [lambdas['Pct_Change_lambda_7D']]\n",
        "power_transformer_2d_pct_change.lambdas_ = [lambdas['Pct_Change_lambda_2D']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "keI8TIbuS-SN"
      },
      "outputs": [],
      "source": [
        "binary_variables = [\n",
        "    \"MACD_buy\",  # 1 if MACD crosses above MACD_Signal, otherwise 0\n",
        "    \"MACD_sell\",  # 1 if MACD crosses below MACD_Signal, otherwise 0\n",
        "    \"MACD_above_signal\",  # 1 if MACD > MACD_Signal, otherwise 0\n",
        "    \"MACD_below_signal\",  # 1 if MACD <= MACD_Signal, otherwise 0\n",
        "    \"BB_Buy\",  # 1 if Price crosses below the Lower Bollinger Band, otherwise 0\n",
        "    \"BB_Sell\",  # 1 if Price crosses above the Upper Bollinger Band, otherwise 0\n",
        "    \"Price_below_BB_lower\",  # 1 if Price < BB_lower, otherwise 0\n",
        "    \"Price_above_BB_upper\",  # 1 if Price > BB_upper, otherwise 0\n",
        "    \"VWAP_Buy\",  # 1 if Price crosses above VWAP, otherwise 0\n",
        "    \"VWAP_Sell\",  # 1 if Price crosses below VWAP, otherwise 0\n",
        "    \"RSI_12_Oversold\",  # 1 if RSI_12 < 30, otherwise 0\n",
        "    \"RSI_12_Overbought\",  # 1 if RSI_12 > 70, otherwise 0\n",
        "    \"RSI_Divergence\",  # 1 if RSI_6 > RSI_12, otherwise 0\n",
        "    \"Positive_7D\",  # 1 if Pct_Change_7D > 0, otherwise 0\n",
        "    \"Positive_2D\",  # 1 if Pct_Change_2D > 0, otherwise 0\n",
        "    \"Higher_Low\",  # 1 if current low > previous low, otherwise 0\n",
        "    \"Breakout\",  # 1 if Price > Resistance_Level, otherwise 0\n",
        "    \"Ascending_Triangle_Breakout\",  # Same as Breakout\n",
        "    \"Breakout_With_Volume\",  # 1 if breakout with volume confirmation, otherwise 0\n",
        "    \"Ascending_Triangle_Breakout_With_Volume\",  # Same as Breakout_With_Volume\n",
        "    \"ADX_Buy_Signal\",  # 1 if ADX > 25 and Positive_DI > Negative_DI, otherwise 0\n",
        "    \"ADX_Sell_Signal\",  # 1 if ADX > 25 and Negative_DI > Positive_DI, otherwise 0\n",
        "    \"Stochastic_Buy_Signal\",  # 1 if Stochastic %K < 20 and %K > %D, otherwise 0\n",
        "    \"Stochastic_Sell_Signal\",  # 1 if Stochastic %K > 80 and %K < %D, otherwise 0\n",
        "    \"Bullish_Engulfing\",  # 1 if Bullish Engulfing pattern is detected, otherwise 0\n",
        "    \"Bearish_Engulfing\",  # 1 if Bearish Engulfing pattern is detected, otherwise 0\n",
        "    \"Hammer\",  # 1 if Hammer candlestick pattern is detected, otherwise 0\n",
        "    \"Three_White_Soldiers\",  # 1 if Three White Soldiers pattern is detected, otherwise 0\n",
        "    \"Three_Black_Crows\", # 1 if Three Black Crows pattern is detected, otherwise 0\n",
        "    'Extreme_Fear', 'Extreme_Greed'\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0e46zGXtS-K3"
      },
      "outputs": [],
      "source": [
        "continuous_variables = [\n",
        "    'Price', 'Open', 'High', 'Low', 'Change',\n",
        "    \"RSI_6\",  # Relative Strength Index with a 6-period length\n",
        "    \"RSI_12\",  # Relative Strength Index with a 12-period length\n",
        "    \"EMA_14\",  # Exponential Moving Average with a 14-period length\n",
        "    \"SMA_14\",  # Simple Moving Average with a 14-period length\n",
        "    \"OBV\",  # On-Balance Volume\n",
        "    \"MACD\",  # Moving Average Convergence Divergence\n",
        "    \"MACD_Signal\",  # Signal line of MACD\n",
        "    \"MACD_cum_buy\",  # Cumulative count of consecutive days where MACD_above_signal is 1\n",
        "    \"MACD_cum_sell\",  # Cumulative count of consecutive days where MACD_below_signal is 1\n",
        "    \"BB_upper\",  # Upper Bollinger Band\n",
        "    \"BB_lower\",  # Lower Bollinger Band\n",
        "    \"BB_cum_Buy\",  # Cumulative count of consecutive days where Price_below_BB_lower is 1\n",
        "    \"BB_cum_Sell\",  # Cumulative count of consecutive days where Price_above_BB_upper is 1\n",
        "    \"ATR\",  # Average True Range\n",
        "    \"ATR_Ratio\",  # ATR as a percentage of Price\n",
        "    \"Stop_Loss_Long\",  # Stop-loss level for long trades\n",
        "    \"Stop_Loss_Short\",  # Stop-loss level for short trades\n",
        "    \"VWAP\",  # Volume Weighted Average Price\n",
        "    \"Support\",  # Lowest low in the past 20 days\n",
        "    \"Resistance\",  # Highest high in the past 20 days\n",
        "    \"Consecutive_RSI_12_Overbought\",  # Cumulative count of consecutive days where RSI_12_Overbought is 1\n",
        "    \"Consecutive_RSI_12_Oversold\",  # Cumulative count of consecutive days where RSI_12_Oversold is 1\n",
        "    \"Pct_Change_7D\",  # Percentage price change over the next 7 days\n",
        "    \"Pct_Change_2D\",  # Percentage price change over the next 2 days\n",
        "    \"MA50\",  # 50-day moving average\n",
        "    \"MA200\",  # 200-day moving average\n",
        "    \"Price_vs_MA50\",  # Price relative to 50-day MA (%)\n",
        "    \"Price_vs_MA200\",  # Price relative to 200-day MA (%)\n",
        "    \"MA_Distance_Ratio\",  # Ratio of MA50 to MA200 distance\n",
        "    \"Avg_Volume_20\",  # 20-day average volume\n",
        "    \"Volume_Ratio\",  # Current volume / 20-day avg volume\n",
        "    \"OBV_Slope\",  # 5-day slope of OBV\n",
        "    \"Resistance_Level\",  # Exponential moving average of high price\n",
        "    \"Avg_Volume\",  # 7-day rolling average of volume\n",
        "    \"Triangle_Completion\",  # Rolling mean of Higher_Low\n",
        "    \"Triangle_Height\",  # Height of the triangle as % of price\n",
        "    \"Triangle_Duration\",  # Sum of Higher_Low over 7 days\n",
        "    \"Breakout_Strength\",  # Volume-adjusted breakout strength\n",
        "    \"ADX\",  # Average Directional Index\n",
        "    \"Positive_DI\",  # Positive Directional Index\n",
        "    \"Negative_DI\",  # Negative Directional Index\n",
        "    \"Consecutive_ADX_Buy\",  # Consecutive days of ADX_Buy_Signal\n",
        "    \"Consecutive_ADX_Sell\",  # Consecutive days of ADX_Sell_Signal\n",
        "    \"ADX_Strength\",  # How far ADX is above the threshold\n",
        "    \"ADX_Trend_Duration\",  # Consecutive days where ADX > 25\n",
        "    \"ADX_Trend_Direction\",  # Difference between Positive_DI and Negative_DI\n",
        "    \"Stochastic_perK\",  # %K line of the Stochastic Oscillator\n",
        "    \"Stochastic_perD\",  # %D line of the Stochastic Oscillator\n",
        "    \"Consecutive_Stochastic_Buy\",  # Consecutive days of Stochastic_Buy_Signal\n",
        "    \"Consecutive_Stochastic_Sell\",  # Consecutive days of Stochastic_Sell_Signal\n",
        "    \"Consecutive_Hammer\",  # Consecutive hammer patterns\n",
        "    \"SPY_Price\",  # Price of SPY (S&P 500 ETF)\n",
        "    \"SPY_Volume\",  # Trading volume of SPY\n",
        "    \"ETH_Price\",  # Price of Ethereum (ETH)\n",
        "    \"ETH_Volume\",  # Trading volume of Ethereum (ETH)\n",
        "    \"Gold_Price\",  # Price of gold\n",
        "    \"Gold_Volume\",  # Trading volume of gold\n",
        "    \"US_10Y\",  # Yield of the US 10-year Treasury bond\n",
        "    \"Yield_Spread\",  # Spread between different bond yields\n",
        "    \"YeoJohnson_Price\",  # Yeo-Johnson transformed Price\n",
        "    \"YeoJohnson_Change\",  # Yeo-Johnson transformed Change\n",
        "    \"YeoJohnson_7D_Pct_Change\",  # Yeo-Johnson transformed Pct_Change_7D\n",
        "    \"YeoJohnson_2D_Pct_Change\", # Yeo-Johnson transformed Pct_Change_2D\n",
        "    'Fear_and_Greed_Index', 'Consecutive_Extreme_Fear', 'Consecutive_Extreme_Greed'\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RKA_3cMOS-CB"
      },
      "outputs": [],
      "source": [
        "# Set binary variables to int64\n",
        "btc_data[binary_variables] = btc_data[binary_variables].astype(\"int64\")\n",
        "\n",
        "# Set continuous variables to float64\n",
        "btc_data[continuous_variables] = btc_data[continuous_variables].astype(\"float64\")\n",
        "\n",
        "# Verify the data types\n",
        "for column in btc_data.columns:\n",
        "    print(f\"{column}: {btc_data[column].dtype}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f-TX3OjvnvEU"
      },
      "outputs": [],
      "source": [
        "numerical_variable = btc_data.select_dtypes(include=['number'])\n",
        "btc_data.columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oy-QyHrV0Q84"
      },
      "source": [
        "**Standardize all numeric variable**\n",
        "\n",
        "store to scaled_btc_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MZ0Euw5rILY-"
      },
      "outputs": [],
      "source": [
        "# Define count columns\n",
        "count_columns = [ 'MACD_Signal', 'MACD_buy', 'MACD_sell', 'MACD_above_signal', 'MACD_cum_buy', 'MACD_cum_sell', 'MACD_below_signal', 'BB_Buy', 'BB_Sell', 'Price_below_BB_lower', 'Price_above_BB_upper',\n",
        "                 'BB_cum_Buy', 'BB_cum_Sell', 'VWAP_Buy', 'VWAP_Sell', 'Price_above_VWAP', 'Price_below_VWAP', 'VWAP_cum_Buy', 'VWAP_cum_Sell', 'RSI12_Oversold' ,'Consecutive_RSI_12_Oversold',\n",
        "                 'RSI_12_Overbought', 'RSI_Divergence', 'Consecutive_RSI_12_Overbought', 'Ascending_Triangle_Breakout', 'Ascending_Triangle_Breakout_With_Volume', 'ADX_Buy_Signal', 'ADX_Sell_Signal', 'Consecutive_ADX_Buy',\n",
        "                 'Consecutive_ADX_Sell', 'Stochastic_Buy_Signal', 'Stochastic_Sell_Signal', 'Consecutive_Stochastic_Buy', 'Consecutive_Stochastic_Sell', 'Bullish_Engulfing', 'Bearish_Engulfing', 'Hammer', 'Consecutive_Hammer',\n",
        "                 'Three_white_soldiers', 'Three_Black_Crows', 'Fear_and_Greed_Index', 'Extreme_Fear', 'Extreme_Greed', 'Consecutive_Extreme_Fear', 'Consecutive_Extreme_Greed'\n",
        "                 ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2PtKNmtQngxy"
      },
      "outputs": [],
      "source": [
        "#Standardize all numeric variable\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "\n",
        "# Initialize the MinMaxScaler and StandardScaler\n",
        "min_max_scaler = MinMaxScaler()\n",
        "standard_scaler = StandardScaler()\n",
        "\n",
        "# Apply log transformation to 'Volume' to reduce the range\n",
        "btc_data['Volume'] = np.log1p(btc_data['Volume'])  # log1p handles log(0) safely\n",
        "\n",
        "# Apply Scaler\n",
        "btc_data['Volume'] = min_max_scaler.fit_transform(btc_data[['Volume']])\n",
        "btc_data['Volume'] = standard_scaler.fit_transform(btc_data[['Volume']])\n",
        "\n",
        "# List of features for scaling\n",
        "numeric_features = continuous_variables\n",
        "\n",
        "# Fit and transform only the numeric features\n",
        "scaled_features = standard_scaler.fit_transform(btc_data[numeric_features])\n",
        "\n",
        "# Create a new DataFrame for scaled data\n",
        "scaled_btc_data = btc_data.copy()  # Copy original DataFrame\n",
        "\n",
        "# Replace numeric columns in the new DataFrame with scaled values\n",
        "scaled_btc_data[numeric_features] = scaled_features\n",
        "\n",
        "scaled_btc_data.tail()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uOlP23hMTjta"
      },
      "outputs": [],
      "source": [
        "#numeric_features = [\n",
        "  #'Price', 'Open', 'High', 'Low', 'Change', 'RSI_6', 'RSI_12', 'EMA_14',\n",
        "  #'SMA_14', 'OBV', 'MACD', 'MACD_Signal','MACD_above_signal', 'MACD_below_signal','MACD_cum_buy', 'MACD_cum_sell',\n",
        "  #'BB_lower', 'BB_upper','Price_below_BB_lower', 'Price_above_BB_upper', 'BB_cum_Buy', 'BB_cum_Sell',\n",
        "  #'ATR', 'Stop_Loss_Long', 'Stop_Loss_Short', 'VWAP','Support', 'Resistance', 'YeoJohnson_Price',\n",
        "  #'YeoJohnson_Change','YeoJohnson_7D_Pct_Change', 'YeoJohnson_2D_Pct_Change', 'Gold_Price', 'Gold_Volume',\n",
        "  #'US_10Y', 'Yield_Spread','Triangle_Completion', 'Triangle_Height', 'Breakout_Strength',\n",
        "  #'ADX_Strength', 'ATR_Ratio', 'Price_vs_MA50', 'Price_vs_MA200', 'MA_Distance_Ratio', 'OBV_Slope',\n",
        "  #'Stochastic_perK', 'Stochastic_perD', 'SPY_Price', 'SPY_Volume', 'ETH_Price', 'ETH_Volume',\n",
        "  #'Fear_and_Greed_Index', 'Extreme_Fear', 'Extreme_Greed', 'Consecutive_Extreme_Fear', 'Consecutive_Extreme_Greed']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mVtmTpdKpGOC"
      },
      "outputs": [],
      "source": [
        "scaled_btc_data.describe().T"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S9dUM_dbs0p1"
      },
      "source": [
        "# **Please use scaled_btc_data for analysis**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vexswtnEIcvp"
      },
      "outputs": [],
      "source": [
        "scaled_btc_data.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RXMA930702Go"
      },
      "source": [
        "**Drop NA before Analysis**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R04OuOWmIc8Q"
      },
      "outputs": [],
      "source": [
        "# Drop rows with NaN values in scaled_btc_data\n",
        "scaled_btc_data = scaled_btc_data.dropna()\n",
        "\n",
        "# Optionally, reset the index after dropping rows\n",
        "scaled_btc_data = scaled_btc_data.reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DZlthiApIiON"
      },
      "outputs": [],
      "source": [
        "scaled_btc_data.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pxz2sLmEIjNf"
      },
      "outputs": [],
      "source": [
        "#Check data are consecutive\n",
        "# Ensure 'Date' is sorted\n",
        "scaled_btc_data = scaled_btc_data.sort_values('Date')\n",
        "\n",
        "# Calculate the difference between consecutive dates\n",
        "scaled_btc_data['Date_Diff'] = scaled_btc_data['Date'].diff().dt.days\n",
        "\n",
        "# Check if all differences are 1 day\n",
        "if (scaled_btc_data['Date_Diff'].iloc[1:] == 1).all():\n",
        "    print(\"All dates are consecutive.\")\n",
        "else:\n",
        "    print(\"There are missing dates.\")\n",
        "    print(scaled_btc_data[scaled_btc_data['Date_Diff'] > 1])  # Display rows with missing dates\n",
        "\n",
        "# Drop the 'Date_Diff' column after the check\n",
        "scaled_btc_data.drop(columns=['Date_Diff'], inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HqrQEBBYIkt2"
      },
      "outputs": [],
      "source": [
        "# Check the start and end dates\n",
        "start_date = scaled_btc_data['Date'].min()\n",
        "end_date = scaled_btc_data['Date'].max()\n",
        "\n",
        "print(f\"Data starts from: {start_date}\")\n",
        "print(f\"Data ends at: {end_date}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b55IBivgGgmg"
      },
      "source": [
        "#Exploratory Data Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5KR7Uq27lAuY"
      },
      "outputs": [],
      "source": [
        "#Print the name of the columns\n",
        "print(scaled_btc_data.columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fi7LeczFpcJr"
      },
      "outputs": [],
      "source": [
        "# Moving date to index\n",
        "scaled_btc_data.set_index('Date', inplace=True)\n",
        "\n",
        "target = 'YeoJohnson_7D_Pct_Change'\n",
        "# target_binary = 'Positive_2D'\n",
        "target_binary = 'Positive_7D'\n",
        "\n",
        "exclude_list = ['Price', 'Open', 'High', 'Low', 'Change', 'Day_of_Week', 'Week_of_Year', 'Month','Quarter','Year',\n",
        "                'VWAP', 'VWAP_Buy', 'VWAP_Sell', 'Resistance_Level', 'Low_Shifted', 'Higher_Low',\n",
        "                'Breakout', 'Ascending_Triangle_Breakout', 'Avg_Volume', 'Breakout_With_Volume', 'Avg_Volume_20','Pct_Change_7D', 'Pct_Change_2D','Positive_2D','YeoJohnson_Price','YeoJohnson_Change','YeoJohnson_2D_Pct_Change']\n",
        "# Select binary columns\n",
        "binary_variables = scaled_btc_data.drop(columns=exclude_list+[target,target_binary]).select_dtypes(include=['int64']).columns.tolist()\n",
        "\n",
        "# Identify 2-dimensional columns\n",
        "for column in binary_variables:\n",
        "    if len(scaled_btc_data[column].shape) != 1:  # Check if the column is not 1D\n",
        "        print(f\"Column '{column}' is 2-dimensional with shape {scaled_btc_data[column].shape}\")\n",
        "\n",
        "# Filter out 2D columns and remove target from predictors\n",
        "binary_variables = [col for col in binary_variables if len(scaled_btc_data[col].shape) == 1 and col != target]\n",
        "\n",
        "\n",
        "# Define predictors, all columns except the 7D_Pct_Change or YeoJohnson_7D_Pct_Change or 2D_Pct_Change or YeoJohnson_2D_Pct_Change\n",
        "predictors = scaled_btc_data.drop(columns=[target, target_binary] + exclude_list + binary_variables, axis=1).columns.tolist()\n",
        "\n",
        "# Display the first few rows of the DataFrame\n",
        "scaled_btc_data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IBuAYn0IlAuY"
      },
      "outputs": [],
      "source": [
        "# Check the predictors\n",
        "predictors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U0XLUiYKHVWQ"
      },
      "source": [
        "##Univariate Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XN-D0AY5HZSL"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "cols = predictors[1:] + [target]\n",
        "ncols = 5\n",
        "nrows = math.ceil(len(cols) / ncols)\n",
        "\n",
        "fig, axes = plt.subplots(nrows, ncols, figsize=(20, 4 * nrows))\n",
        "axes = axes.flatten()\n",
        "\n",
        "for ax, col in zip(axes, cols):\n",
        "    scaled_btc_data[col].hist(\n",
        "        bins=30,\n",
        "        ax=ax,\n",
        "        edgecolor='black'\n",
        "    )\n",
        "    ax.set_title(col)\n",
        "    ax.set_xlabel('')\n",
        "    ax.set_ylabel('')\n",
        "\n",
        "# turn off any unused subplots\n",
        "for ax in axes[len(cols):]:\n",
        "    fig.delaxes(ax)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yE1HSaF4HZt0"
      },
      "source": [
        "##Bivariate Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K9z1Q8Wgt8nN"
      },
      "outputs": [],
      "source": [
        "# Select numeric columns\n",
        "numeric_variables = scaled_btc_data.select_dtypes(include=['float64']).columns.tolist()\n",
        "\n",
        "# Identify 2-dimensional columns\n",
        "for column in numeric_variables:\n",
        "    if len(scaled_btc_data[column].shape) != 1:  # Check if the column is not 1D\n",
        "        print(f\"Column '{column}' is 2-dimensional with shape {scaled_btc_data[column].shape}\")\n",
        "\n",
        "# Filter out 2D columns and remove target from predictors\n",
        "numeric_variables = [col for col in numeric_variables if len(scaled_btc_data[col].shape) == 1 and col != target]\n",
        "\n",
        "# Drop rows with missing values\n",
        "data_numeric = scaled_btc_data[numeric_variables + [target]].dropna()\n",
        "\n",
        "# Define number of rows and columns for the subplot grid\n",
        "n_cols = 3\n",
        "n_rows = (len(numeric_variables) + n_cols - 1) // n_cols  # Ceiling division\n",
        "\n",
        "# Create figure with subplots\n",
        "fig = plt.figure(figsize=(15, 5 * n_rows))\n",
        "\n",
        "# Create scatter plots\n",
        "for i, column in enumerate(numeric_variables, 1):\n",
        "    ax = plt.subplot(n_rows, n_cols, i)\n",
        "    sns.scatterplot(data=data_numeric, x=column, y=target, ax=ax)\n",
        "    ax.set_title(f\"{column} vs {target}\")\n",
        "    ax.set_xlabel(column)\n",
        "    ax.set_ylabel(target)\n",
        "\n",
        "    # Rotate x-axis labels if they're too long\n",
        "    plt.xticks(rotation=45)\n",
        "\n",
        "# Adjust layout to prevent overlap\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LdtI_-7wugDu"
      },
      "outputs": [],
      "source": [
        "# Create correlation matrix\n",
        "correlation_matrix = data_numeric.corr()\n",
        "\n",
        "plt.figure(figsize=(20, 20))\n",
        "\n",
        "# Create heatmap with annotations\n",
        "sns.heatmap(correlation_matrix,\n",
        "            annot=True,\n",
        "            cmap='coolwarm',\n",
        "            center=0,\n",
        "            square=True,\n",
        "            fmt='.2f',\n",
        "            linewidths=0.5,\n",
        "            cbar_kws={\"shrink\": .5})\n",
        "\n",
        "plt.title('Correlation Heatmap: Numeric Variables vs Price Change', pad=20)\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.yticks(rotation=0)\n",
        "plt.tight_layout()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qmB9ZBX4qeb8"
      },
      "outputs": [],
      "source": [
        "# Drop rows with missing values\n",
        "data_binary = scaled_btc_data[binary_variables + [target]].dropna()\n",
        "\n",
        "# Define number of rows and columns for the subplot grid\n",
        "n_cols = 3\n",
        "n_rows = (len(binary_variables) + n_cols - 1) // n_cols  # Ceiling division\n",
        "\n",
        "# Create figure with subplots\n",
        "fig = plt.figure(figsize=(15, 5 * n_rows))\n",
        "\n",
        "# Create scatter plots\n",
        "for i, column in enumerate(binary_variables, 1):\n",
        "    ax = plt.subplot(n_rows, n_cols, i)\n",
        "    sns.scatterplot(data=data_binary, x=column, y=target, ax=ax)\n",
        "    ax.set_title(f\"{column} vs {target}\")\n",
        "    ax.set_xlabel(column)\n",
        "    ax.set_ylabel(target)\n",
        "\n",
        "    # Rotate x-axis labels if they're too long\n",
        "    plt.xticks(rotation=45)\n",
        "\n",
        "# Adjust layout to prevent overlap\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Y9u2_ldsDjY"
      },
      "outputs": [],
      "source": [
        "# Create correlation matrix\n",
        "correlation_matrix = data_binary.corr()\n",
        "\n",
        "plt.figure(figsize=(12, 10))\n",
        "\n",
        "# Create heatmap with annotations\n",
        "sns.heatmap(correlation_matrix,\n",
        "            annot=True,\n",
        "            cmap='coolwarm',\n",
        "            center=0,\n",
        "            square=True,\n",
        "            fmt='.3f',\n",
        "            linewidths=0.5,\n",
        "            cbar_kws={\"shrink\": .5})\n",
        "\n",
        "plt.title('Correlation Heatmap: Binary Variables vs Price Change', pad=20)\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.yticks(rotation=0)\n",
        "plt.tight_layout()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xVqfeS8kIIV7"
      },
      "source": [
        "# Model Building"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bm9dvWirVqQZ"
      },
      "source": [
        "- Time-Based Splitting\n",
        "- Selecting predictors\n",
        "- Interaction terms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j22H2a1Fxb5t"
      },
      "outputs": [],
      "source": [
        "from itertools import chain, combinations\n",
        "\n",
        "def gen_combinations(input):\n",
        "    return sum([list(map(list, combinations(input, i))) for i in range(1,len(input) + 1)], [])\n",
        "\n",
        "def gen_interactions(X,de,pname):\n",
        "    s = gen_combinations(range(len(pname)))\n",
        "    VX = []\n",
        "    VarX = []\n",
        "    for i in range(len(s)):\n",
        "        if len(s[i]) <= de:\n",
        "            VX.append(np.prod(X[:,s[i]],axis=1))\n",
        "            VarX.append(list([pname[x] for x in s[i]]))\n",
        "    VarX2 = []\n",
        "    for x in VarX:\n",
        "        if len(x) > 1:\n",
        "            VarX2.append('_'.join(x))\n",
        "        else:\n",
        "            VarX2.append(x[0])\n",
        "    VX = pd.DataFrame(np.array(VX).T,columns=VarX2)\n",
        "    return VX, VarX2\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uNDQMhX0rS0B"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import itertools\n",
        "\n",
        "# Define feature groups\n",
        "feature_groups = {\n",
        "    'Group 1': [\n",
        "        'Negative_DI', 'Positive_DI', 'ADX_Buy_Signal', 'ADX_Sell_Signal', 'ADX_Strength',\n",
        "        'ADX_Trend_Direction', 'ADX_Trend_Duration', 'ADX'\n",
        "    ],\n",
        "    'Group 2': [\n",
        "        'Ascending_Triangle_Breakout_With_Volume', 'Triangle_Completion', 'Triangle_Duration', 'Triangle_Height'\n",
        "    ],\n",
        "    'Group 3': [\n",
        "        'ATR_Ratio', 'ATR'\n",
        "    ],\n",
        "    'Group 4': [\n",
        "        'BB_Buy', 'BB_cum_Buy', 'BB_cum_Sell', 'BB_lower', 'BB_Sell', 'BB_upper',\n",
        "        'Price_above_BB_upper', 'Price_below_BB_lower'\n",
        "    ],\n",
        "    'Group 7': [\n",
        "        'Consecutive_RSI_12_Overbought', 'Consecutive_RSI_12_Oversold', 'RSI_12_Overbought',\n",
        "        'RSI_12_Oversold', 'RSI_12', 'RSI_6', 'RSI_Divergence'\n",
        "    ],\n",
        "    'Group 8': [\n",
        "        'Consecutive_Stochastic_Buy', 'Consecutive_Stochastic_Sell', 'Stochastic_perD',\n",
        "        'Stochastic_perK', 'Stochastic_Buy_Signal', 'Stochastic_Sell_Signal'\n",
        "    ],\n",
        "    'Group 11': [\n",
        "        'MA_Distance_Ratio', 'MA200', 'MA50', 'MACD_above_signal', 'MACD_below_signal',\n",
        "        'MACD_buy', 'MACD_cum_buy', 'MACD_cum_sell', 'MACD_sell', 'MACD_Signal',\n",
        "        'MACD', 'Price_vs_MA200', 'Price_vs_MA50'\n",
        "    ],\n",
        "    'Group 12': [\n",
        "        'OBV_Slope', 'OBV', 'Resistance', 'Support', 'Volume_Ratio', 'Volume'\n",
        "    ],\n",
        "    'Group 13': [\n",
        "        'SPY_Price', 'SPY_Volume'\n",
        "    ],\n",
        "    'Group 14': [\n",
        "        'US_10Y', 'Yield_Spread','Fear_and_Greed_Index', 'Extreme_Fear', 'Extreme_Greed', 'Consecutive_Extreme_Fear', 'Consecutive_Extreme_Greed'\n",
        "    ],\n",
        "    'Group 15': [\n",
        "        'Gold_Price', 'Gold_Volume'\n",
        "    ],\n",
        "    'Group 16': [\n",
        "        'ETH_Price', 'ETH_Volume'\n",
        "    ]\n",
        "}\n",
        "\n",
        "# Define interaction rules (between groups)\n",
        "interaction_rules = [\n",
        "    ('Group 1', 'Group 2'),  # ADX x Triangle Patterns\n",
        "    ('Group 3', 'Group 4'),  # ATR x Bollinger Bands\n",
        "    ('Group 7', 'Group 8'),  # RSI x Stochastic Oscillator\n",
        "    ('Group 11', 'Group 12'),  # MACD x Volume and Support/Resistance\n",
        "    ('Group 14', 'Group 13'),  # Macro Indicators x SPY\n",
        "    ('Group 15', 'Group 16')  # Gold x ETH\n",
        "]\n",
        "\n",
        "# Initialize an empty DataFrame for interaction terms\n",
        "interaction_terms = pd.DataFrame(index=scaled_btc_data.index)\n",
        "\n",
        "# Generate between-group interaction terms\n",
        "for group_a, group_b in interaction_rules:\n",
        "    print(f\"Generating interactions between {group_a} and {group_b}...\")\n",
        "    features_a = feature_groups[group_a]\n",
        "    features_b = feature_groups[group_b]\n",
        "    for feature_a, feature_b in itertools.product(features_a, features_b):  # All combinations between groups\n",
        "        interaction_name = f\"{feature_a}_x_{feature_b}\"\n",
        "        interaction_terms[interaction_name] = scaled_btc_data[feature_a] * scaled_btc_data[feature_b]\n",
        "\n",
        "\n",
        "interaction_list = interaction_terms.columns.tolist()\n",
        "# Add interaction terms to the original DataFrame\n",
        "scaled_btc_data_with_interactions = scaled_btc_data.join(interaction_terms)\n",
        "\n",
        "scaled_btc_data_with_interactions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s0xNlgt0T3I5"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "# Generate within-group interaction terms\n",
        "for group_name, features in feature_groups.items():\n",
        "    print(f\"Generating within-group interactions for {group_name}...\")\n",
        "    for feature_a, feature_b in itertools.combinations(features, 2):  # All unique pairs within the group\n",
        "        interaction_name = f\"{feature_a}_x_{feature_b}\"\n",
        "        interaction_terms[interaction_name] = scaled_btc_data[feature_a] * scaled_btc_data[feature_b]\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1XAV9LIN7kWU"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "# 1) Select and clean your base predictors '''\n",
        "relevant_predictors = [\n",
        "    'VWAP_Buy', 'MACD_buy', 'BB_Buy',\n",
        "    'RSI_12_Oversold', 'RSI_Divergence',\n",
        "    'VWAP', 'MACD', 'RSI_6', 'RSI_12', 'ATR'\n",
        "]\n",
        "df_base = scaled_btc_data[relevant_predictors].dropna()\n",
        "\n",
        "# 2) Generate all up-to-2-way interactions\n",
        "X       = df_base.values\n",
        "pnames  = df_base.columns.tolist()\n",
        "inter_df, inter_names = gen_interactions(X, de=2, pname=pnames)\n",
        "\n",
        "# 3) Build a set for quick membership tests\n",
        "excluded = { tuple(sorted(pair)) for pair in [\n",
        "    ('VWAP_Buy', 'VWAP_Sell'),\n",
        "    ('MACD_buy','MACD_sell'),\n",
        "    ('BB_Buy','BB_Sell'),\n",
        "]}\n",
        "\n",
        "# 4) Keep only the interactions we want\n",
        "keep_mask = [\n",
        "    not (len(name.split('_')) == 2\n",
        "         and tuple(sorted(name.split('_'))) in excluded)\n",
        "    for name in inter_names\n",
        "]\n",
        "valid_names = [name for name, keep in zip(inter_names, keep_mask) if keep]\n",
        "\n",
        "# 5) Subset and align with the original index\n",
        "inter_df = inter_df.loc[:, valid_names]\n",
        "inter_df.index = df_base.index\n",
        "\n",
        "# 6) Add a suffix to interaction columns to avoid overlap\n",
        "inter_df = inter_df.add_suffix('_interaction')\n",
        "\n",
        "# Join back onto your full DataFrame\n",
        "scaled_btc_data_with_interactions = scaled_btc_data.join(inter_df)\n",
        "scaled_btc_data_with_interactions\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4rcfqiT_yK9f"
      },
      "source": [
        "## Splitting the data into training, testing and validation datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lkd0FEqub-HK"
      },
      "outputs": [],
      "source": [
        "# Define predictors for classification\n",
        "\n",
        "final_df_2 = scaled_btc_data_with_interactions.copy()\n",
        "X_2 = final_df_2[predictors + binary_variables + interaction_list]  # Independent variables\n",
        "y_2 = final_df_2[target_binary]  # Target variable\n",
        "\n",
        "y_2 = pd.to_numeric(y_2, errors='coerce')\n",
        "\n",
        "\n",
        "# Time-based splitting (70% train, 15% validation, 15% test)\n",
        "train_size = int(len(final_df_2) * 0.7)\n",
        "val_size = int(len(final_df_2) * 0.15)\n",
        "\n",
        "# Create the splits for binary classification\n",
        "X_train_2 = X_2.iloc[:train_size]\n",
        "y_train_2 = y_2.iloc[:train_size]\n",
        "\n",
        "X_val_2 = X_2.iloc[train_size:train_size + val_size]\n",
        "y_val_2 = y_2.iloc[train_size:train_size + val_size]\n",
        "\n",
        "X_test_2 = X_2.iloc[train_size + val_size:]\n",
        "y_test_2 = y_2.iloc[train_size + val_size:]\n",
        "\n",
        "print(f\"Binary classification data shape: {X_2.shape}\")\n",
        "print(f\"Training set: {X_train_2.shape[0]} samples\")\n",
        "print(f\"Validation set: {X_val_2.shape[0]} samples\")\n",
        "print(f\"Test set: {X_test_2.shape[0]} samples\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wihir_uoypKo"
      },
      "source": [
        "## Defining functions for model performance evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ltsI3J8alty"
      },
      "outputs": [],
      "source": [
        "# Function to evaluate classification models\n",
        "def model_performance_classification(model, X, y):\n",
        "    \"\"\"Evaluate classification model performance with comprehensive metrics\"\"\"\n",
        "    from sklearn.metrics import (accuracy_score, precision_score, recall_score,\n",
        "                               f1_score, roc_curve, auc, confusion_matrix)\n",
        "\n",
        "    # Generate predictions\n",
        "    y_pred = model.predict(X)\n",
        "\n",
        "    # Calculate confusion matrix values\n",
        "    tn, fp, fn, tp = confusion_matrix(y, y_pred).ravel()\n",
        "\n",
        "    # Get probability predictions if available\n",
        "    if hasattr(model, \"predict_proba\"):\n",
        "        y_prob = model.predict_proba(X)[:, 1]\n",
        "        fpr, tpr, _ = roc_curve(y == list(set(y))[1], y_prob)\n",
        "        roc_auc = auc(fpr, tpr)\n",
        "    else:\n",
        "        roc_auc = None\n",
        "\n",
        "    # Return all metrics\n",
        "    return {\n",
        "        'accuracy': accuracy_score(y, y_pred),\n",
        "        'precision': precision_score(y, y_pred),\n",
        "        'recall': recall_score(y, y_pred),\n",
        "        'f1': f1_score(y, y_pred),\n",
        "        'error_rate': 1 - accuracy_score(y, y_pred),\n",
        "        'specificity': tn / (tn + fp),\n",
        "        'roc_auc': roc_auc\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oOYg-XksbJOF"
      },
      "source": [
        "##Model 1: ANN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S0wV4AcTSgqv"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "def ann_cla(x_train, y_train, x_test, y_test,\n",
        "            size, max_iter=100, trace=False, n_try=5):\n",
        "    best_train_mse = np.inf\n",
        "    best_test_mse  = np.inf\n",
        "    best_train_model = None\n",
        "    best_test_model  = None\n",
        "\n",
        "    for i in range(n_try):\n",
        "        model = MLPClassifier(\n",
        "            hidden_layer_sizes=(size,),\n",
        "            activation='relu',\n",
        "            solver='adam',\n",
        "            max_iter=max_iter,\n",
        "            random_state=i\n",
        "        )\n",
        "        model.fit(x_train, y_train)\n",
        "\n",
        "        # predict class labels (0/1)\n",
        "        y_train_pred = model.predict(x_train)\n",
        "        y_test_pred  = model.predict(x_test)\n",
        "\n",
        "        # MSE on 0/1 labels = error rate\n",
        "        train_mse = mean_squared_error(y_train, y_train_pred)\n",
        "        test_mse  = mean_squared_error(y_test,  y_test_pred)\n",
        "\n",
        "        if trace:\n",
        "            print(f\"Run {i+1}/{n_try} → train MSE: {train_mse:.4f}, test MSE: {test_mse:.4f}\")\n",
        "\n",
        "        # pick best by train\n",
        "        if train_mse < best_train_mse:\n",
        "            best_train_mse   = train_mse\n",
        "            best_train_model = model\n",
        "\n",
        "        # pick best by test\n",
        "        if test_mse < best_test_mse:\n",
        "            best_test_mse    = test_mse\n",
        "            best_test_model  = model\n",
        "\n",
        "    return {\n",
        "        'best_train_mse':   best_train_mse,\n",
        "        'best_test_mse':    best_test_mse,\n",
        "        'best_train_model': best_train_model,\n",
        "        'best_test_model':  best_test_model\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LpnUHoJHSgqv"
      },
      "outputs": [],
      "source": [
        "model_ann_cla = ann_cla(X_train_2, y_train_2, X_test_2, y_test_2,\n",
        "                8, max_iter=100, trace=False, n_try=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pX7aT57bSgqv"
      },
      "outputs": [],
      "source": [
        "model_ann_cla\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UHIQNsr_v6eB"
      },
      "outputs": [],
      "source": [
        "ann_model = model_ann_cla['best_test_model']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8X9WR8l-f4zg"
      },
      "source": [
        "## Model 2: Classification Tree & Random Forest\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DI7Dqgr3f6WO"
      },
      "outputs": [],
      "source": [
        "# Function to evaluate classification performance\n",
        "def model_performance_classification(model, X, y):\n",
        "    \"\"\"Evaluate classification model performance with comprehensive metrics\"\"\"\n",
        "    from sklearn.metrics import (accuracy_score, precision_score, recall_score,\n",
        "                               f1_score, roc_curve, auc, confusion_matrix)\n",
        "\n",
        "    # Generate predictions\n",
        "    y_pred = model.predict(X)\n",
        "\n",
        "    # Calculate confusion matrix values\n",
        "    tn, fp, fn, tp = confusion_matrix(y, y_pred).ravel()\n",
        "\n",
        "    # Get probability predictions if available\n",
        "    if hasattr(model, \"predict_proba\"):\n",
        "        y_prob = model.predict_proba(X)[:, 1]\n",
        "        fpr, tpr, _ = roc_curve(y == list(set(y))[1], y_prob)\n",
        "        roc_auc = auc(fpr, tpr)\n",
        "    else:\n",
        "        roc_auc = None\n",
        "\n",
        "    # Return all metrics\n",
        "    return {\n",
        "        'accuracy': accuracy_score(y, y_pred),\n",
        "        'precision': precision_score(y, y_pred),\n",
        "        'recall': recall_score(y, y_pred),\n",
        "        'f1': f1_score(y, y_pred),\n",
        "        'error_rate': 1 - accuracy_score(y, y_pred),\n",
        "        'specificity': tn / (tn + fp),\n",
        "        'roc_auc': roc_auc\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lHNps0suXQk-"
      },
      "outputs": [],
      "source": [
        "# Build Classification tree\n",
        "dt_model_2 = DecisionTreeClassifier(\n",
        "    max_depth=5,\n",
        "    min_samples_split=30,\n",
        "    min_samples_leaf=15,\n",
        "    criterion='gini',\n",
        "    class_weight='balanced',\n",
        "    splitter='best',\n",
        "    max_features='sqrt',\n",
        "    min_impurity_decrease=0.0001,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "dt_model_2.fit(X_train_2, y_train_2)\n",
        "dt_model_2.fit(X_train_2, y_train_2)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xpDihOSQFnqS"
      },
      "outputs": [],
      "source": [
        "# Visualize decision tree\n",
        "plt.figure(figsize=(25, 20))\n",
        "plot_tree(dt_model_2, feature_names=X_train_2.columns, filled=True,\n",
        "          rounded=True, fontsize=10, class_names=['No Increase', 'Increase'])\n",
        "plt.title('Basic Decision Tree for Bitcoin Price Direction', fontsize=15)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Evaluate on validation\n",
        "dt_val_performance = model_performance_classification(dt_model_2, X_val_2, y_val_2)\n",
        "print(\"Basic Decision Tree performance:\", dt_val_performance)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize decision tree\n",
        "plt.figure(figsize=(30, 25))\n",
        "plot_tree(dt_model_2, feature_names=X_train_2.columns, filled=True,\n",
        "          rounded=True, fontsize=10, class_names=['No Increase', 'Increase'])\n",
        "plt.title('Basic Decision Tree for Bitcoin Price Direction', fontsize=15)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Evaluate on validation\n",
        "dt_val_performance = model_performance_classification(dt_model_2, X_val_2, y_val_2)\n",
        "print(\"Basic Decision Tree performance:\", dt_val_performance)"
      ],
      "metadata": {
        "id": "vAplb2YWl5zT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KCg69edRgM_7"
      },
      "source": [
        "Validation performance: {'Accuracy': 0.47307692307692306, 'Precision': 0.47186147186147187, 'Recall': 0.8790322580645161, 'F1 Score': 0.6140845070422535}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2djGeVXPgVrh"
      },
      "outputs": [],
      "source": [
        "# Build random forest classifier\n",
        "classifier = RandomForestClassifier(\n",
        "    n_estimators=100,\n",
        "    max_depth=8,\n",
        "    min_samples_split=20,\n",
        "    min_samples_leaf=10,\n",
        "    max_features='sqrt',\n",
        "    bootstrap=True,\n",
        "    criterion='gini',\n",
        "    max_leaf_nodes=None,\n",
        "    min_impurity_decrease=0.0001,\n",
        "    class_weight='balanced',\n",
        "    max_samples=0.8,\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "\n",
        "with tqdm(total=1, desc=\"Fitting classifier\") as pbar:\n",
        "    classifier.fit(X_train_2, y_train_2)\n",
        "    pbar.update(1)\n",
        "\n",
        "# Evaluate on validation set\n",
        "val_performance = model_performance_classification(classifier, X_val_2, y_val_2)\n",
        "print(\"Validation performance:\", val_performance)\n",
        "\n",
        "# Calculate permutation importance on validation set with progress bar\n",
        "print(\"\\nCalculating feature importance...\")\n",
        "with tqdm(total=1, desc=\"Permutation importance\") as pbar:\n",
        "    perm_result = permutation_importance(\n",
        "        classifier, X_val_2, y_val_2,\n",
        "        n_repeats=10,\n",
        "        random_state=42,\n",
        "        n_jobs=-1\n",
        "    )\n",
        "    pbar.update(1)\n",
        "\n",
        "# Get importance scores and sort them\n",
        "importances = perm_result.importances_mean\n",
        "indices = np.argsort(importances)[::-1]  # Sort in descending order\n",
        "\n",
        "# Plot only top 15 feature importances\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.title('Top 15 Feature Importance for Direction Prediction (Permutation Method)', fontsize=15)\n",
        "\n",
        "# Select top 15 features\n",
        "top_n = 15\n",
        "top_indices = indices[:top_n]\n",
        "top_importances = importances[top_indices]\n",
        "feature_names = [X_train_2.columns[i] for i in top_indices]\n",
        "\n",
        "# Create the bar plot for top features\n",
        "plt.bar(range(len(top_indices)), top_importances, color='darkblue')\n",
        "plt.xticks(range(len(top_indices)), feature_names, rotation=90)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Select features with positive importance\n",
        "threshold = 0  # Only include features that improve predictions\n",
        "selected_features = [X_train_2.columns[i] for i in indices if importances[i] > threshold]\n",
        "print(f\"Selected {len(selected_features)} out of {X_train_2.shape[1]} features\")\n",
        "print(\"Selected features:\", selected_features)\n",
        "\n",
        "# Create datasets with only selected features\n",
        "X_train_2_selected = X_train_2[selected_features]\n",
        "X_val_2_selected = X_val_2[selected_features]\n",
        "X_test_2_selected = X_test_2[selected_features]\n",
        "\n",
        "# Hyperparameter tuning for classification\n",
        "rf_tuned = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Grid of parameters specifically for classification\n",
        "parameters = {\n",
        "    'n_estimators': [100, 200],\n",
        "    'max_depth': [None, 10, 20],\n",
        "    'max_features': ['sqrt', 'log2', 0.7],\n",
        "    'min_samples_split': [2, 5],\n",
        "    'min_samples_leaf': [1, 2, 4],\n",
        "    'class_weight': ['balanced', 'balanced_subsample', None]  # Important for imbalanced classes\n",
        "}\n",
        "\n",
        "# Use time series cross-validation\n",
        "tscv = TimeSeriesSplit(n_splits=5)\n",
        "\n",
        "# Calculate total parameter combinations\n",
        "total_combinations = (\n",
        "    len(parameters['n_estimators']) *\n",
        "    len(parameters['max_depth']) *\n",
        "    len(parameters['max_features']) *\n",
        "    len(parameters['min_samples_split']) *\n",
        "    len(parameters['min_samples_leaf']) *\n",
        "    len(parameters['class_weight'])\n",
        ")\n",
        "total_fits = total_combinations * tscv.n_splits\n",
        "\n",
        "# Run the grid search with progress tracking\n",
        "print(f\"\\nStarting grid search with {total_combinations} parameter combinations\")\n",
        "print(f\"Total model fits to perform: {total_fits}\\n\")\n",
        "\n",
        "# Create a wrapper to track progress\n",
        "class ProgressGridSearchCV(GridSearchCV):\n",
        "    def fit(self, X, y):\n",
        "        with tqdm(total=1, desc=\"Grid search progress\") as pbar:\n",
        "            result = super().fit(X, y)\n",
        "            pbar.update(1)\n",
        "        return result\n",
        "\n",
        "grid_obj = ProgressGridSearchCV(\n",
        "    rf_tuned,\n",
        "    parameters,\n",
        "    scoring='f1',  # F1 score is good for imbalanced classes\n",
        "    cv=tscv,\n",
        "    n_jobs=-1,     # Use all CPU cores\n",
        "    verbose=0      # Disable default verbose since we're using tqdm\n",
        ")\n",
        "\n",
        "print(\"Starting hyperparameter search...\")\n",
        "grid_obj.fit(X_train_2_selected, y_train_2)\n",
        "print(\"Grid search completed!\")\n",
        "\n",
        "# Get the best model\n",
        "rf_tuned_classifier = grid_obj.best_estimator_\n",
        "print(\"Best parameters:\", grid_obj.best_params_)\n",
        "tuned_val_performance = model_performance_classification(rf_tuned_classifier, X_val_2_selected, y_val_2)\n",
        "print(\"Tuned model validation performance:\", tuned_val_performance)\n",
        "\n",
        "# Show confusion matrix\n",
        "y_val_pred = rf_tuned_classifier.predict(X_val_2_selected)\n",
        "cm = confusion_matrix(y_val_2, y_val_pred)\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
        "plt.title('Confusion Matrix - Validation Set')\n",
        "plt.colorbar()\n",
        "tick_marks = np.arange(2)\n",
        "plt.xticks(tick_marks, ['No Increase', 'Increase'], rotation=45)\n",
        "plt.yticks(tick_marks, ['No Increase', 'Increase'])\n",
        "plt.tight_layout()\n",
        "plt.ylabel('True Label')\n",
        "plt.xlabel('Predicted Label')\n",
        "\n",
        "# Add text annotations\n",
        "thresh = cm.max() / 2\n",
        "for i in range(cm.shape[0]):\n",
        "    for j in range(cm.shape[1]):\n",
        "        plt.text(j, i, format(cm[i, j], 'd'),\n",
        "                 horizontalalignment=\"center\",\n",
        "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "plt.show()\n",
        "\n",
        "# Train final model on combined training and validation data\n",
        "print(\"\\nTraining final model on combined training and validation data...\")\n",
        "X_train_val_2 = pd.concat([X_train_2_selected, X_val_2_selected])\n",
        "y_train_val_2 = pd.concat([y_train_2, y_val_2])\n",
        "\n",
        "final_rfmodel_2 = RandomForestClassifier(**grid_obj.best_params_, random_state=1)\n",
        "with tqdm(total=1, desc=\"Training final model\") as pbar:\n",
        "    final_rfmodel_2.fit(X_train_val_2, y_train_val_2)\n",
        "    pbar.update(1)\n",
        "\n",
        "# Final evaluation on test set only once\n",
        "final_test_performance = model_performance_classification(final_rfmodel_2, X_test_2_selected, y_test_2)\n",
        "print(\"\\nFinal test performance:\", final_test_performance)\n",
        "\n",
        "# Plot prediction probabilities on test set\n",
        "y_test_proba = final_rfmodel_2.predict_proba(X_test_2_selected)[:, 1]  # Probability of class 1 (increase)\n",
        "plt.figure(figsize=(14, 6))\n",
        "plt.title('Bitcoin Direction: Probability of Price Increase', fontsize=15)\n",
        "plt.plot(y_test_2.index, y_test_proba, label='Probability of Increase', color='blue')\n",
        "plt.axhline(y=0.5, color='red', linestyle='--', label='Decision Threshold')\n",
        "# Add actual outcomes as points\n",
        "for idx, actual in zip(y_test_2.index, y_test_2):\n",
        "    marker = 'o' if actual == 1 else 'x'\n",
        "    color = 'green' if actual == 1 else 'red'\n",
        "    plt.scatter(idx, y_test_proba[y_test_2.index.get_loc(idx)], marker=marker, color=color, s=50)\n",
        "plt.xlabel('Date', fontsize=12)\n",
        "plt.ylabel('Probability', fontsize=12)\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Function to predict with confidence using prediction probabilities\n",
        "def predict_with_confidence_classification(model, X_data, n_bootstrap=100):\n",
        "    \"\"\"Generate predictions with confidence intervals for classification probabilities\"\"\"\n",
        "    all_proba = []\n",
        "\n",
        "    # Get all individual tree predictions (probabilities) with progress bar\n",
        "    for tree in tqdm(model.estimators_, desc=\"Building confidence intervals\"):\n",
        "        # For each tree, get the probability of class 1 (price increase)\n",
        "        tree_proba = tree.predict_proba(X_data)[:, 1]\n",
        "        all_proba.append(tree_proba)\n",
        "\n",
        "    # Convert to numpy array\n",
        "    all_proba = np.array(all_proba)\n",
        "\n",
        "    # Calculate mean probability (overall prediction)\n",
        "    mean_proba = np.mean(all_proba, axis=0)\n",
        "\n",
        "    # Calculate confidence intervals\n",
        "    lower_bound = np.percentile(all_proba, 2.5, axis=0)\n",
        "    upper_bound = np.percentile(all_proba, 97.5, axis=0)\n",
        "\n",
        "    # Predict labels based on mean probability\n",
        "    pred_labels = (mean_proba >= 0.5).astype(int)\n",
        "\n",
        "    return mean_proba, lower_bound, upper_bound, pred_labels\n",
        "\n",
        "# Generate predictions with confidence intervals (already has progress bar)\n",
        "print(\"\\nGenerating predictions with confidence intervals...\")\n",
        "mean_probs, lower_bounds, upper_bounds, pred_labels = predict_with_confidence_classification(\n",
        "    final_rfmodel_2, X_test_2_selected\n",
        ")\n",
        "\n",
        "# Display prediction for the first test sample\n",
        "first_idx = 0\n",
        "direction = \"UP\" if pred_labels[first_idx] == 1 else \"DOWN\"\n",
        "confidence = mean_probs[first_idx] if pred_labels[first_idx] == 1 else 1 - mean_probs[first_idx]\n",
        "margin = (upper_bounds[first_idx] - lower_bounds[first_idx]) / 2\n",
        "\n",
        "print(f\"\\nPredicted Bitcoin direction: {direction}\")\n",
        "print(f\"Confidence: {confidence*100:.2f}% ± {margin*100:.2f}% with 95% confidence interval\")\n",
        "print(f\"Probability range: [{lower_bounds[first_idx]*100:.2f}%, {upper_bounds[first_idx]*100:.2f}%]\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RS2LrpqWgcLa"
      },
      "outputs": [],
      "source": [
        "# Feature importance for final model\n",
        "final_importances = pd.DataFrame({\n",
        "    'Feature': selected_features,\n",
        "    'Importance': final_rfmodel_2.feature_importances_\n",
        "})\n",
        "final_importances = final_importances.sort_values('Importance', ascending=False)\n",
        "\n",
        "# Get only top 15 features\n",
        "top_15_features = final_importances.head(15)\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.title('Final Model: Top 15 Features for Direction Prediction', fontsize=15)\n",
        "plt.barh(range(len(top_15_features)), top_15_features['Importance'], color='darkblue')\n",
        "plt.yticks(range(len(top_15_features)), top_15_features['Feature'])\n",
        "plt.xlabel('Importance')\n",
        "plt.gca().invert_yaxis()  # To have the highest importance at the top\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nTop 15 most important features for predicting price direction:\")\n",
        "print(final_importances.head(15))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ACXOVxARF2Xr"
      },
      "source": [
        "##Model 3: Logistic Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gsmv-Wx-gviF"
      },
      "source": [
        "Process time around 80 mins"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TCWJ5-DWF__Q"
      },
      "outputs": [],
      "source": [
        "# Stepwise regression function\n",
        "def stepwise_logistic_regression(X, y, initial_features=None, p_value_threshold=0.05,\n",
        "                               direction='forward', max_iterations=50, max_features=15):\n",
        "\n",
        "    all_features = X.columns.tolist()\n",
        "\n",
        "    # Start with empty list for forward selection\n",
        "    current_features = [] if initial_features is None else [f for f in initial_features if f in all_features]\n",
        "\n",
        "    # Add constant for statsmodels\n",
        "    X_with_const = sm.add_constant(X)\n",
        "\n",
        "    # Initialize variables\n",
        "    best_features = current_features.copy()\n",
        "    iterations_log = []\n",
        "    iteration = 0\n",
        "    improved = True\n",
        "    best_aic = float('inf')\n",
        "    best_model = None\n",
        "\n",
        "    print(f\"Starting with {len(current_features)} features\")\n",
        "    if current_features:\n",
        "        print(f\"Initial features: {current_features}\")\n",
        "\n",
        "    # Start stepwise selection\n",
        "    while improved and iteration < max_iterations and len(current_features) < max_features:\n",
        "        improved = False\n",
        "        iteration += 1\n",
        "        print(f\"\\nIteration {iteration}:\")\n",
        "\n",
        "        # Define candidate features for forward selection\n",
        "        forward_candidates = [f for f in all_features if f not in current_features]\n",
        "\n",
        "        # Try adding one feature (forward selection)\n",
        "        best_forward_feature = None\n",
        "        best_forward_aic = float('inf')\n",
        "\n",
        "        if direction in ['forward', 'both']:\n",
        "            for feature in forward_candidates:\n",
        "                candidate_features = current_features + [feature]\n",
        "                try:\n",
        "                    # Prepare features including constant\n",
        "                    X_subset = X_with_const[['const'] + candidate_features]\n",
        "\n",
        "                    # Fit model and get AIC\n",
        "                    try:\n",
        "                        model = sm.Logit(y, X_subset).fit(disp=0, method='lbfgs', maxiter=200)\n",
        "                    except:\n",
        "                        model = sm.Logit(y, X_subset).fit(disp=0, method='bfgs', maxiter=200)\n",
        "\n",
        "                    aic = model.aic\n",
        "\n",
        "                    # Check if p-value for the newly added feature is significant\n",
        "                    p_value = model.pvalues.get(feature, 1.0)\n",
        "\n",
        "                    if p_value <= p_value_threshold and aic < best_forward_aic:\n",
        "                        best_forward_aic = aic\n",
        "                        best_forward_feature = feature\n",
        "                        best_forward_model = model\n",
        "                        print(f\"  Testing adding {feature}: AIC={aic:.2f}, p-value={p_value:.4f}\")\n",
        "                except Exception as e:\n",
        "                    print(f\"  Error testing {feature}: {str(e)[:60]}...\")\n",
        "                    continue\n",
        "\n",
        "        # Check if we can add the feature\n",
        "        if best_forward_feature and (best_forward_aic < best_aic):\n",
        "            # Add the best feature\n",
        "            print(f\"Adding feature: {best_forward_feature}, AIC: {best_forward_aic:.2f}\")\n",
        "            current_features.append(best_forward_feature)\n",
        "            best_aic = best_forward_aic\n",
        "            best_model = best_forward_model\n",
        "            improved = True\n",
        "            iterations_log.append({\n",
        "                'iteration': iteration,\n",
        "                'action': 'add',\n",
        "                'feature': best_forward_feature,\n",
        "                'aic': best_forward_aic,\n",
        "                'features': current_features.copy()\n",
        "            })\n",
        "\n",
        "        # If direction includes backward, check for insignificant features to remove\n",
        "        if direction in ['backward', 'both'] and len(current_features) > 1 and best_model is not None:\n",
        "            # Get p-values for all features\n",
        "            p_values = {col: best_model.pvalues.get(col, 1.0) for col in current_features}\n",
        "\n",
        "            # Find the most insignificant feature (if any)\n",
        "            insignificant_features = [f for f in current_features if p_values.get(f, 0) > p_value_threshold]\n",
        "\n",
        "            if insignificant_features:\n",
        "                # Sort by p-value (most insignificant first)\n",
        "                insignificant_features.sort(key=lambda f: p_values.get(f, 0), reverse=True)\n",
        "\n",
        "                # Remove the most insignificant feature and refit\n",
        "                feature_to_remove = insignificant_features[0]\n",
        "                candidate_features = [f for f in current_features if f != feature_to_remove]\n",
        "\n",
        "                try:\n",
        "                    # Prepare features including constant\n",
        "                    X_subset = X_with_const[['const'] + candidate_features]\n",
        "\n",
        "                    # Fit model and get AIC\n",
        "                    model = sm.Logit(y, X_subset).fit(disp=0, method='lbfgs', maxiter=200)\n",
        "                    aic = model.aic\n",
        "\n",
        "                    print(f\"Testing removal of {feature_to_remove}: AIC={aic:.2f}, p-value={p_values.get(feature_to_remove, 1):.4f}\")\n",
        "\n",
        "                    # If removing improves AIC, remove it\n",
        "                    if aic < best_aic:\n",
        "                        print(f\"Removing feature: {feature_to_remove}, AIC: {aic:.2f}\")\n",
        "                        current_features = candidate_features\n",
        "                        best_aic = aic\n",
        "                        best_model = model\n",
        "                        improved = True\n",
        "                        iterations_log.append({\n",
        "                            'iteration': iteration,\n",
        "                            'action': 'remove',\n",
        "                            'feature': feature_to_remove,\n",
        "                            'aic': aic,\n",
        "                            'features': current_features.copy()\n",
        "                        })\n",
        "                except Exception as e:\n",
        "                    print(f\"Error removing {feature_to_remove}: {str(e)[:60]}...\")\n",
        "\n",
        "        # Update best features if improved\n",
        "        if improved:\n",
        "            best_features = current_features.copy()\n",
        "\n",
        "        print(f\"Current features ({len(current_features)}): {current_features}\")\n",
        "        print(f\"Current AIC: {best_aic:.2f}\")\n",
        "\n",
        "    # Final model with best features\n",
        "    if best_features:\n",
        "        try:\n",
        "            X_final = X_with_const[['const'] + best_features]\n",
        "            final_model = sm.Logit(y, X_final).fit(disp=0, method='lbfgs', maxiter=200)\n",
        "            return best_features, final_model, iterations_log\n",
        "        except Exception as e:\n",
        "            print(f\"Error fitting final model: {str(e)}\")\n",
        "            return best_features, best_model, iterations_log\n",
        "\n",
        "    return [], None, iterations_log\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vLxUcwc4hJ8Z"
      },
      "outputs": [],
      "source": [
        "# Calculate comprehensive metrics\n",
        "def cal_metrics(y_true, y_pred):\n",
        "    \"\"\"Calculate detailed classification metrics\"\"\"\n",
        "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
        "    accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
        "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
        "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
        "    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
        "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
        "    false_negative_rate = fn / (tp + fn) if (tp + fn) > 0 else 0\n",
        "    error_rate = (fp + fn) / (tp + tn + fp + fn)\n",
        "\n",
        "    return {\n",
        "        'Accuracy': accuracy,\n",
        "        'Precision': precision,\n",
        "        'Recall': recall,\n",
        "        'Specificity': specificity,\n",
        "        'F1_Score': f1,\n",
        "        'False_Negative_Rate': false_negative_rate,\n",
        "        'Error_Rate': error_rate\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_l7fy4niKXaz"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "# Build stepwise logistic regression\n",
        "start_time = time.time()\n",
        "\n",
        "initial_features = []  # Start with empty set\n",
        "\n",
        "# Run stepwise regression\n",
        "final_features, final_model, iterations_log = stepwise_logistic_regression(\n",
        "    X_train_2,\n",
        "    y_train_2,\n",
        "    initial_features=initial_features,\n",
        "    p_value_threshold=0.05,\n",
        "    direction='both',  # Use both forward and backward steps\n",
        "    max_iterations=100,\n",
        "    max_features=300\n",
        ")\n",
        "\n",
        "elapsed_time = time.time() - start_time\n",
        "print(f\"\\nStepwise regression completed in {elapsed_time:.2f} seconds\")\n",
        "print(f\"Selected {len(final_features)} features out of {len(X_train_2.columns)} candidates\")\n",
        "print(\"Selected features:\", final_features)\n",
        "\n",
        "# Visualize the feature selection process\n",
        "iterations_df = pd.DataFrame(iterations_log)\n",
        "if not iterations_df.empty:\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.plot(iterations_df['iteration'], iterations_df['aic'], 'o-', color='blue')\n",
        "    plt.xlabel('Iteration')\n",
        "    plt.ylabel('AIC Score')\n",
        "    plt.title('AIC Score Improvement During Stepwise Selection')\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "    # Annotate add/remove actions\n",
        "    for i, row in iterations_df.iterrows():\n",
        "        action_color = 'green' if row['action'] == 'add' else 'red'\n",
        "        plt.annotate(\n",
        "            f\"{row['action']}: {row['feature'][:15]}\",\n",
        "            (row['iteration'], row['aic']),\n",
        "            textcoords=\"offset points\",\n",
        "            xytext=(0, 10 if i % 2 == 0 else -20),\n",
        "            ha='center',\n",
        "            color=action_color,\n",
        "            fontsize=8\n",
        "        )\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Feature importance from the final model\n",
        "if final_model is not None:\n",
        "    # Get model coefficients (excluding constant)\n",
        "    coefficients = final_model.params.drop('const', errors='ignore')\n",
        "\n",
        "    # Calculate odds ratios and confidence intervals\n",
        "    conf_int = final_model.conf_int()\n",
        "    odds_ratios = np.exp(coefficients)\n",
        "    odds_ratios_ci = np.exp(conf_int.drop('const', errors='ignore'))\n",
        "\n",
        "    # Create a DataFrame with the results\n",
        "    feature_importance = pd.DataFrame({\n",
        "        'Feature': coefficients.index,\n",
        "        'Coefficient': coefficients.values,\n",
        "        'Odds_Ratio': odds_ratios.values,\n",
        "        'CI_Lower': odds_ratios_ci[0].values,\n",
        "        'CI_Upper': odds_ratios_ci[1].values,\n",
        "        'P_Value': final_model.pvalues.drop('const', errors='ignore').values\n",
        "    })\n",
        "\n",
        "    # Sort by absolute coefficient value\n",
        "    feature_importance['Abs_Coefficient'] = feature_importance['Coefficient'].abs()\n",
        "    feature_importance = feature_importance.sort_values('Abs_Coefficient', ascending=False).reset_index(drop=True)\n",
        "\n",
        "    # Plot feature importance\n",
        "    plt.figure(figsize=(12, max(8, len(feature_importance) * 0.3)))\n",
        "\n",
        "    # Color based on positive/negative impact\n",
        "    colors = ['green' if c > 0 else 'red' for c in feature_importance['Coefficient']]\n",
        "\n",
        "    # Create the bar plot\n",
        "    plt.barh(\n",
        "        feature_importance['Feature'],\n",
        "        feature_importance['Coefficient'],\n",
        "        color=colors\n",
        "    )\n",
        "\n",
        "    plt.axvline(x=0, color='black', linestyle='-', alpha=0.3)\n",
        "    plt.title('Feature Importance for Bitcoin Price Direction')\n",
        "    plt.xlabel('Coefficient (Log Odds)')\n",
        "    plt.ylabel('Feature')\n",
        "    plt.grid(True, axis='x', alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Print detailed feature importance table\n",
        "    print(\"\\nFeature Importance:\")\n",
        "    pd.set_option('display.max_rows', None)\n",
        "    print(feature_importance[['Feature', 'Coefficient', 'Odds_Ratio', 'P_Value', 'CI_Lower', 'CI_Upper']])\n",
        "\n",
        "    # Print model summary\n",
        "    print(\"\\nFinal Model Summary:\")\n",
        "    print(final_model.summary())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k6EitHDvKY7r"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "# Evaluate on validation set\n",
        "print(\"\\nEvaluating on validation set...\")\n",
        "\n",
        "# Add constant for predictions\n",
        "X_val_selected = sm.add_constant(X_val_2[final_features])\n",
        "\n",
        "# Get validation predictions\n",
        "val_probs = final_model.predict(X_val_selected)\n",
        "val_preds = (val_probs > 0.5).astype(int)\n",
        "\n",
        "\n",
        "# Calculate detailed validation metrics\n",
        "val_metrics = cal_metrics(y_val_2, val_preds)\n",
        "val_auc = roc_auc_score(y_val_2, val_probs)\n",
        "\n",
        "print(\"\\nValidation Set Performance:\")\n",
        "print(f\"Accuracy: {val_metrics['Accuracy']:.4f}\")\n",
        "print(f\"AUC: {val_auc:.4f}\")\n",
        "print(f\"Recall (Sensitivity): {val_metrics['Recall']:.4f}\")\n",
        "print(f\"Specificity: {val_metrics['Specificity']:.4f}\")\n",
        "print(f\"Precision: {val_metrics['Precision']:.4f}\")\n",
        "print(f\"F1 Score: {val_metrics['F1_Score']:.4f}\")\n",
        "print(f\"False Negative Rate: {val_metrics['False_Negative_Rate']:.4f}\")\n",
        "print(f\"Error Rate: {val_metrics['Error_Rate']:.4f}\")\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_val_2, val_preds))\n",
        "\n",
        "# Plot confusion matrix\n",
        "plt.figure(figsize=(8, 6))\n",
        "cm = confusion_matrix(y_val_2, val_preds)\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
        "plt.title('Confusion Matrix (Validation)')\n",
        "plt.ylabel('True Label')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.xticks([0.5, 1.5], ['Decrease/No Change', 'Increase'])\n",
        "plt.yticks([0.5, 1.5], ['Decrease/No Change', 'Increase'])\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Find optimal threshold\n",
        "print(\"\\nFinding optimal threshold...\")\n",
        "# Calculate metrics at different thresholds\n",
        "thresholds = np.arange(0.01, 1, 0.01)\n",
        "accuracy_scores = []\n",
        "f1_scores = []\n",
        "precision_scores = []\n",
        "recall_scores = []\n",
        "specificity_scores = []\n",
        "\n",
        "for threshold in thresholds:\n",
        "    y_pred = (val_probs >= threshold).astype(int)\n",
        "    metrics = cal_metrics(y_val_2, y_pred)\n",
        "    accuracy_scores.append(metrics['Accuracy'])\n",
        "    f1_scores.append(metrics['F1_Score'])\n",
        "    precision_scores.append(metrics['Precision'])\n",
        "    recall_scores.append(metrics['Recall'])\n",
        "    specificity_scores.append(metrics['Specificity'])\n",
        "\n",
        "# Plot metrics vs threshold\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.plot(thresholds, accuracy_scores, 'b-', label='Accuracy')\n",
        "plt.plot(thresholds, f1_scores, 'g-', label='F1 Score')\n",
        "plt.plot(thresholds, precision_scores, 'r-', label='Precision')\n",
        "plt.plot(thresholds, recall_scores, 'y-', label='Recall (Sensitivity)')\n",
        "plt.plot(thresholds, specificity_scores, 'm-', label='Specificity')\n",
        "plt.xlabel('Threshold')\n",
        "plt.ylabel('Score')\n",
        "plt.title('Metrics vs. Threshold')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.legend()\n",
        "\n",
        "# Find optimal thresholds for different metrics\n",
        "opt_acc_threshold = thresholds[np.argmax(accuracy_scores)]\n",
        "opt_f1_threshold = thresholds[np.argmax(f1_scores)]\n",
        "\n",
        "# Find threshold where precision and recall are closest (balanced)\n",
        "pr_diff = np.abs(np.array(precision_scores) - np.array(recall_scores))\n",
        "opt_pr_threshold = thresholds[np.argmin(pr_diff)]\n",
        "\n",
        "plt.axvline(x=0.5, color='grey', linestyle='--', label='Default (0.5)')\n",
        "plt.axvline(x=opt_acc_threshold, color='blue', linestyle='--',\n",
        "            label=f'Optimal Accuracy ({opt_acc_threshold:.2f})')\n",
        "plt.axvline(x=opt_f1_threshold, color='green', linestyle='--',\n",
        "            label=f'Optimal F1 ({opt_f1_threshold:.2f})')\n",
        "plt.axvline(x=opt_pr_threshold, color='purple', linestyle='--',\n",
        "            label=f'Balanced P-R ({opt_pr_threshold:.2f})')\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"Optimal threshold for accuracy: {opt_acc_threshold:.2f}\")\n",
        "print(f\"Optimal threshold for F1 score: {opt_f1_threshold:.2f}\")\n",
        "print(f\"Balanced precision-recall threshold: {opt_pr_threshold:.2f}\")\n",
        "\n",
        "# Choose the threshold to use (you might prefer F1 for imbalanced data)\n",
        "chosen_threshold = opt_f1_threshold\n",
        "print(f\"Using threshold: {chosen_threshold:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oNHxTMf9KaeJ"
      },
      "outputs": [],
      "source": [
        "# Retrain on combined train+validation data with selected features\n",
        "print(\"\\nRetraining on combined training and validation data...\")\n",
        "X_train_val = pd.concat([X_train_2[final_features], X_val_2[final_features]])\n",
        "y_train_val = pd.concat([y_train_2, y_val_2])\n",
        "\n",
        "# Add constant for statsmodels\n",
        "X_train_val_sm = sm.add_constant(X_train_val)\n",
        "\n",
        "# Fit final statsmodels model\n",
        "try:\n",
        "    final_combined_model = sm.Logit(y_train_val, X_train_val_sm).fit(\n",
        "        disp=0,\n",
        "        method='lbfgs',\n",
        "        maxiter=200\n",
        "    )\n",
        "    print(\"\\nFinal Combined Model Summary:\")\n",
        "    print(final_combined_model.summary())\n",
        "except Exception as e:\n",
        "    print(f\"Error fitting combined model: {str(e)}\")\n",
        "    print(\"Using original model instead\")\n",
        "    final_combined_model = final_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0aSStkb7lU8q"
      },
      "outputs": [],
      "source": [
        "# Add constant explicitly\n",
        "X_test_selected = sm.add_constant(X_test_2[final_features].copy())\n",
        "\n",
        "# Ensure test data columns exactly match model's expected columns\n",
        "model_columns = final_combined_model.params.index\n",
        "X_test_selected_lr = pd.DataFrame(index=X_test_2.index)\n",
        "X_test_selected_lr['const'] = 1.0\n",
        "\n",
        "# Add each feature in the exact order expected by the model\n",
        "for col in model_columns:\n",
        "    if col != 'const':\n",
        "        if col in X_test_2.columns:\n",
        "            X_test_selected_lr[col] = X_test_2[col]\n",
        "        else:\n",
        "            print(f\"Missing column in test data: {col}\")\n",
        "            # Fill with zeros as a fallback\n",
        "            X_test_selected_lr[col] = 0\n",
        "\n",
        "# Verify the alignment\n",
        "print(f\"Test data shape: {X_test_selected_lr.shape}\")\n",
        "print(f\"Model params shape: {final_combined_model.params.shape}\")\n",
        "print(f\"Columns match: {list(X_test_selected_lr.columns) == list(model_columns)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bXQC91LjKc4F"
      },
      "outputs": [],
      "source": [
        "# Evaluate on test set\n",
        "print(\"\\nEvaluating on test set...\")\n",
        "\n",
        "# Get test predictions\n",
        "test_probs = final_combined_model.predict(X_test_selected_lr)\n",
        "test_preds = (test_probs >= chosen_threshold).astype(int)\n",
        "\n",
        "# Calculate detailed test metrics\n",
        "test_metrics = cal_metrics(y_test_2, test_preds)\n",
        "test_auc = roc_auc_score(y_test_2, test_probs)\n",
        "\n",
        "print(\"\\nTest Set Performance:\")\n",
        "print(f\"Accuracy: {test_metrics['Accuracy']:.4f}\")\n",
        "print(f\"AUC: {test_auc:.4f}\")\n",
        "print(f\"Recall (Sensitivity): {test_metrics['Recall']:.4f}\")\n",
        "print(f\"Specificity: {test_metrics['Specificity']:.4f}\")\n",
        "print(f\"Precision: {test_metrics['Precision']:.4f}\")\n",
        "print(f\"F1 Score: {test_metrics['F1_Score']:.4f}\")\n",
        "print(f\"False Negative Rate: {test_metrics['False_Negative_Rate']:.4f}\")\n",
        "print(f\"Error Rate: {test_metrics['Error_Rate']:.4f}\")\n",
        "print(\"\\nClassification Report:\")\n",
        "classification_report(y_test_2, test_preds)\n",
        "\n",
        "# Plot confusion matrix for test set\n",
        "plt.figure(figsize=(8, 6))\n",
        "cm_test = confusion_matrix(y_test_2, test_preds)\n",
        "sns.heatmap(cm_test, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
        "plt.title('Confusion Matrix (Test)')\n",
        "plt.ylabel('True Label')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.xticks([0.5, 1.5], ['Decrease/No Change', 'Increase'])\n",
        "plt.yticks([0.5, 1.5], ['Decrease/No Change', 'Increase'])\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Plot ROC curve\n",
        "fpr, tpr, _ = roc_curve(y_test_2, test_probs)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.3f})')\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC Curve for Bitcoin Price Direction Prediction')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0LDAyrcrKfFB"
      },
      "outputs": [],
      "source": [
        "# Print comprehensive final summary\n",
        "print(\"\\n===== FINAL MODEL SUMMARY =====\")\n",
        "print(f\"Selected {len(final_features)} features: {final_features}\")\n",
        "\n",
        "print(\"\\nValidation Performance:\")\n",
        "print(f\"Accuracy: {val_metrics['Accuracy']:.4f}\")\n",
        "print(f\"AUC: {val_auc:.4f}\")\n",
        "print(f\"Precision: {val_metrics['Precision']:.4f}\")\n",
        "print(f\"Recall (Sensitivity): {val_metrics['Recall']:.4f}\")\n",
        "print(f\"Specificity: {val_metrics['Specificity']:.4f}\")\n",
        "print(f\"F1 Score: {val_metrics['F1_Score']:.4f}\")\n",
        "print(f\"False Negative Rate: {val_metrics['False_Negative_Rate']:.4f}\")\n",
        "print(f\"Error Rate: {val_metrics['Error_Rate']:.4f}\")\n",
        "\n",
        "print(\"\\nTest Performance:\")\n",
        "print(f\"Accuracy: {test_metrics['Accuracy']:.4f}\")\n",
        "print(f\"AUC: {test_auc:.4f}\")\n",
        "print(f\"Precision: {test_metrics['Precision']:.4f}\")\n",
        "print(f\"Recall (Sensitivity): {test_metrics['Recall']:.4f}\")\n",
        "print(f\"Specificity: {test_metrics['Specificity']:.4f}\")\n",
        "print(f\"F1 Score: {test_metrics['F1_Score']:.4f}\")\n",
        "print(f\"False Negative Rate: {test_metrics['False_Negative_Rate']:.4f}\")\n",
        "print(f\"Error Rate: {test_metrics['Error_Rate']:.4f}\")\n",
        "\n",
        "print(f\"\\nOptimal threshold: {chosen_threshold:.2f}\")\n",
        "print(\"===============================\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CuEBLSrgQXMc"
      },
      "source": [
        "#Model Performance Comparison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_xTyXIiUwL9O"
      },
      "outputs": [],
      "source": [
        "# Define function to evaluate model performance with different feature sets\n",
        "def evaluate_all_models(models, X_test_dict, y_test, model_names):\n",
        "    \"\"\"\n",
        "    Parameters:\n",
        "    -----------\n",
        "    models : list\n",
        "        List of trained model objects\n",
        "    X_test_dict : dict\n",
        "        Dictionary mapping model names to their test features\n",
        "    y_test : array-like\n",
        "        Target values\n",
        "    model_names : list\n",
        "        List of model names corresponding to the models\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    metrics_df : DataFrame\n",
        "        DataFrame containing performance metrics for all models\n",
        "    \"\"\"\n",
        "    # First ensure y_test is binary (0/1)\n",
        "    print(f\"Original y_test shape: {y_test.shape}, dtype: {y_test.dtype}\")\n",
        "    print(f\"Unique values in y_test: {np.unique(y_test)}\")\n",
        "\n",
        "    # Convert y_test to binary if needed\n",
        "    if not np.array_equal(y_test, y_test.astype(bool).astype(int)):\n",
        "        print(\"Converting y_test to binary (0/1)\")\n",
        "        y_test_binary = (y_test > 0.5).astype(int)\n",
        "    else:\n",
        "        y_test_binary = y_test.copy()\n",
        "\n",
        "    print(f\"After conversion - unique values in y_test: {np.unique(y_test_binary)}\")\n",
        "\n",
        "    metrics = {\n",
        "        'Accuracy': [],\n",
        "        'Error_Rate': [],\n",
        "        'Precision': [],\n",
        "        'Recall': [],\n",
        "        'F1_Score': [],\n",
        "        'ROC_AUC': [],\n",
        "        'Specificity': []\n",
        "    }\n",
        "\n",
        "    all_predictions = {}\n",
        "    all_probabilities = {}\n",
        "\n",
        "    successful_models = []\n",
        "\n",
        "    for i, (model, name) in enumerate(zip(models, model_names)):\n",
        "        print(f\"\\nProcessing model: {name}\")\n",
        "\n",
        "        # Get the appropriate X_test for this model\n",
        "        if name in X_test_dict:\n",
        "            X_test_model = X_test_dict[name]\n",
        "        else:\n",
        "            print(f\"Warning: No specific X_test found for {name}. Using first available.\")\n",
        "            X_test_model = list(X_test_dict.values())[0]\n",
        "\n",
        "        try:\n",
        "            # Make predictions\n",
        "            y_pred_raw = model.predict(X_test_model)\n",
        "\n",
        "            # Convert predictions to binary if needed\n",
        "            if not np.array_equal(y_pred_raw, y_pred_raw.astype(bool).astype(int)):\n",
        "                print(f\"Converting predictions for {name} to binary (0/1)\")\n",
        "                y_pred = (y_pred_raw > 0.5).astype(int)\n",
        "            else:\n",
        "                y_pred = y_pred_raw\n",
        "\n",
        "            # Try to get probability scores\n",
        "            try:\n",
        "                y_prob = model.predict_proba(X_test_model)[:, 1]\n",
        "            except:\n",
        "                print(f\"No predict_proba for {name}, using alternative approach\")\n",
        "                # If predict_proba is not available, use predictions as probabilities\n",
        "                if np.max(y_pred_raw) > 1 or np.min(y_pred_raw) < 0:\n",
        "                    # For regression outputs, use sigmoid transformation\n",
        "                    y_prob = 1 / (1 + np.exp(-y_pred_raw))\n",
        "                else:\n",
        "                    y_prob = y_pred_raw\n",
        "\n",
        "            # Calculate confusion matrix elements\n",
        "            cm = confusion_matrix(y_test_binary, y_pred)\n",
        "            tn, fp, fn, tp = cm.ravel()\n",
        "\n",
        "            # Calculate metrics\n",
        "            accuracy = accuracy_score(y_test_binary, y_pred)\n",
        "            precision = precision_score(y_test_binary, y_pred)\n",
        "            recall = recall_score(y_test_binary, y_pred)\n",
        "            f1 = f1_score(y_test_binary, y_pred)\n",
        "\n",
        "            # ROC curve and AUC\n",
        "            fpr, tpr, _ = roc_curve(y_test_binary, y_prob)\n",
        "            roc_auc = auc(fpr, tpr)\n",
        "\n",
        "            # Specificity\n",
        "            specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
        "\n",
        "            # Store results\n",
        "            metrics['Accuracy'].append(accuracy)\n",
        "            metrics['Error_Rate'].append(1 - accuracy)\n",
        "            metrics['Precision'].append(precision)\n",
        "            metrics['Recall'].append(recall)\n",
        "            metrics['F1_Score'].append(f1)\n",
        "            metrics['ROC_AUC'].append(roc_auc)\n",
        "            metrics['Specificity'].append(specificity)\n",
        "\n",
        "            all_predictions[name] = y_pred\n",
        "            all_probabilities[name] = y_prob\n",
        "            successful_models.append(name)\n",
        "\n",
        "            print(f\"Successfully evaluated {name}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing model {name}: {e}\")\n",
        "            # Skip this model\n",
        "            continue\n",
        "\n",
        "    # Check if we have evaluated any models\n",
        "    if not successful_models:\n",
        "        print(\"No models were successfully evaluated!\")\n",
        "        return None\n",
        "\n",
        "    # Create DataFrame for metrics with only successful models\n",
        "    metrics_df = pd.DataFrame({k: v for k, v in metrics.items()}, index=successful_models)\n",
        "\n",
        "    # 1. Metrics Table\n",
        "    plt.figure(figsize=(15, 4))\n",
        "    sns.heatmap(metrics_df, annot=True, cmap='YlGnBu', fmt='.2f',\n",
        "                center=0.5, vmin=0, vmax=1)\n",
        "    plt.title('Model Performance Comparison')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # 2. ROC Curves\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    for name in successful_models:\n",
        "        fpr, tpr, _ = roc_curve(y_test_binary, all_probabilities[name])\n",
        "        roc_auc = auc(fpr, tpr)\n",
        "        plt.plot(fpr, tpr, label=f'{name} (AUC = {roc_auc:.2f})')\n",
        "\n",
        "    plt.plot([0, 1], [0, 1], 'k--')\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title('ROC Curves Comparison')\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    plt.show()\n",
        "\n",
        "    # 3. Confusion Matrices with percentages\n",
        "    fig, axes = plt.subplots(1, len(successful_models), figsize=(5*len(successful_models), 4))\n",
        "    if len(successful_models) == 1:\n",
        "        axes = [axes]\n",
        "\n",
        "    for ax, name in zip(axes, successful_models):\n",
        "        cm = confusion_matrix(y_test_binary, all_predictions[name])\n",
        "        cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "\n",
        "        # Plot counts\n",
        "        sns.heatmap(cm, annot=True, fmt='d', ax=ax, cmap='Blues')\n",
        "        ax.set_title(f'Confusion Matrix\\n{name}')\n",
        "        ax.set_ylabel('True Label')\n",
        "        ax.set_xlabel('Predicted Label')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # 4. Print detailed reports\n",
        "    for name in successful_models:\n",
        "        print(f\"\\nDetailed Performance Report for {name}:\")\n",
        "        print(\"-\" * 50)\n",
        "        print(\"Classification Report:\")\n",
        "        print(classification_report(y_test_binary, all_predictions[name]))\n",
        "\n",
        "        # Calculate additional metrics for detailed reporting\n",
        "        tn, fp, fn, tp = confusion_matrix(y_test_binary, all_predictions[name]).ravel()\n",
        "        print(\"\\nAdditional Metrics:\")\n",
        "        print(f\"Error Rate: {(1 - accuracy_score(y_test_binary, all_predictions[name])):.2f}\")\n",
        "        print(f\"Specificity: {tn/(tn+fp) if (tn+fp) > 0 else 0:.3f}\")\n",
        "\n",
        "    return metrics_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wFG9S99hwYwJ"
      },
      "outputs": [],
      "source": [
        "# Create a dictionary mapping model names to their corresponding test data\n",
        "X_test_dict = {\n",
        "    'ANN': X_test_2,\n",
        "    'Decision Tree': X_test_2,\n",
        "    'Random Forest': X_test_2,\n",
        "    'Tuned Random Forest': X_test_2_selected,\n",
        "    'Logistic Regression': X_test_selected_lr\n",
        "}\n",
        "\n",
        "# Set up the models list with the extracted ANN model\n",
        "models = [ann_model, dt_model_2, classifier, final_rfmodel_2, final_combined_model]\n",
        "model_names = ['ANN', 'Decision Tree', 'Random Forest', 'Tuned Random Forest', 'Logistic Regression']\n",
        "\n",
        "# Evaluate all models using the dictionary of test datasets\n",
        "metrics_df = evaluate_all_models(models, X_test_dict, y_test_2, model_names)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}